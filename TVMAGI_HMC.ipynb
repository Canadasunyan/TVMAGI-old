{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "integrated-reader",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import scipy\n",
    "import argparse\n",
    "import sys\n",
    "from arma import ode_system, solve_magi\n",
    "from arma import matrix\n",
    "import time\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "\n",
    "# Derivatves of X according to the ODE structure\n",
    "def fOde(theta, x):\n",
    "    \"\"\"\n",
    "    theta: list[4]: beta, ve, vi, pd\n",
    "    x: array(n, 4)\n",
    "    r: array(n, 2)\n",
    "    \"\"\"\n",
    "    global N\n",
    "    logS = x[:, 0]\n",
    "    logE = x[:, 1]\n",
    "    logI = x[:, 2]\n",
    "    logD = x[:, 3]\n",
    "    logSdt = -theta[0] * np.exp(logI) / N  # (1)\n",
    "    logEdt = theta[0] * np.exp(logS + logI - logE) / N - theta[1]  # (2)\n",
    "    logIdt = np.exp(logE - logI) * theta[1] - theta[2]  # (3)\n",
    "    logDdt = np.exp(logI - logD) * 0.25 * theta[3] * theta[2]  # (4)\n",
    "    return np.stack([logSdt, logEdt, logIdt, logDdt], axis=1)\n",
    "\n",
    "\n",
    "# Derivatives of X\n",
    "def fOdeDx(theta, x):\n",
    "    \"\"\"\n",
    "    returns derivation of x given theta\n",
    "    theta: list[4]\n",
    "    x: array(n, 4)\n",
    "    r: array(n, 4, 4)\n",
    "    \"\"\"\n",
    "    resultDx = np.zeros(shape=[np.shape(x)[0], np.shape(x)[1], np.shape(x)[1]])\n",
    "    global N\n",
    "    logS = x[:, 0]\n",
    "    logE = x[:, 1]\n",
    "    logI = x[:, 2]\n",
    "    logD = x[:, 3]\n",
    "    # [:, i, j]: 第j个方程关于第i个状态求导\n",
    "    # (1) / dI\n",
    "    resultDx[:, 2, 0] = -theta[0] * np.exp(logI) / N\n",
    "    # (1) / dS, (1) /dE, (1) / dD = 0\n",
    "    # (2) / dS\n",
    "    resultDx[:, 0, 1] = theta[0] * np.exp(logS + logI - logE) / N\n",
    "    # (2) / dE\n",
    "    resultDx[:, 1, 1] = -theta[0] * np.exp(logS + logI - logE) / N\n",
    "    # (2) / dI\n",
    "    resultDx[:, 2, 1] = theta[0] * np.exp(logS + logI - logE) / N\n",
    "    # (2) / dD = 0\n",
    "    # (3) / dS = 0\n",
    "    # (3) / dE\n",
    "    resultDx[:, 1, 2] = np.exp(logE - logI) * theta[1]\n",
    "    # (3) / dI\n",
    "    resultDx[:, 2, 2] = -np.exp(logE - logI) * theta[1]\n",
    "    # (3) / dD = 0, (4) / dS, dE = 0\n",
    "    # (4) / dI\n",
    "    resultDx[:, 2, 3] = np.exp(logI - logD) * 0.25 * theta[3] * theta[2]\n",
    "    # (4) / dD\n",
    "    resultDx[:, 3, 3] = -np.exp(logI - logD) * 0.25 * theta[3] * theta[2]\n",
    "    return resultDx\n",
    "\n",
    "\n",
    "def fOdeDtheta(theta, x):\n",
    "    \"\"\"\n",
    "    returns derivation of theta given x\n",
    "    theta: list[4]\n",
    "    x: array(n, 4)\n",
    "    r: array(n, 4, 4)\n",
    "    \"\"\"\n",
    "    global N\n",
    "    resultDtheta = np.zeros(shape=[np.shape(x)[0], np.shape(theta)[0], np.shape(x)[1]])\n",
    "    logS = x[:, 0]\n",
    "    logE = x[:, 1]\n",
    "    logI = x[:, 2]\n",
    "    logD = x[:, 3]\n",
    "    # [:, i, j]: 第j个方程对theta_i求导\n",
    "    # (1) / dRe\n",
    "    resultDtheta[:, 0, 0] = -np.exp(logI) / N\n",
    "    # (2) / d theta[0]\n",
    "    resultDtheta[:, 0, 1] = np.exp(logS + logI - logE) / N\n",
    "    # (2) / theta[1]\n",
    "    resultDtheta[:, 1, 1] = -1.\n",
    "    # (3) / dtheta[1]\n",
    "    resultDtheta[:, 1, 2] = np.exp(logE - logI)\n",
    "    # (3) / dtheta[2]\n",
    "    resultDtheta[:, 2, 2] = -1.\n",
    "    # (4) / theta[2]\n",
    "    resultDtheta[:, 2, 3] = np.exp(logI - logD) * 0.25 * theta[3] \n",
    "    # (4) / theta[3]\n",
    "    resultDtheta[:, 3, 3] = np.exp(logI - logD) * 0.25 * theta[2]\n",
    "    return resultDtheta\n",
    "\n",
    "def fOdeTorch(theta, x, constant_param_ls):\n",
    "    \"\"\"\n",
    "    theta: list[4]: beta, ve, vi, pd\n",
    "    x: array(n, 4)\n",
    "    r: array(n, 2)\n",
    "    \"\"\"\n",
    "    global N\n",
    "    logS = x[:, 0]\n",
    "    logE = x[:, 1]\n",
    "    logI = x[:, 2]\n",
    "    logD = x[:, 3]\n",
    "    logSdt = -theta[:, 0] * torch.exp(logI) / N  # (1)\n",
    "    logEdt = theta[:, 0] * torch.exp(logS + logI - logE) / N - theta[:, 1]  # (2)\n",
    "    logIdt = torch.exp(logE - logI) * theta[:, 1] - constant_param_ls[0]  # (3)\n",
    "    # reparametrize on pd\n",
    "    logDdt = torch.exp(logI - logD) * 0.25 * theta[:, 2] * constant_param_ls[0]  # (4)\n",
    "    return torch.stack([logSdt, logEdt, logIdt, logDdt], axis=1)\n",
    "\n",
    "\n",
    "def copy_mat(arma_mat):\n",
    "    return np.copy(matrix(arma_mat).reshape([-1])).reshape([arma_mat.n_rows, arma_mat.n_cols])\n",
    "\n",
    "\n",
    "def pointwisethetasigmallikTorch(xlatent, theta, time_constant_param_ls, sigma, inferred_theta, ydata, CovAllDimensionsPyList, fOdeTorch,\n",
    "                                 priorTemperature, obs_per_day, positive_param=True):\n",
    "    # length of observed y (t)\n",
    "    n = ydata.shape[0]\n",
    "    pdimension = ydata.shape[1]\n",
    "    thetadimension = theta.shape[1]\n",
    "    sigmaSq = torch.pow(sigma, 2)\n",
    "    fderiv = fOdeTorch(theta, xlatent, time_constant_param_ls)\n",
    "    res = torch.zeros([pdimension, 3]).double()\n",
    "    fitDerivError = torch.zeros([n, pdimension]).double()\n",
    "    nobs = torch.zeros([pdimension]).double()\n",
    "    fitLevelErrorSumSq = torch.zeros([pdimension]).double()\n",
    "    for vEachDim in range(pdimension):\n",
    "        fitDerivError[:, vEachDim] = fderiv[:, vEachDim]\n",
    "        tmp = torch.sparse.mm(CovAllDimensionsPyList[vEachDim]['mphi'], xlatent[:, vEachDim].reshape(-1, 1))\n",
    "        fitDerivError[:, vEachDim] -= tmp[:, 0]\n",
    "        nobs[vEachDim] = torch.sum(torch.isfinite(ydata[:, vEachDim]))\n",
    "        obsIdx = torch.isfinite(ydata[:, vEachDim])\n",
    "        fitLevelErrorSumSq[vEachDim] = torch.sum(torch.square(xlatent[obsIdx, vEachDim] - ydata[obsIdx, vEachDim]))\n",
    "    res[:, 0] = -0.5 * fitLevelErrorSumSq / sigmaSq - torch.log(sigma + 0.001) * nobs\n",
    "    res[:, 0] /= priorTemperature[2]\n",
    "    KinvfitDerivError = torch.zeros([n, pdimension]).double()\n",
    "    CinvX = torch.zeros([n, pdimension]).double()\n",
    "    for vEachDim in range(pdimension):\n",
    "        # inverse of K\n",
    "        KinvfitDerivError[:, vEachDim] = torch.sparse.mm(CovAllDimensionsPyList[vEachDim]['Kinv'], fitDerivError[:, vEachDim].reshape(-1, 1))[:, 0]\n",
    "        # inverse of Cd\n",
    "        CinvX[:, vEachDim] = torch.sparse.mm(CovAllDimensionsPyList[vEachDim]['Cinv'], xlatent[:, vEachDim].reshape(-1, 1))[:, 0]\n",
    "    res[:, 1] = -0.5 * torch.sum(fitDerivError * KinvfitDerivError, dim=0) / priorTemperature[0]\n",
    "    #  prior distriobution of X\n",
    "    res[:, 2] = -0.5 * torch.sum(xlatent * CinvX, dim=0) / priorTemperature[1]\n",
    "    theta_lb = torch.clamp(theta[:, 2], min=0.)\n",
    "#     theta_ub = torch.clamp(theta[:, 2], max=0.3)\n",
    "    return torch.sum(res) - 1e6 * torch.sum(torch.square(theta[:, 2] - theta_lb))\n",
    "\n",
    "def MaternKernel(d, phi_1, phi_2, nu=2.5):\n",
    "    \"\"\"\n",
    "    construct a kernel given time points and hyper parameters\n",
    "    \"\"\"\n",
    "    if nu == 2.5:  \n",
    "        a = torch.square(phi_1) * (\n",
    "                1. + np.sqrt(5) * d / phi_2 + 5. * torch.square(d) / (3. * torch.square(phi_2))) * torch.exp(\n",
    "            -np.sqrt(5) * d / phi_2)\n",
    "        return a.double()\n",
    "    else:\n",
    "        a = torch.square(phi_1) * (1. + np.sqrt(3) * d / phi_2) * torch.exp(-np.sqrt(3) * d / phi_2)       \n",
    "        return a.double()\n",
    "\n",
    "\n",
    "def kernelllik(phi_1, phi_2, sigma, y, d_matrix, phi1_lb, phi2_lb, sigma_lb):\n",
    "    \"\"\"\n",
    "    optimize the kernel hyperparameters by maximizing marginal likelihood\n",
    "    \"\"\"\n",
    "    phi_1_bounded = torch.clamp(phi_1, min=phi1_lb)\n",
    "    phi_2_bounded = torch.clamp(phi_2, min=phi2_lb)\n",
    "    sigma_bounded = torch.clamp(sigma, min=sigma_lb)\n",
    "    K = MaternKernel(d_matrix, phi_1, phi_2)\n",
    "    K += torch.square(sigma) * torch.eye(y.shape[0]).double()\n",
    "    return -y.shape[0] * np.log(np.sqrt(2 * np.pi)) - 0.5 * y @ torch.inverse(K) @ y - 0.5 * torch.logdet(K) - 1e8 * torch.square(phi_1 - phi_1_bounded) - 1e8 * torch.square(phi_2 - phi_2_bounded) - 1e8 * torch.square(sigma - sigma_bounded)\n",
    "\n",
    "def to_band(matrix, bandwidth):\n",
    "    dim = matrix.shape[0]\n",
    "    for i in range(dim):\n",
    "        for j in range(dim):\n",
    "            if i > j + bandwidth or i < j - bandwidth:\n",
    "                matrix[i][j] = 0\n",
    "    return matrix.to_sparse()\n",
    "\n",
    "def xthetasigmallikTorch(xlatent, theta, time_constant_param_ls, sigma, inferred_theta, ydata, CovAllDimensionsPyList, fOdeTorch,\n",
    "                         priorTemperature,\n",
    "                         KinvthetaList, positive=True):\n",
    "    # length of observed y (t)\n",
    "    n = ydata.shape[0]\n",
    "    pdimension = ydata.shape[1]\n",
    "    thetadimension = theta.shape[1]\n",
    "    sigmaSq = torch.pow(sigma, 2)\n",
    "    fderiv = fOdeTorch(theta, xlatent, time_constant_param_ls)\n",
    "    res = torch.zeros([pdimension, 3]).double()\n",
    "    res_theta = torch.zeros(thetadimension).double()\n",
    "    res2 = torch.zeros(1).double()\n",
    "    fitDerivError = torch.zeros([n, pdimension]).double()\n",
    "    nobs = torch.zeros([pdimension]).double()\n",
    "    fitLevelErrorSumSq = torch.zeros([pdimension]).double()\n",
    "    for vEachDim in range(pdimension):\n",
    "        fitDerivError[:, vEachDim] = fderiv[:, vEachDim]\n",
    "        fitDerivError[:, vEachDim] -= torch.sparse.mm(CovAllDimensionsPyList[vEachDim]['mphi'], xlatent[:, vEachDim].reshape(-1, 1))[:, 0]\n",
    "        nobs[vEachDim] = torch.sum(torch.isfinite(ydata[:, vEachDim]))\n",
    "        obsIdx = torch.isfinite(ydata[:, vEachDim])\n",
    "        fitLevelErrorSumSq[vEachDim] = torch.sum(torch.square(xlatent[obsIdx, vEachDim] - ydata[obsIdx, vEachDim]))\n",
    "    res[:, 0] = -0.5 * fitLevelErrorSumSq / sigmaSq - torch.log(sigma + 0.0001) * nobs\n",
    "    res[:, 0] /= priorTemperature[2]\n",
    "    KinvfitDerivError = torch.zeros([n, pdimension]).double()\n",
    "    CinvX = torch.zeros([n, pdimension]).double()\n",
    "    for vEachDim in range(pdimension):\n",
    "        # inverse of K\n",
    "        KinvfitDerivError[:, vEachDim] = torch.sparse.mm(CovAllDimensionsPyList[vEachDim]['Kinv'], fitDerivError[:, vEachDim].reshape(-1, 1))[:, 0]\n",
    "        # inverse of Cd\n",
    "        CinvX[:, vEachDim] = torch.sparse.mm(CovAllDimensionsPyList[vEachDim]['Cinv'], xlatent[:, vEachDim].reshape(-1, 1))[:, 0]\n",
    "    for thetaEachDim in range(thetadimension):\n",
    "        res_theta[thetaEachDim] = -0.5 * torch.sum(\n",
    "            (theta[:, thetaEachDim] - inferred_theta[thetaEachDim]) @ torch.sparse.mm(KinvthetaList[thetaEachDim], (\n",
    "                    theta[:, thetaEachDim] - inferred_theta[thetaEachDim]).reshape(-1, 1)))\n",
    "    res[:, 1] = -0.5 * torch.sum(fitDerivError * KinvfitDerivError, dim=0) / priorTemperature[0]\n",
    "    res[:, 2] = -0.5 * torch.sum(xlatent * CinvX, dim=0) / priorTemperature[1]\n",
    "    theta_lb = torch.clamp(theta[:, 2], min = 0.)\n",
    "    return torch.sum(res) + torch.sum(res_theta)  - 1e6 * torch.sum(torch.square(theta[:, 2] - theta_lb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "super-shirt",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "mathematical-enforcement",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_data_idx = 0\n",
    "days = 32\n",
    "discretization = 2\n",
    "obs_per_day = 1\n",
    "theta_lowerbound=np.array([0., 0., 0., 0.])\n",
    "theta_upperbound=np.array([np.inf, 1., 1., 1.])\n",
    "param_names = ['re', 've', 'vi', 'pd']\n",
    "is_time_varying=[True, True, False, True]\n",
    "use_trajectory='inferred'\n",
    "learning_rate=np.array([1e-4, 1e-3, 1e-5])\n",
    "n_iter = [15001, 100000, 15000, 100000]\n",
    "phi1_lb_ls=np.array([2., 0.1, 0.1])\n",
    "phi2_lb_ls=np.array([2., 2., 2.])\n",
    "sigma_lb_ls = np.array([0.01, 0.01, 0.01])\n",
    "bandwidth=20\n",
    "\n",
    "\n",
    "observations = np.load('SEIRD observations.npy')\n",
    "N = 100000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conventional-retail",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aware-viewer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.87143191 0.10205595 0.0924365  0.19611035]\n"
     ]
    }
   ],
   "source": [
    "yobs = observations[use_data_idx]\n",
    "yobs[:, 1] = np.interp(np.arange(0, days, 1), np.arange(0, days, 2), yobs[::2, 1])\n",
    "start_time = time.time()\n",
    "nobs, p_dim = yobs.shape[0], yobs.shape[1]\n",
    "n_points = days * discretization\n",
    "theta_dim = theta_lowerbound.shape[0]\n",
    "d_matrix = torch.zeros((n_points, n_points), dtype=torch.double)\n",
    "for i in range(n_points):\n",
    "    for j in range(n_points):\n",
    "        if i > j:\n",
    "            d_matrix[i][j] = (i - j) / (obs_per_day * discretization)\n",
    "        else:\n",
    "            d_matrix[i][j] = (j - i) / (obs_per_day * discretization)\n",
    "Ode_system = ode_system(\"ODE-python\", fOde, fOdeDx, fOdeDtheta,\n",
    "                         thetaLowerBound=theta_lowerbound,\n",
    "                         thetaUpperBound=theta_upperbound)\n",
    "tvecObs = np.arange(0, days, 1. / obs_per_day)\n",
    "tvecFull = np.arange(0, days, 1. / (obs_per_day * discretization))\n",
    "yFull = np.ndarray([n_points, p_dim])\n",
    "yFull.fill(np.nan)\n",
    "yFull[np.arange(0, discretization * nobs, discretization).astype(int), :] = yobs\n",
    "xInitExogenous = np.zeros_like(yFull)\n",
    "# interpolate: find the ydata of tvecFull given observations\n",
    "for i in range(p_dim):\n",
    "    xInitExogenous[:, i] = np.interp(tvecFull, tvecObs, yobs[:, i])\n",
    "\n",
    "\n",
    "# First stage: use MAGI package to optimize theta as constant #####################\n",
    "\n",
    "time_1 = time.time()\n",
    "result = solve_magi(\n",
    "    yFull,\n",
    "    Ode_system,\n",
    "    tvecFull,\n",
    "    sigmaExogenous=np.array([]),\n",
    "    phiExogenous=np.array([[]]),\n",
    "    xInitExogenous=xInitExogenous,\n",
    "    thetaInitExogenous=np.array([]),\n",
    "    muExogenous=np.array([[]]),\n",
    "    dotmuExogenous=np.array([[]]),\n",
    "    priorTemperatureLevel=yFull.shape[0] / yobs.shape[0],\n",
    "    priorTemperatureDeriv=yFull.shape[0] / yobs.shape[0],\n",
    "    priorTemperatureObs=1.0,\n",
    "    kernel=\"generalMatern\",\n",
    "    nstepsHmc=100,\n",
    "    burninRatioHmc=0.5,\n",
    "    niterHmc=n_iter[0],\n",
    "    stepSizeFactorHmc=0.01,\n",
    "    nEpoch=1,\n",
    "    bandSize=bandwidth,\n",
    "    useFrequencyBasedPrior=True,\n",
    "    useBand=True,\n",
    "    useMean=False,\n",
    "    useScalerSigma=False,\n",
    "    useFixedSigma=False,\n",
    "    verbose=True)\n",
    "samplesCpp = result['samplesCpp']\n",
    "llikId = 0\n",
    "xId = range(np.max(llikId) + 1, np.max(llikId) + yFull.size + 1)\n",
    "# dimension of theta\n",
    "thetaId = range(np.max(xId) + 1, np.max(xId) + theta_dim + 1)\n",
    "sigmaId = range(np.max(thetaId) + 1, np.max(thetaId) + yFull.shape[1] + 1)\n",
    "burnin = int(n_iter[0] * 0.5)\n",
    "xsampled = samplesCpp[xId, (burnin + 1):]\n",
    "xsampled = xsampled.reshape([yFull.shape[1], yFull.shape[0], -1])\n",
    "CovAllDimensionsPyList = []\n",
    "thetaSampled = samplesCpp[thetaId, (burnin + 1):]\n",
    "inferred_theta = np.mean(thetaSampled, axis=-1)\n",
    "print(inferred_theta)\n",
    "sigmaSampled = samplesCpp[sigmaId, (burnin + 1):]\n",
    "inferred_sigma = np.mean(sigmaSampled, axis=-1)\n",
    "inferred_trajectory = np.mean(xsampled, axis=-1)\n",
    "MAGI_time = time.time() - time_1\n",
    "for each_gpcov in result['result_solved'].covAllDimensions:\n",
    "        each_pycov = dict(\n",
    "            Cinv=to_band(torch.from_numpy(matrix(each_gpcov.Cinv)).double(),bandwidth=bandwidth), \n",
    "            Kinv=to_band(torch.from_numpy(matrix(each_gpcov.Kinv)).double(),bandwidth=bandwidth),\n",
    "            mphi=to_band(torch.from_numpy(matrix(each_gpcov.mphi)).double(),bandwidth=bandwidth),\n",
    "        )\n",
    "        CovAllDimensionsPyList.append(each_pycov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civil-floor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "subjective-income",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -33.46622826583106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py:132: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  allow_unreachable=True)  # allow_unreachable flag\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 -61.46563253865944\n",
      "400 -65.46867597014563\n",
      "600 -70.30719567185052\n",
      "800 -75.8779406930725\n",
      "1000 -82.14628031770047\n",
      "1200 -89.11858788285758\n",
      "1400 -96.83452296441611\n",
      "1600 -105.37385043792555\n",
      "1800 -114.86939955901603\n",
      "2000 -125.52391045651167\n",
      "2200 -137.63084675785728\n",
      "2400 -151.57108526671408\n",
      "2600 -167.36598236474424\n",
      "2800 -182.12545914942248\n",
      "3000 -191.45635228751482\n",
      "3200 -197.9308097034147\n",
      "3400 -203.6519816645105\n",
      "3600 -209.0308313392814\n",
      "3800 -214.26151423425827\n",
      "4000 -219.5572896632014\n",
      "4200 -225.18761975798978\n",
      "4400 -231.55542682526388\n",
      "4600 -238.9431599926636\n",
      "4800 -245.99614954845265\n",
      "5000 -251.6919028597476\n",
      "5200 -256.94624629263217\n",
      "5400 -262.0846765678946\n",
      "5600 -267.20006527565323\n",
      "5800 -272.33597765371474\n",
      "6000 -277.5218352361838\n",
      "6200 -282.7747847475248\n",
      "6400 -288.0937203044424\n",
      "6600 -293.4700904446499\n",
      "6800 -298.88509143458487\n",
      "7000 -304.37054876995614\n",
      "7200 -309.98225229812874\n",
      "7400 -315.72715841067986\n",
      "7600 -321.2681428789656\n",
      "7800 -326.0010593191453\n",
      "8000 -329.78837822384327\n",
      "8200 -332.74233223820045\n",
      "8400 -335.0024363230434\n",
      "8600 -336.7114557767537\n",
      "8800 -337.9967715604188\n",
      "9000 -338.97326171144164\n",
      "9200 -339.7225517604884\n",
      "9400 -340.3093871413464\n",
      "9600 -340.7717346832153\n",
      "9800 -341.1408218906167\n",
      "10000 -341.432472822683\n",
      "10200 -341.5599332130558\n",
      "10400 -341.6731362343355\n",
      "10600 -341.7752989308383\n",
      "10800 -341.8667280700818\n",
      "11000 -341.94791873481427\n",
      "11200 -342.01914229688305\n",
      "11400 -342.08130036754335\n",
      "11600 -342.13447329669685\n",
      "11800 -342.17986345757635\n",
      "12000 -342.21761985567616\n",
      "12200 -342.2493352138674\n",
      "12400 -342.2747117382854\n",
      "12600 -342.2945978060435\n",
      "12800 -342.31101310300653\n",
      "13000 -342.32337795359365\n",
      "13200 -342.33273868056483\n",
      "13400 -342.3397591318935\n",
      "13600 -342.344655968153\n",
      "13800 -342.3487514475568\n",
      "14000 -342.3512357735412\n",
      "14200 -342.35308456554554\n",
      "14400 -342.3547810188786\n",
      "14600 -342.35573119143754\n",
      "14800 -342.35638748773994\n",
      "15000 -342.35689255011545\n",
      "15200 -342.35741661748114\n",
      "15400 -342.35752226409403\n",
      "15600 -342.3575904001705\n",
      "15800 -342.3576785348457\n",
      "16000 -342.3577331388785\n",
      "16200 -342.35776596641176\n",
      "16400 -342.3577891156985\n",
      "16600 -342.3578136076236\n",
      "16800 -342.35750100346036\n",
      "17000 -342.3577453925567\n",
      "17200 -342.3576950130128\n",
      "17400 -342.35796295968925\n",
      "17600 -342.35780766009515\n",
      "17800 -342.3578033438474\n",
      "18000 -342.35788965690506\n",
      "18200 -342.3576989848165\n",
      "18400 -342.35778543840865\n",
      "18600 -342.35794628951953\n",
      "18800 -342.35745676872386\n",
      "19000 -342.35755186200834\n",
      "19200 -342.357860204139\n",
      "19400 -342.3575510787878\n",
      "19600 -342.35770570957953\n",
      "19800 -342.3574818629818\n",
      "20000 -342.35673456477184\n",
      "20200 -342.3580900987602\n",
      "20400 -342.3580898653592\n",
      "20600 -342.35807983783474\n",
      "20800 -342.3580707639342\n",
      "21000 -342.35806981540463\n",
      "21200 -342.35807109117604\n",
      "21400 -342.35802681202574\n",
      "21600 -342.3578813643657\n",
      "21800 -342.3579450774761\n",
      "22000 -342.35804251604776\n",
      "22200 -342.3580517796214\n",
      "22400 -342.35802556024896\n",
      "22600 -342.3579995893484\n",
      "22800 -342.35804161805555\n",
      "23000 -342.3580260360532\n",
      "23200 -342.3579489418417\n",
      "23400 -342.35803309965297\n",
      "23600 -342.3580439557304\n",
      "23800 -342.35804420276634\n",
      "24000 -342.3580359203186\n",
      "24200 -342.3578448936381\n",
      "24400 -342.35803228527107\n",
      "24600 -342.3580033069654\n",
      "24800 -342.35799106478976\n",
      "25000 -342.3580588145443\n",
      "25200 -342.3580312386739\n",
      "25400 -342.3580175430024\n",
      "25600 -342.3580560945643\n",
      "25800 -342.35791264773906\n",
      "26000 -342.35806563653244\n",
      "26200 -342.35802201661886\n",
      "26400 -342.35804665256285\n",
      "26600 -342.3579898630657\n",
      "26800 -342.3580314711889\n",
      "27000 -342.357975231447\n",
      "27200 -342.3580639237042\n",
      "27400 -342.3580171752264\n",
      "27600 -342.358005843772\n",
      "27800 -342.358016245552\n",
      "28000 -342.35803307148177\n",
      "28200 -342.357953484572\n",
      "28400 -342.35802513192573\n",
      "28600 -342.358038884357\n",
      "28800 -342.3579487890685\n",
      "29000 -342.35790990990637\n",
      "29200 -342.35803610016154\n",
      "29400 -342.35797724075525\n",
      "29600 -342.3580261714343\n",
      "29800 -342.3580323350543\n",
      "30000 -342.35799394887704\n",
      "30200 -342.3580919152471\n",
      "30400 -342.35809191930855\n",
      "30600 -342.3580919202619\n",
      "30800 -342.3580919185044\n",
      "31000 -342.35809189855416\n",
      "31200 -342.35809123069504\n",
      "31400 -342.35809032148467\n",
      "31600 -342.35805218263175\n",
      "31800 -342.3580820392709\n",
      "32000 -342.35805462307303\n",
      "32200 -342.35808120825726\n",
      "32400 -342.3580754463838\n",
      "32600 -342.3580751658538\n",
      "32800 -342.35807323735673\n",
      "33000 -342.3580751175106\n",
      "33200 -342.3580671461252\n",
      "33400 -342.3580830215078\n",
      "33600 -342.3580767436487\n",
      "33800 -342.35807258714055\n",
      "34000 -342.3580844854069\n",
      "34200 -342.3580563199304\n",
      "34400 -342.35807559360484\n",
      "34600 -342.3580612557484\n",
      "34800 -342.3580812672743\n",
      "35000 -342.35805983390424\n",
      "35200 -342.3580853846412\n",
      "35400 -342.35806053757005\n",
      "35600 -342.35805506941483\n",
      "35800 -342.3580764242717\n",
      "36000 -342.3580566262547\n",
      "36200 -342.358055381375\n",
      "36400 -342.35807871671915\n",
      "36600 -342.35808172986896\n",
      "36800 -342.35807136848587\n",
      "37000 -342.3580385322563\n",
      "37200 -342.3580813737105\n",
      "37400 -342.35803900975424\n",
      "37600 -342.3580747286471\n",
      "37800 -342.3580770891628\n",
      "38000 -342.3580879047108\n",
      "38200 -342.35804488264006\n",
      "38400 -342.35804275957116\n",
      "38600 -342.358069700506\n",
      "38800 -342.3580836157221\n",
      "39000 -342.3580562022509\n",
      "39200 -342.35802733042027\n",
      "39400 -342.35806395564725\n",
      "39600 -342.3580692297127\n",
      "39800 -342.3580684793937\n",
      "40000 -342.3580807735489\n",
      "40200 -342.3580919177176\n",
      "40400 -342.3580919143747\n",
      "40600 -342.358091921841\n",
      "40800 -342.3580919143446\n",
      "41000 -342.3580919191926\n",
      "41200 -342.3580919130283\n",
      "41400 -342.3580919166294\n",
      "41600 -342.3580906316793\n",
      "41800 -342.35809148849097\n",
      "42000 -342.3580903015786\n",
      "42200 -342.3580883670907\n",
      "42400 -342.35808588272994\n",
      "42600 -342.3580886066448\n",
      "42800 -342.358088438894\n",
      "43000 -342.3580744198634\n",
      "43200 -342.3580863531203\n",
      "43400 -342.35808224376274\n",
      "43600 -342.35809104827445\n",
      "43800 -342.35808934021344\n",
      "44000 -342.35809101194053\n",
      "44200 -342.3580858449236\n",
      "44400 -342.35808939601037\n",
      "44600 -342.35808579493977\n",
      "44800 -342.35809036151494\n",
      "45000 -342.3580902242361\n",
      "45200 -342.35808858610534\n",
      "45400 -342.3580817425371\n",
      "45600 -342.3580903843549\n",
      "45800 -342.35808832723103\n",
      "46000 -342.358078382581\n",
      "46200 -342.35809047145585\n",
      "46400 -342.35808539113617\n",
      "46600 -342.3580906473741\n",
      "46800 -342.35809121262133\n",
      "47000 -342.35808641414553\n",
      "47200 -342.35809147596456\n",
      "47400 -342.35808243587303\n",
      "47600 -342.35808652051213\n",
      "47800 -342.35808718441814\n",
      "48000 -342.35809101052985\n",
      "48200 -342.35808276134514\n",
      "48400 -342.35808919008787\n",
      "48600 -342.35808918739247\n",
      "48800 -342.3580865971425\n",
      "49000 -342.35808864674635\n",
      "49200 -342.35807980743544\n",
      "49400 -342.35808966973366\n",
      "49600 -342.3580896100523\n",
      "49800 -342.3580870474196\n",
      "50000 -342.3580887185846\n",
      "50200 -342.3580919117945\n",
      "50400 -342.35809191899216\n",
      "50600 -342.35809191640055\n",
      "50800 -342.35809191830106\n",
      "51000 -342.3580919117077\n",
      "51200 -342.3580919141636\n",
      "51400 -342.35809191392457\n",
      "51600 -342.35809148513215\n",
      "51800 -342.358089682838\n",
      "52000 -342.3580918319819\n",
      "52200 -342.35809169721506\n",
      "52400 -342.3580917198183\n",
      "52600 -342.3580915804657\n",
      "52800 -342.3580899545602\n",
      "53000 -342.35809108705405\n",
      "53200 -342.35809139070835\n",
      "53400 -342.35809119991825\n",
      "53600 -342.3580905813944\n",
      "53800 -342.3580905523524\n",
      "54000 -342.3580917443159\n",
      "54200 -342.3580909824341\n",
      "54400 -342.35809089521\n",
      "54600 -342.35809108467856\n",
      "54800 -342.3580916590263\n",
      "55000 -342.358091121372\n",
      "55200 -342.35809157524824\n",
      "55400 -342.35809118330724\n",
      "55600 -342.35809149081973\n",
      "55800 -342.35809077502887\n",
      "56000 -342.35809092190215\n",
      "56200 -342.3580907034643\n",
      "56400 -342.35809164731097\n",
      "56600 -342.3580884060699\n",
      "56800 -342.3580917111666\n",
      "57000 -342.3580907390846\n",
      "57200 -342.35809086820086\n",
      "57400 -342.3580916355104\n",
      "57600 -342.3580915728815\n",
      "57800 -342.35808960289745\n",
      "58000 -342.3580918290424\n",
      "58200 -342.3580885447139\n",
      "58400 -342.3580917661774\n",
      "58600 -342.3580856718825\n",
      "58800 -342.3580904668123\n",
      "59000 -342.35809057890947\n",
      "59200 -342.35809127413904\n",
      "59400 -342.35809050530787\n",
      "59600 -342.35809137920006\n",
      "59800 -342.3580893753422\n",
      "60000 -342.35809035358\n",
      "60200 -342.3580919121203\n",
      "60400 -342.35809191723774\n",
      "60600 -342.3580919181397\n",
      "60800 -342.3580919184721\n",
      "61000 -342.3580919159571\n",
      "61200 -342.3580919154846\n",
      "61400 -342.3580919189344\n",
      "61600 -342.3580919118746\n",
      "61800 -342.35809189018875\n",
      "62000 -342.3580918784733\n",
      "62200 -342.3580916064556\n",
      "62400 -342.35809126578675\n",
      "62600 -342.3580917749659\n",
      "62800 -342.3580918541429\n",
      "63000 -342.3580918267023\n",
      "63200 -342.3580915597983\n",
      "63400 -342.3580917032814\n",
      "63600 -342.3580917077685\n",
      "63800 -342.35809178491553\n",
      "64000 -342.35809161462186\n",
      "64200 -342.3580918357093\n",
      "64400 -342.3580918623022\n",
      "64600 -342.3580918320033\n",
      "64800 -342.35809181969677\n",
      "65000 -342.35809169219976\n",
      "65200 -342.3580915849459\n",
      "65400 -342.35809159411934\n",
      "65600 -342.35809170233665\n",
      "65800 -342.35809177008457\n",
      "66000 -342.358091847622\n",
      "66200 -342.3580917945288\n",
      "66400 -342.3580916726456\n",
      "66600 -342.3580918596933\n",
      "66800 -342.3580918115992\n",
      "67000 -342.35809183803144\n",
      "67200 -342.3580918352246\n",
      "67400 -342.35809134493144\n",
      "67600 -342.3580917734958\n",
      "67800 -342.358091747124\n",
      "68000 -342.35809182509684\n",
      "68200 -342.3580917920475\n",
      "68400 -342.35809176335346\n",
      "68600 -342.35809181575786\n",
      "68800 -342.3580918028483\n",
      "69000 -342.3580919002009\n",
      "69200 -342.3580915809931\n",
      "69400 -342.35809183930536\n",
      "69600 -342.3580916259511\n",
      "69800 -342.35809165755813\n",
      "70000 -342.35809153771316\n",
      "70200 -342.35809191387585\n",
      "70400 -342.35809191616335\n",
      "70600 -342.3580919188049\n",
      "70800 -342.35809191082075\n",
      "71000 -342.3580919141594\n",
      "71200 -342.35809191768305\n",
      "71400 -342.3580919126026\n",
      "71600 -342.3580919120421\n",
      "71800 -342.35809189498787\n",
      "72000 -342.35809187492663\n",
      "72200 -342.35809188355665\n",
      "72400 -342.35809180488707\n",
      "72600 -342.35809190871447\n",
      "72800 -342.3580918525813\n",
      "73000 -342.3580918005846\n",
      "73200 -342.3580918987376\n",
      "73400 -342.35809187332046\n",
      "73600 -342.3580919104027\n",
      "73800 -342.3580918783407\n",
      "74000 -342.3580918613205\n",
      "74200 -342.3580918867211\n",
      "74400 -342.35809188759947\n",
      "74600 -342.3580918921017\n",
      "74800 -342.358091908253\n",
      "75000 -342.35809186623175\n",
      "75200 -342.3580918766086\n",
      "75400 -342.3580918694973\n",
      "75600 -342.35809189036706\n",
      "75800 -342.3580919064195\n",
      "76000 -342.3580918581878\n",
      "76200 -342.358091892178\n",
      "76400 -342.35809189269804\n",
      "76600 -342.3580918244375\n",
      "76800 -342.35809191110263\n",
      "77000 -342.3580918915574\n",
      "77200 -342.3580917920858\n",
      "77400 -342.35809180092684\n",
      "77600 -342.35809189525565\n",
      "77800 -342.3580918572929\n",
      "78000 -342.35809179489036\n",
      "78200 -342.35809182461446\n",
      "78400 -342.3580918883905\n",
      "78600 -342.35809175126116\n",
      "78800 -342.35809171501865\n",
      "79000 -342.3580915758631\n",
      "79200 -342.35809163125\n",
      "79400 -342.35809185928537\n",
      "79600 -342.35809189477993\n",
      "79800 -342.35809189447764\n",
      "80000 -342.3580918540753\n",
      "80200 -342.3580919129703\n",
      "80400 -342.3580919230408\n",
      "80600 -342.35809191713065\n",
      "80800 -342.35809191233\n",
      "81000 -342.3580919147591\n",
      "81200 -342.35809191262763\n",
      "81400 -342.3580919129067\n",
      "81600 -342.3580919108693\n",
      "81800 -342.35809190283635\n",
      "82000 -342.35809191421544\n",
      "82200 -342.35809191180596\n",
      "82400 -342.35809190628464\n",
      "82600 -342.35809190556984\n",
      "82800 -342.35809191109263\n",
      "83000 -342.35809192035356\n",
      "83200 -342.35809186261696\n",
      "83400 -342.35809191180084\n",
      "83600 -342.35809190342553\n",
      "83800 -342.3580919021306\n",
      "84000 -342.3580919113601\n",
      "84200 -342.3580919047395\n",
      "84400 -342.35809191129243\n",
      "84600 -342.3580919053318\n",
      "84800 -342.3580918745999\n",
      "85000 -342.3580919144352\n",
      "85200 -342.3580919125075\n",
      "85400 -342.3580918965218\n",
      "85600 -342.3580919103326\n",
      "85800 -342.35809191042836\n",
      "86000 -342.3580919086011\n",
      "86200 -342.35809184986545\n",
      "86400 -342.35809185308244\n",
      "86600 -342.3580919120482\n",
      "86800 -342.3580919071831\n",
      "87000 -342.35809189954995\n",
      "87200 -342.35809190729526\n",
      "87400 -342.35809191755027\n",
      "87600 -342.35809191030546\n",
      "87800 -342.35809187306137\n",
      "88000 -342.35809190872794\n",
      "88200 -342.35809189981495\n",
      "88400 -342.3580919152883\n",
      "88600 -342.35809189676945\n",
      "88800 -342.35809191056956\n",
      "89000 -342.3580919139549\n",
      "89200 -342.35809191460714\n",
      "89400 -342.3580918847881\n",
      "89600 -342.3580918864387\n",
      "89800 -342.3580918934227\n",
      "90000 -342.3580919005972\n",
      "90200 -342.35809191296414\n",
      "90400 -342.35809191496395\n",
      "90600 -342.35809191400017\n",
      "90800 -342.3580919168236\n",
      "91000 -342.35809191655164\n",
      "91200 -342.3580919170757\n",
      "91400 -342.35809191282965\n",
      "91600 -342.35809190874966\n",
      "91800 -342.35809191862234\n",
      "92000 -342.35809191473686\n",
      "92200 -342.3580919132092\n",
      "92400 -342.35809189654907\n",
      "92600 -342.3580919153567\n",
      "92800 -342.3580919113533\n",
      "93000 -342.35809191339695\n",
      "93200 -342.35809191464557\n",
      "93400 -342.3580919150507\n",
      "93600 -342.3580919142598\n",
      "93800 -342.3580919156026\n",
      "94000 -342.3580919166453\n",
      "94200 -342.3580919173976\n",
      "94400 -342.3580919107901\n",
      "94600 -342.3580919115506\n",
      "94800 -342.35809191185785\n",
      "95000 -342.35809191581654\n",
      "95200 -342.3580919115294\n",
      "95400 -342.3580919179942\n",
      "95600 -342.35809191752776\n",
      "95800 -342.35809191247665\n",
      "96000 -342.3580919156395\n",
      "96200 -342.35809191517706\n",
      "96400 -342.3580919093701\n",
      "96600 -342.358091913971\n",
      "96800 -342.3580919119923\n",
      "97000 -342.3580919127261\n",
      "97200 -342.3580919117357\n",
      "97400 -342.35809191478364\n",
      "97600 -342.35809191447083\n",
      "97800 -342.35809191484435\n",
      "98000 -342.35809190974123\n",
      "98200 -342.35809191432173\n",
      "98400 -342.3580919095989\n",
      "98600 -342.35809191302167\n",
      "98800 -342.35809191992143\n",
      "99000 -342.358091910717\n",
      "99200 -342.35809190476203\n",
      "99400 -342.3580919153469\n",
      "99600 -342.3580919174\n",
      "99800 -342.35809191607\n"
     ]
    }
   ],
   "source": [
    "time_2 = time.time()\n",
    "TV_theta_mean = np.zeros(int(sum(is_time_varying)))\n",
    "tv_index = 0\n",
    "for thetaEachDim in range(theta_dim):\n",
    "    if is_time_varying[thetaEachDim] == True:\n",
    "        TV_theta_mean[tv_index] = inferred_theta[thetaEachDim]\n",
    "        tv_index += 1\n",
    "\n",
    "if use_trajectory == 'observation':\n",
    "    pointwise_xlatent_torch = torch.tensor(xInitExogenous, requires_grad=True, dtype=torch.double)\n",
    "elif use_trajectory == 'inferred':\n",
    "    pointwise_xlatent_torch = torch.tensor(inferred_trajectory.transpose(), requires_grad=True, dtype=torch.double)\n",
    "else:\n",
    "    raise ValueError\n",
    "tmp1 = np.array([TV_theta_mean])\n",
    "initial_tvtheta = np.repeat(tmp1, pointwise_xlatent_torch.shape[0], axis=0)\n",
    "pointwise_theta_torch = torch.tensor(initial_tvtheta, requires_grad=True, dtype=torch.double)\n",
    "pointwise_sigma_torch = torch.tensor(inferred_sigma, requires_grad=True, dtype=torch.double)\n",
    "time_constant_param_ls = []\n",
    "for thetaEachDim in range(theta_dim):\n",
    "    if is_time_varying[thetaEachDim] == 0:\n",
    "        param_name = param_names[thetaEachDim]\n",
    "        locals()[param_name] = torch.tensor([inferred_theta[thetaEachDim]], requires_grad=True, dtype=torch.double)\n",
    "        time_constant_param_ls.append(eval(param_name))\n",
    "\n",
    "ydata = torch.from_numpy(yFull).double()\n",
    "priorTemperature = torch.tensor([discretization, discretization, 1.0])  # ?\n",
    "pointwise_optimizer = torch.optim.Adam([pointwise_xlatent_torch, pointwise_theta_torch, pointwise_sigma_torch] + time_constant_param_ls, lr=1e-4)  # , weight_decay = 1.0\n",
    "pointwise_lr_scheduler = torch.optim.lr_scheduler.StepLR(pointwise_optimizer, step_size=10000, gamma=0.5)\n",
    "cur_loss = 1e12\n",
    "LossVal = np.zeros(n_iter[1])\n",
    "backward_time_ls = np.zeros(n_iter[1])\n",
    "step_time_ls = np.zeros(n_iter[1])\n",
    "for epoch in range(n_iter[1]):\n",
    "    pointwise_optimizer.zero_grad()\n",
    "    # compute loss function\n",
    "    llik = pointwisethetasigmallikTorch(pointwise_xlatent_torch, pointwise_theta_torch, time_constant_param_ls, pointwise_sigma_torch,\n",
    "                                        TV_theta_mean, ydata, CovAllDimensionsPyList, fOdeTorch,\n",
    "                                        priorTemperature, obs_per_day)\n",
    "    new_loss = -llik\n",
    "    LossVal[epoch] = new_loss\n",
    "    if epoch % 200 == 0:\n",
    "        print(epoch, new_loss.item())\n",
    "        diff = new_loss.item() - cur_loss\n",
    "#             if torch.isnan(new_loss) == False and diff > -0.01 and diff < 0.01:\n",
    "#                 break\n",
    "        cur_loss = new_loss.item()\n",
    "    time_b = time.time()\n",
    "    new_loss.backward()\n",
    "    backward_time_ls[epoch] = time.time() - time_b\n",
    "    time_s = time.time()\n",
    "    pointwise_optimizer.step()\n",
    "    step_time_ls[epoch] = time.time() - time_s\n",
    "    pointwise_lr_scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consolidated-newport",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi1_lb_ls=np.array([1., 0.1, 0.1])\n",
    "phi2_lb_ls=np.array([5, 5, 5.])\n",
    "sigma_lb_ls = np.array([0.01, 0.01, 0.01])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "respiratory-attachment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -17.045264437511406\n",
      "100 -57.2654439289052\n",
      "200 -57.27242347884419\n",
      "300 -57.28138347798024\n",
      "400 -57.29195530470518\n",
      "500 -57.30396269686233\n",
      "600 -57.31738500879\n",
      "700 -57.332349364101844\n",
      "800 -57.34915290256771\n",
      "900 -57.368317875259876\n",
      "1000 -57.390699468539935\n",
      "1100 -57.417692022457416\n",
      "1200 -57.451636201171624\n",
      "1300 -57.49667337202453\n",
      "1400 -57.56069997424096\n",
      "1500 -57.66039803057788\n",
      "1600 -57.836565957498394\n",
      "1700 -58.21567291866501\n",
      "1800 -59.49614532050225\n",
      "1900 -58.96083492281923\n",
      "2000 -62.35150660336815\n",
      "2100 -63.74390692352637\n",
      "2200 -63.998503001268176\n",
      "2300 -66.85619200669004\n",
      "2400 -71.11577783291844\n",
      "2500 -71.59268937064996\n",
      "2600 -70.98546062398373\n",
      "2700 -73.34819020876114\n",
      "2800 -68.81057515107771\n",
      "2900 -73.42607005152757\n",
      "3000 -73.41744352840105\n",
      "3100 -73.50282694949593\n",
      "3200 -72.72483850555945\n",
      "3300 -72.09603762006438\n",
      "3400 -73.5423100194126\n",
      "3500 -72.99400527399011\n",
      "3600 -71.94628328568733\n",
      "3700 -73.35206709332266\n",
      "3800 -73.2894271869855\n",
      "3900 -73.427182663423\n",
      "4000 -73.25782942425396\n",
      "4100 -73.40019049277657\n",
      "4200 -72.27239802827867\n",
      "4300 -72.58834381444602\n",
      "4400 -71.32868520684984\n",
      "4500 -73.51806544008677\n",
      "4600 -70.23963323506578\n",
      "4700 -73.10486795711236\n",
      "4800 -73.19524923772447\n",
      "4900 -72.63241532081017\n",
      "5000 -72.85166652946079\n",
      "5100 -71.36484697864537\n",
      "5200 -72.31757154678635\n",
      "5300 -72.84269973101432\n",
      "5400 -73.15549701096319\n",
      "5500 -69.636256018855\n",
      "5600 -73.10836585665844\n",
      "5700 -72.84443533272693\n",
      "5800 -73.23374830090233\n",
      "5900 -70.53713565087742\n",
      "6000 -73.0315438583427\n",
      "6100 -73.50775536141826\n",
      "6200 -71.63954902451025\n",
      "6300 -73.52567837129425\n",
      "6400 -72.57783795912161\n",
      "6500 -73.12704253010045\n",
      "6600 -73.3611586014183\n",
      "6700 -71.36545974066648\n",
      "6800 -73.4876734041944\n",
      "6900 -73.48124615511131\n",
      "7000 -71.8982749045016\n",
      "7100 -73.15818537923465\n",
      "7200 -73.11572873310504\n",
      "7300 -73.50628916820561\n",
      "7400 -72.8102357252264\n",
      "7500 -73.1056704239524\n",
      "7600 -72.1312643411854\n",
      "7700 -73.43762287040104\n",
      "7800 -72.91521984059082\n",
      "7900 -73.03282261440152\n",
      "8000 -73.36113407289193\n",
      "8100 -73.190486237696\n",
      "8200 -71.87609956596845\n",
      "8300 -70.98070924327776\n",
      "8400 -72.89988682324218\n",
      "8500 -70.88945399313135\n",
      "8600 -73.45501738005873\n",
      "8700 -73.4456166386965\n",
      "8800 -73.44859706472725\n",
      "8900 -72.78877006458205\n",
      "9000 -73.27859730278655\n",
      "9100 -72.51237203835804\n",
      "9200 -73.25379860145296\n",
      "9300 -70.26033939317821\n",
      "9400 -72.60777803656617\n",
      "9500 -72.20237561969726\n",
      "9600 -73.15839497599009\n",
      "9700 -73.12874088805665\n",
      "9800 -71.91779469722502\n",
      "9900 -71.40149367589565\n",
      "10000 -72.56358496980845\n",
      "10100 -72.8658400580328\n",
      "10200 -72.67493169643632\n",
      "10300 -71.74571697680412\n",
      "10400 -72.10332050451572\n",
      "10500 -72.41587263686395\n",
      "10600 -73.33765883510311\n",
      "10700 -72.84732036712936\n",
      "10800 -72.32721534798372\n",
      "10900 -73.23956175111847\n",
      "11000 -73.1753724283757\n",
      "11100 -72.50963119097227\n",
      "11200 -73.46265163157767\n",
      "11300 -72.65898242185573\n",
      "11400 -72.928067680091\n",
      "11500 -73.00895182801597\n",
      "11600 -72.35057579862014\n",
      "11700 -72.87615684809438\n",
      "11800 -73.07351597441372\n",
      "11900 -71.83497647709865\n",
      "12000 -73.12879143669498\n",
      "12100 -72.42551547162283\n",
      "12200 -71.20661298996959\n",
      "12300 -73.44932869859299\n",
      "12400 -72.83390798837341\n",
      "12500 -71.78249793402402\n",
      "12600 -73.12931561857211\n",
      "12700 -72.46628814626186\n",
      "12800 -71.02791307691352\n",
      "12900 -73.39611391526775\n",
      "13000 -73.06126822859775\n",
      "13100 -72.55216003652984\n",
      "13200 -71.41187199111394\n",
      "13300 -73.36565406092748\n",
      "13400 -72.29063057965382\n",
      "13500 -72.8302037493642\n",
      "13600 -73.40134816863994\n",
      "13700 -72.75309083908029\n",
      "13800 -71.761728960145\n",
      "13900 -73.05172143789619\n",
      "14000 -72.9475688262251\n",
      "14100 -73.14548797731514\n",
      "14200 -73.12221429617503\n",
      "14300 -73.13122244101484\n",
      "14400 -70.33690350274199\n",
      "14500 -72.91562797543398\n",
      "14600 -71.60121736373648\n",
      "14700 -71.13434092096537\n",
      "14800 -73.48382912196959\n",
      "14900 -72.35557565986669\n",
      "2.4441616100366073 5.006551861067592 0.009965715818059615\n",
      "0 -194.9166726114396\n",
      "100 -194.56722064043183\n",
      "200 -195.04948226773098\n",
      "300 -195.39783424075716\n",
      "400 -195.7282229806411\n",
      "500 -196.03976297541638\n",
      "600 -196.33169602467422\n",
      "700 -196.55570271793437\n",
      "800 -196.71379947617345\n",
      "900 -196.8567466507758\n",
      "1000 -196.99052157749696\n",
      "1100 -194.78519119105695\n",
      "1200 -196.96622663617316\n",
      "1300 -197.3299409565009\n",
      "1400 -197.42757337984855\n",
      "1500 -197.50380590367413\n",
      "1600 -197.58503242928995\n",
      "1700 -197.31083875608175\n",
      "1800 -197.7307965200731\n",
      "1900 -197.79913304137784\n",
      "2000 -188.96120829796848\n",
      "2100 -193.8566998872057\n",
      "2200 -197.94585673455128\n",
      "2300 -197.99798679163027\n",
      "2400 -198.03448936469903\n",
      "2500 -198.07740831939103\n",
      "2600 -198.11061137287697\n",
      "2700 -198.13548353702168\n",
      "2800 -196.5903198580781\n",
      "2900 -198.15885373495902\n",
      "3000 -198.20234540578736\n",
      "3100 -198.203564266168\n",
      "3200 -198.2354822439776\n",
      "3300 -198.22673499833994\n",
      "3400 -198.24977326749385\n",
      "3500 -198.27560230020134\n",
      "3600 -198.28003939688338\n",
      "3700 -193.13213527340736\n",
      "3800 -198.1086941397233\n",
      "3900 -197.45271371848793\n",
      "4000 -198.29872223937855\n",
      "4100 -198.28812730023563\n",
      "4200 -198.31055564067555\n",
      "4300 -198.30049873395166\n",
      "4400 -198.31687196816694\n",
      "4500 -198.31548257822493\n",
      "4600 -193.51398593769602\n",
      "4700 -198.06799578665536\n",
      "4800 -198.25915255968087\n",
      "4900 -198.3201991139909\n",
      "5000 -198.3197797509017\n",
      "5100 -198.31053996879643\n",
      "5200 -198.31921284280028\n",
      "5300 -198.31539902600855\n",
      "5400 -198.3172081592876\n",
      "5500 -197.16581075884255\n",
      "5600 -195.33047673643955\n",
      "5700 -198.31304469223917\n",
      "5800 -198.31851197371876\n",
      "5900 -198.31854356044977\n",
      "6000 -198.31450504605726\n",
      "6100 -198.28835332032563\n",
      "6200 -198.3114532460216\n",
      "6300 -198.31851883960766\n",
      "6400 -196.1982349248936\n",
      "6500 -198.2980695927633\n",
      "6600 -198.31705101548692\n",
      "6700 -198.30651320919748\n",
      "6800 -198.31548673265434\n",
      "6900 -194.91865692565995\n",
      "7000 -198.07207590437685\n",
      "7100 -198.28037026448092\n",
      "7200 -198.29173898462523\n",
      "7300 -198.31476472698859\n",
      "7400 -198.30699243833286\n",
      "7500 -198.31781366609212\n",
      "7600 -198.31246055228453\n",
      "7700 -198.3145221079138\n",
      "7800 -198.17784446378283\n",
      "7900 -191.87017910358048\n",
      "8000 -196.3926350317956\n",
      "8100 -198.30121519799616\n",
      "8200 -198.31050354309124\n",
      "8300 -198.25184194487164\n",
      "8400 -198.29547022435364\n",
      "8500 -198.30774359102097\n",
      "8600 -198.31485480444684\n",
      "8700 -198.28312093104904\n",
      "8800 -193.95035302525662\n",
      "8900 -194.9913830961155\n",
      "9000 -196.3435389441383\n",
      "9100 -198.2313809269802\n",
      "9200 -198.2667902782058\n",
      "9300 -198.27594339006484\n",
      "9400 -198.28303396710126\n",
      "9500 -198.28998437116056\n",
      "9600 -198.29688098238327\n",
      "9700 -198.3036992436893\n",
      "9800 -198.31040837712897\n",
      "9900 -184.8795396015177\n",
      "10000 -196.6387819590484\n",
      "10100 -198.293269642182\n",
      "10200 -198.30024345449215\n",
      "10300 -198.30683276698744\n",
      "10400 -198.31331049950074\n",
      "10500 -198.27710362130378\n",
      "10600 -198.30893109636466\n",
      "10700 -198.31605423727908\n",
      "10800 -193.48363236602603\n",
      "10900 -196.0351507526767\n",
      "11000 -197.72781616121756\n",
      "11100 -198.3013459322449\n",
      "11200 -198.30951743086177\n",
      "11300 -198.26085333855394\n",
      "11400 -198.3092118098827\n",
      "11500 -198.11638477456026\n",
      "11600 -198.28856074402313\n",
      "11700 -198.30137216224938\n",
      "11800 -194.77476783682965\n",
      "11900 -196.97468061271678\n",
      "12000 -198.24197460174136\n",
      "12100 -198.26379526419603\n",
      "12200 -198.27181744057967\n",
      "12300 -198.27875915526383\n",
      "12400 -198.28562934228805\n",
      "12500 -198.29247017839066\n",
      "12600 -190.81493712505284\n",
      "12700 -195.4596592819937\n",
      "12800 -198.23206860370092\n",
      "12900 -198.30525303685977\n",
      "13000 -198.20111656264552\n",
      "13100 -198.28689344842607\n",
      "13200 -198.30291387852034\n",
      "13300 -198.3101415757884\n",
      "13400 -198.31582821233422\n",
      "13500 -196.61062011691658\n",
      "13600 -197.153348802986\n",
      "13700 -196.76271200603034\n",
      "13800 -198.30997319176777\n",
      "13900 -198.1958621140796\n",
      "14000 -198.29446329894623\n",
      "14100 -198.30627358898147\n",
      "14200 -198.31340836828477\n",
      "14300 -195.7444967845853\n",
      "14400 -195.32967857201106\n",
      "14500 -198.2316057776472\n",
      "14600 -197.06857970961187\n",
      "14700 -198.26695753766734\n",
      "14800 -198.28711108509995\n",
      "14900 -198.29473036673195\n",
      "0.10230323266624272 7.8103529885395755 0.009979571610336002\n",
      "0 754.3606458652159\n",
      "100 -32.88668340682571\n",
      "200 -49.30668253203231\n",
      "300 -54.82041249962957\n",
      "400 -57.2112546109891\n",
      "500 -58.37537642397547\n",
      "600 -58.97634437581239\n",
      "700 -59.295339752666905\n",
      "800 -59.466119352510006\n",
      "900 -59.557096614682735\n",
      "1000 -59.60482990114846\n",
      "1100 -59.62929827538238\n",
      "1200 -59.64147847357154\n",
      "1300 -59.647346962497096\n",
      "1400 -59.650088870176006\n",
      "1500 -59.651348040379006\n",
      "1600 -59.65193839056394\n",
      "1700 -59.652243387698974\n",
      "1800 -59.65243370167413\n",
      "1900 -59.65258062059695\n",
      "2000 -59.652711918360495\n",
      "2100 -59.652837632295785\n",
      "2200 -59.65296112361817\n",
      "2300 -59.65308346443473\n",
      "2400 -59.65320503763368\n",
      "2500 -59.653326069924475\n",
      "2600 -59.65344679202613\n",
      "2700 -59.65356748016559\n",
      "2800 -59.65368846312086\n",
      "2900 -59.65381012001802\n",
      "3000 -59.6539328757153\n",
      "3100 -59.65405719550209\n",
      "3200 -59.65418357958602\n",
      "3300 -59.65431255757079\n",
      "3400 -59.654444683079106\n",
      "3500 -59.65458052866401\n",
      "3600 -59.65472068114603\n",
      "3700 -59.65486573750144\n",
      "3800 -59.65501630141023\n",
      "3900 -59.655172980551626\n",
      "4000 -59.65533638471081\n",
      "4100 -59.65550712473083\n",
      "4200 -59.655685812314516\n",
      "4300 -59.65587306065271\n",
      "4400 -59.65606948582294\n",
      "4500 -59.656275708882035\n",
      "4600 -59.65649235855102\n",
      "4700 -59.656720074382235\n",
      "4800 -59.65695951028741\n",
      "4900 -59.657211338311996\n",
      "5000 -59.65747625254605\n",
      "5100 -59.65775497308053\n",
      "5200 -59.658048249937906\n",
      "5300 -59.65835686692779\n",
      "5400 -59.6586816454048\n",
      "5500 -59.65902344792573\n",
      "5600 -59.6593831818236\n",
      "5700 -59.65821341167327\n",
      "5800 -59.65860991163774\n",
      "5900 -59.65902741419511\n",
      "6000 -59.659400489419156\n",
      "6100 -59.65878902295778\n",
      "6200 -59.65927282042327\n",
      "6300 -59.65717026872102\n",
      "6400 -59.65770180622215\n",
      "6500 -59.65826343022461\n",
      "6600 -59.65885450182438\n",
      "6700 -59.65764198398391\n",
      "6800 -59.65563370174959\n",
      "6900 -59.65631259126039\n",
      "7000 -59.65702650494721\n",
      "7100 -59.65777815697295\n",
      "7200 -59.65856990303955\n",
      "7300 -59.65940405097287\n",
      "7400 -59.651907013415666\n",
      "7500 -59.65279538380612\n",
      "7600 -59.653728023473576\n",
      "7700 -59.65471028601088\n",
      "7800 -59.65574557827357\n",
      "7900 -59.656837022512676\n",
      "8000 -59.657987855752296\n",
      "8100 -59.6592015092624\n",
      "8200 -59.641691487653304\n",
      "8300 -59.642825673808204\n",
      "8400 -59.644008447255274\n",
      "8500 -59.64525482954747\n",
      "8600 -59.64656946643403\n",
      "8700 -59.64795640016291\n",
      "8800 -59.6494198977571\n",
      "8900 -59.65096451052689\n",
      "9000 -59.652595098963076\n",
      "9100 -59.65431685855509\n",
      "9200 -59.65613534829059\n",
      "9300 -59.658056522275146\n",
      "9400 -59.645993293405894\n",
      "9500 -59.64737008417703\n",
      "9600 -59.649386596994404\n",
      "9700 -59.65151794976552\n",
      "9800 -59.65377147554722\n",
      "9900 -59.65615499657483\n",
      "10000 -59.658676939561104\n",
      "10100 -59.62631326949604\n",
      "10200 -59.62740971599251\n",
      "10300 -59.62858509840434\n",
      "10400 -59.629824581711844\n",
      "10500 -59.63113188537534\n",
      "10600 -59.63251097126697\n",
      "10700 -59.63396606196892\n",
      "10800 -59.63550166142912\n",
      "10900 -59.63712257768307\n",
      "11000 -59.638833947904345\n",
      "11100 -59.64064126608595\n",
      "11200 -59.64255041370467\n",
      "11300 -59.644567693778555\n",
      "11400 -59.64669986879639\n",
      "11500 -59.64895420308622\n",
      "11600 -59.651338510286706\n",
      "11700 -59.65386120671795\n",
      "11800 -59.656531371595435\n",
      "11900 -59.65935881522806\n",
      "12000 -59.64100662882743\n",
      "12100 -59.64351773156915\n",
      "12200 -59.64617614858926\n",
      "12300 -59.64899080988194\n",
      "12400 -59.65197221428255\n",
      "12500 -59.655131739589905\n",
      "12600 -59.65848174068083\n",
      "12700 -59.62698074743864\n",
      "12800 -59.62851748856845\n",
      "12900 -59.630157752920326\n",
      "13000 -59.6318895862841\n",
      "13100 -59.633718553572905\n",
      "13200 -59.63565061301338\n",
      "13300 -59.637692150211066\n",
      "13400 -59.63985001662165\n",
      "13500 -59.64213157251771\n",
      "13600 -59.644544735126544\n",
      "13700 -59.64709803274198\n",
      "13800 -59.64980066576294\n",
      "13900 -59.65266257581047\n",
      "14000 -59.65569452430876\n",
      "14100 -59.65890818222053\n",
      "14200 -59.63880618586336\n",
      "14300 -59.64153325321766\n",
      "14400 -59.64443799237932\n",
      "14500 -59.64751543272281\n",
      "14600 -59.6507774369867\n",
      "14700 -59.65423689759024\n",
      "14800 -59.65790785663037\n",
      "14900 -59.623072824044954\n",
      "0.19079791922206032 5.0302095909572095 0.07443094632478073\n"
     ]
    }
   ],
   "source": [
    "# Kernel estimation ########################################\n",
    "time_3 = time.time()\n",
    "pointwise_theta = pointwise_theta_torch.detach().numpy()\n",
    "hyperparamList = []\n",
    "\n",
    "# optimize the hyperparameters of kernels\n",
    "for thetaEachDimension in range(pointwise_theta.shape[1]):\n",
    "    phi_1 = torch.tensor(phi1_lb_ls[thetaEachDimension], requires_grad=True, dtype=torch.double)\n",
    "    phi_2 = torch.tensor(phi2_lb_ls[thetaEachDimension], requires_grad=True, dtype=torch.double)\n",
    "    sigma = torch.tensor(sigma_lb_ls[thetaEachDimension], requires_grad=True, dtype=torch.double)\n",
    "    kernel_optimizer = torch.optim.Adam([phi_1, phi_2, sigma], lr=learning_rate[1])\n",
    "    kernel_backward_time = np.zeros(n_iter[2])\n",
    "    kernel_step_time = np.zeros(n_iter[2])\n",
    "    for epoch in range(n_iter[2]):\n",
    "        kernel_optimizer.zero_grad()\n",
    "        loss = -kernelllik(phi_1, \n",
    "                           phi_2, \n",
    "                           sigma, \n",
    "                           pointwise_theta_torch[:, thetaEachDimension] - torch.mean(pointwise_theta_torch[:, thetaEachDimension]), \n",
    "                           d_matrix, \n",
    "                           phi1_lb_ls[thetaEachDimension], \n",
    "                           phi2_lb_ls[thetaEachDimension],\n",
    "                           sigma_lb_ls[thetaEachDimension])\n",
    "        time0 = time.time()\n",
    "        loss.backward()\n",
    "        kernel_backward_time[epoch] = time.time() - time0\n",
    "        time0 = time.time()\n",
    "        kernel_optimizer.step()\n",
    "        kernel_step_time[epoch] = time.time() - time0\n",
    "        if epoch % 100 == 0:\n",
    "            print(epoch, loss.item())\n",
    "    print(phi_1.detach().item(), phi_2.detach().item(), sigma.detach().item()) \n",
    "    kernel_backward_time = np.mean(kernel_backward_time)\n",
    "    kernel_step_time = np.mean(kernel_step_time)\n",
    "    hyperparamList.append([phi_1.detach().item(), phi_2.detach().item(), sigma.detach().item()])\n",
    "\n",
    "KinvthetaList = []\n",
    "for thetaEachDimension in range(pointwise_theta.shape[1]):\n",
    "    ker = MaternKernel(d_matrix, torch.tensor(hyperparamList[thetaEachDimension][0]), torch.tensor(hyperparamList[thetaEachDimension][1]))\n",
    "    KinvthetaList.append(to_band(torch.inverse(ker), bandwidth = bandwidth))\n",
    "kernel_total_time = time.time() - time_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honest-rubber",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "alpha-somewhere",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 8677.230300203548\n",
      "500 7084.911684459861\n",
      "1000 6270.923083552891\n",
      "1500 5665.589692969606\n",
      "2000 5164.096155352367\n",
      "2500 4715.652201950382\n",
      "3000 4302.933584259656\n",
      "3500 3921.814829906526\n",
      "4000 3568.710153949779\n",
      "4500 3241.8668810882045\n",
      "5000 2940.0549358827216\n",
      "5500 2661.6977259807745\n",
      "6000 2405.083298227235\n",
      "6500 2168.5520248723847\n",
      "7000 1950.3648716573725\n",
      "7500 1748.4887628921792\n",
      "8000 1561.6385105653337\n",
      "8500 1388.933533214501\n",
      "9000 1229.5811175378385\n",
      "9500 1083.0008458897635\n",
      "10000 950.1337724357802\n",
      "10500 841.3051554523119\n",
      "11000 741.8941711827061\n",
      "11500 653.1821980256996\n",
      "12000 578.6145971726423\n",
      "12500 512.3954588960343\n",
      "13000 451.00519409931206\n",
      "13500 390.95631765684243\n",
      "14000 332.2131911813717\n",
      "14500 275.35171811231555\n",
      "15000 220.8854070001292\n",
      "15500 169.20559176325284\n",
      "16000 120.49591427648546\n",
      "16500 74.69214117482456\n",
      "17000 31.711330057877564\n",
      "17500 -8.38926895033383\n",
      "18000 -45.4071632179195\n",
      "18500 -79.01725194454148\n",
      "19000 -108.90342643620296\n",
      "19500 -135.12631003787277\n",
      "20000 -158.41099195368233\n",
      "20500 -176.98622592846922\n",
      "21000 -194.5254172830893\n",
      "21500 -210.63172143231688\n",
      "22000 -224.76894133529126\n",
      "22500 -237.06665145014418\n",
      "23000 -247.9254882906507\n",
      "23500 -257.5548579161545\n",
      "24000 -266.22572670277407\n",
      "24500 -274.0127377257811\n",
      "25000 -280.9554581474123\n",
      "25500 -287.10351785398973\n",
      "26000 -292.5201844529732\n",
      "26500 -297.27660042206554\n",
      "27000 -301.44690676172326\n",
      "27500 -305.1028126329339\n",
      "28000 -308.30882034484955\n",
      "28500 -311.1198755966969\n",
      "29000 -313.5816945393946\n",
      "29500 -315.73165546752307\n",
      "30000 -317.6016148123835\n",
      "30500 -319.07388448941657\n",
      "31000 -320.36820124338794\n",
      "31500 -321.4982595212826\n",
      "32000 -322.4791669535228\n",
      "32500 -323.3222212194159\n",
      "33000 -324.052962615404\n",
      "33500 -324.6810643223825\n",
      "34000 -325.2223935394368\n",
      "34500 -325.69777551923687\n",
      "35000 -326.1213802941955\n",
      "35500 -326.4994041122378\n",
      "36000 -326.8356099785061\n",
      "36500 -327.13393808914884\n",
      "37000 -327.3988230312216\n",
      "37500 -327.6348098860843\n",
      "38000 -327.8468128539675\n",
      "38500 -328.03859539580645\n",
      "39000 -328.21247745768693\n",
      "39500 -328.37542465137557\n",
      "40000 -328.52494404004847\n",
      "40500 -328.6508140817132\n",
      "41000 -328.768435001539\n",
      "41500 -328.877518394412\n",
      "42000 -328.97861333431666\n",
      "42500 -329.07178602561066\n",
      "43000 -329.157909292138\n",
      "43500 -329.2350048821365\n",
      "44000 -329.31319666020937\n",
      "44500 -329.38189135736104\n",
      "45000 -329.45184720270106\n",
      "45500 -329.51636953789364\n",
      "46000 -329.5780440212722\n",
      "46500 -329.636869210342\n",
      "47000 -329.6928132466139\n",
      "47500 -329.74583908265123\n",
      "48000 -329.79590351123255\n",
      "48500 -329.843097233587\n",
      "49000 -329.8872721979254\n",
      "49500 -329.9284604846589\n",
      "50000 -329.9666456176962\n",
      "50500 -329.9989490749836\n",
      "51000 -330.0291234575058\n",
      "51500 -330.05697842108316\n",
      "52000 -330.0824663772103\n",
      "52500 -330.10558108860005\n",
      "53000 -330.12638613530436\n",
      "53500 -330.14498954143227\n",
      "54000 -330.1614039761232\n",
      "54500 -330.1757437688525\n",
      "55000 -330.18739672685047\n",
      "55500 -330.1986761958328\n",
      "56000 -330.20749473820337\n",
      "56500 -330.2148216422332\n",
      "57000 -330.22077978985715\n",
      "57500 -330.2256148687676\n",
      "58000 -330.2294529126544\n",
      "58500 -330.2325259411231\n",
      "59000 -330.2350242390313\n",
      "59500 -330.23705904532983\n",
      "60000 -330.23869892387813\n",
      "60500 -330.2401077544519\n",
      "61000 -330.24136653624106\n",
      "61500 -330.24165335552277\n",
      "62000 -330.24336544800167\n",
      "62500 -330.2441990184843\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-1b8851c64211>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     llik = xthetasigmallikTorch(TVMAGI_xlatent_torch, TVMAGI_theta_torch, time_constant_param_ls, TVMAGI_sigma_torch,\n\u001b[1;32m     15\u001b[0m                                                 \u001b[0mTV_theta_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mydata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCovAllDimensionsPyList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfOdeTorch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                                                 priorTemperature, KinvthetaList)\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mllik\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m500\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-35ae259608c9>\u001b[0m in \u001b[0;36mxthetasigmallikTorch\u001b[0;34m(xlatent, theta, time_constant_param_ls, sigma, inferred_theta, ydata, CovAllDimensionsPyList, fOdeTorch, priorTemperature, KinvthetaList, positive)\u001b[0m\n\u001b[1;32m    224\u001b[0m         res_theta[thetaEachDim] = -0.5 * torch.sum(\n\u001b[1;32m    225\u001b[0m             (theta[:, thetaEachDim] - inferred_theta[thetaEachDim]) @ torch.sparse.mm(KinvthetaList[thetaEachDim], (\n\u001b[0;32m--> 226\u001b[0;31m                     theta[:, thetaEachDim] - inferred_theta[thetaEachDim]).reshape(-1, 1)))\n\u001b[0m\u001b[1;32m    227\u001b[0m     \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfitDerivError\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mKinvfitDerivError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mpriorTemperature\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxlatent\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mCinvX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mpriorTemperature\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TVMAGI optimization #############################\n",
    "time_4 = time.time()\n",
    "TVMAGI_xlatent_torch = torch.tensor(pointwise_xlatent_torch.detach().numpy(), requires_grad=True, dtype=torch.double)\n",
    "TVMAGI_theta_torch = torch.tensor(pointwise_theta_torch.detach().numpy(), requires_grad=True, dtype=torch.double)\n",
    "TVMAGI_sigma_torch = torch.tensor(inferred_sigma, requires_grad=True, dtype=torch.double)\n",
    "TVMAGI_optimizer = torch.optim.Adam([TVMAGI_xlatent_torch, TVMAGI_theta_torch, TVMAGI_sigma_torch] + time_constant_param_ls, lr=1e-5)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(TVMAGI_optimizer, step_size=10000, gamma=0.9)\n",
    "cur_loss = 1e12\n",
    "TVMAGI_backward_time = np.zeros(n_iter[3])\n",
    "TVMAGI_step_time = np.zeros(n_iter[3])\n",
    "for epoch in range(n_iter[3]):\n",
    "    TVMAGI_optimizer.zero_grad()\n",
    "    # compute loss function\n",
    "    llik = xthetasigmallikTorch(TVMAGI_xlatent_torch, TVMAGI_theta_torch, time_constant_param_ls, TVMAGI_sigma_torch,\n",
    "                                                TV_theta_mean, ydata, CovAllDimensionsPyList, fOdeTorch,\n",
    "                                                priorTemperature, KinvthetaList)\n",
    "    loss = -llik\n",
    "    if epoch % 500 == 0:\n",
    "        print(epoch, loss.item())\n",
    "#             if torch.isnan(loss) == False and loss - cur_loss > -0.01 and loss - cur_loss < 0.01:\n",
    "#                 break\n",
    "        cur_loss = loss.item()\n",
    "    time0 = time.time()\n",
    "    loss.backward()\n",
    "    TVMAGI_backward_time[epoch] = time.time() - time0\n",
    "    time0 = time.time()\n",
    "    TVMAGI_optimizer.step()\n",
    "    TVMAGI_step_time[epoch] = time.time() - time0\n",
    "    lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "remarkable-sculpture",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biological-public",
   "metadata": {},
   "outputs": [],
   "source": [
    "KinvthetaList = []\n",
    "for thetaEachDimension in range(pointwise_theta.shape[1]):\n",
    "    ker = MaternKernel(d_matrix, torch.tensor(0.1), torch.tensor(8.))\n",
    "    KinvthetaList.append(to_band(torch.inverse(ker), bandwidth = bandwidth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affecting-corruption",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "exempt-inventory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "tensor(-153.0684, dtype=torch.float64)\n",
      "acceptance rate:  0.67\n",
      "300\n",
      "tensor(-147.8625, dtype=torch.float64)\n",
      "acceptance rate:  0.79\n",
      "400\n",
      "tensor(-134.8585, dtype=torch.float64)\n",
      "acceptance rate:  0.75\n",
      "500\n",
      "tensor(-137.8189, dtype=torch.float64)\n",
      "acceptance rate:  0.73\n",
      "600\n",
      "tensor(-126.2097, dtype=torch.float64)\n",
      "acceptance rate:  0.63\n",
      "700\n",
      "tensor(-136.0552, dtype=torch.float64)\n",
      "acceptance rate:  0.75\n",
      "800\n",
      "tensor(-135.9073, dtype=torch.float64)\n",
      "acceptance rate:  0.75\n",
      "900\n",
      "tensor(-121.8453, dtype=torch.float64)\n",
      "acceptance rate:  0.76\n",
      "1000\n",
      "tensor(-120.1046, dtype=torch.float64)\n",
      "acceptance rate:  0.71\n",
      "1100\n",
      "tensor(-137.5219, dtype=torch.float64)\n",
      "acceptance rate:  0.76\n",
      "1200\n",
      "tensor(-130.1501, dtype=torch.float64)\n",
      "acceptance rate:  0.79\n",
      "1300\n",
      "tensor(-119.6414, dtype=torch.float64)\n",
      "acceptance rate:  0.77\n",
      "1400\n",
      "tensor(-84.6482, dtype=torch.float64)\n",
      "acceptance rate:  0.79\n",
      "1500\n",
      "tensor(-134.1069, dtype=torch.float64)\n",
      "acceptance rate:  0.77\n",
      "1600\n",
      "tensor(-113.1810, dtype=torch.float64)\n",
      "acceptance rate:  0.74\n",
      "1700\n",
      "tensor(-127.9880, dtype=torch.float64)\n",
      "acceptance rate:  0.88\n",
      "1800\n",
      "tensor(-117.2038, dtype=torch.float64)\n",
      "acceptance rate:  0.81\n",
      "1900\n",
      "tensor(-135.6050, dtype=torch.float64)\n",
      "acceptance rate:  0.72\n",
      "2000\n",
      "tensor(-113.1628, dtype=torch.float64)\n",
      "acceptance rate:  0.84\n",
      "2100\n",
      "tensor(-121.4110, dtype=torch.float64)\n",
      "acceptance rate:  0.89\n",
      "2200\n",
      "tensor(-102.0452, dtype=torch.float64)\n",
      "acceptance rate:  0.74\n",
      "2300\n",
      "tensor(-152.6799, dtype=torch.float64)\n",
      "acceptance rate:  0.9\n",
      "2400\n",
      "tensor(-138.2742, dtype=torch.float64)\n",
      "acceptance rate:  0.82\n",
      "2500\n",
      "tensor(-130.5795, dtype=torch.float64)\n",
      "acceptance rate:  0.75\n",
      "2600\n",
      "tensor(-105.2115, dtype=torch.float64)\n",
      "acceptance rate:  0.78\n",
      "2700\n",
      "tensor(-125.9641, dtype=torch.float64)\n",
      "acceptance rate:  0.72\n",
      "2800\n",
      "tensor(-127.1642, dtype=torch.float64)\n",
      "acceptance rate:  0.83\n",
      "2900\n",
      "tensor(-127.3772, dtype=torch.float64)\n",
      "acceptance rate:  0.82\n",
      "3000\n",
      "tensor(-107.8765, dtype=torch.float64)\n",
      "acceptance rate:  0.69\n",
      "3100\n",
      "tensor(-131.0427, dtype=torch.float64)\n",
      "acceptance rate:  0.89\n",
      "3200\n",
      "tensor(-125.5141, dtype=torch.float64)\n",
      "acceptance rate:  0.84\n",
      "3300\n",
      "tensor(-124.5293, dtype=torch.float64)\n",
      "acceptance rate:  0.81\n",
      "3400\n",
      "tensor(-147.1917, dtype=torch.float64)\n",
      "acceptance rate:  0.72\n",
      "3500\n",
      "tensor(-123.0131, dtype=torch.float64)\n",
      "acceptance rate:  0.78\n",
      "3600\n",
      "tensor(-114.2134, dtype=torch.float64)\n",
      "acceptance rate:  0.73\n",
      "3700\n",
      "tensor(-108.1572, dtype=torch.float64)\n",
      "acceptance rate:  0.76\n",
      "3800\n",
      "tensor(-134.1229, dtype=torch.float64)\n",
      "acceptance rate:  0.8\n",
      "3900\n",
      "tensor(-108.0939, dtype=torch.float64)\n",
      "acceptance rate:  0.8\n",
      "4000\n",
      "tensor(-134.3559, dtype=torch.float64)\n",
      "acceptance rate:  0.75\n",
      "4100\n",
      "tensor(-94.3328, dtype=torch.float64)\n",
      "acceptance rate:  0.83\n",
      "4200\n",
      "tensor(-114.6390, dtype=torch.float64)\n",
      "acceptance rate:  0.73\n",
      "4300\n",
      "tensor(-119.5102, dtype=torch.float64)\n",
      "acceptance rate:  0.84\n",
      "4400\n",
      "tensor(-120.6527, dtype=torch.float64)\n",
      "acceptance rate:  0.92\n",
      "4500\n",
      "tensor(-101.5884, dtype=torch.float64)\n",
      "acceptance rate:  0.81\n",
      "4600\n",
      "tensor(-98.6515, dtype=torch.float64)\n",
      "acceptance rate:  0.85\n",
      "4700\n",
      "tensor(-124.1744, dtype=torch.float64)\n",
      "acceptance rate:  0.85\n",
      "4800\n",
      "tensor(-131.6120, dtype=torch.float64)\n",
      "acceptance rate:  0.79\n",
      "4900\n",
      "tensor(-107.1973, dtype=torch.float64)\n",
      "acceptance rate:  0.83\n",
      "5000\n",
      "tensor(-105.4131, dtype=torch.float64)\n",
      "acceptance rate:  0.82\n",
      "5100\n",
      "tensor(-129.2246, dtype=torch.float64)\n",
      "acceptance rate:  0.81\n",
      "5200\n",
      "tensor(-128.2560, dtype=torch.float64)\n",
      "acceptance rate:  0.83\n",
      "5300\n",
      "tensor(-113.7397, dtype=torch.float64)\n",
      "acceptance rate:  0.81\n",
      "5400\n",
      "tensor(-103.1288, dtype=torch.float64)\n",
      "acceptance rate:  0.79\n",
      "5500\n",
      "tensor(-120.3855, dtype=torch.float64)\n",
      "acceptance rate:  0.81\n",
      "5600\n",
      "tensor(-163.9184, dtype=torch.float64)\n",
      "acceptance rate:  0.79\n",
      "5700\n",
      "tensor(-125.3457, dtype=torch.float64)\n",
      "acceptance rate:  0.81\n",
      "5800\n",
      "tensor(-146.8133, dtype=torch.float64)\n",
      "acceptance rate:  0.84\n",
      "5900\n",
      "tensor(-114.2964, dtype=torch.float64)\n",
      "acceptance rate:  0.83\n",
      "6000\n",
      "tensor(-121.3732, dtype=torch.float64)\n",
      "acceptance rate:  0.87\n",
      "6100\n",
      "tensor(-88.9456, dtype=torch.float64)\n",
      "acceptance rate:  0.85\n",
      "6200\n",
      "tensor(-89.9973, dtype=torch.float64)\n",
      "acceptance rate:  0.77\n",
      "6300\n",
      "tensor(-114.3983, dtype=torch.float64)\n",
      "acceptance rate:  0.76\n",
      "6400\n",
      "tensor(-123.4922, dtype=torch.float64)\n",
      "acceptance rate:  0.78\n",
      "6500\n",
      "tensor(-104.7920, dtype=torch.float64)\n",
      "acceptance rate:  0.79\n",
      "6600\n",
      "tensor(-117.2302, dtype=torch.float64)\n",
      "acceptance rate:  0.78\n",
      "6700\n",
      "tensor(-127.5056, dtype=torch.float64)\n",
      "acceptance rate:  0.8\n",
      "6800\n",
      "tensor(-95.1915, dtype=torch.float64)\n",
      "acceptance rate:  0.69\n",
      "6900\n",
      "tensor(-126.6338, dtype=torch.float64)\n",
      "acceptance rate:  0.79\n",
      "7000\n",
      "tensor(-126.6705, dtype=torch.float64)\n",
      "acceptance rate:  0.82\n",
      "7100\n",
      "tensor(-116.6312, dtype=torch.float64)\n",
      "acceptance rate:  0.85\n",
      "7200\n",
      "tensor(-105.5820, dtype=torch.float64)\n",
      "acceptance rate:  0.87\n",
      "7300\n",
      "tensor(-87.3769, dtype=torch.float64)\n",
      "acceptance rate:  0.77\n",
      "7400\n",
      "tensor(-108.2182, dtype=torch.float64)\n",
      "acceptance rate:  0.84\n",
      "7500\n",
      "tensor(-84.6711, dtype=torch.float64)\n",
      "acceptance rate:  0.77\n",
      "7600\n",
      "tensor(-107.7303, dtype=torch.float64)\n",
      "acceptance rate:  0.8\n",
      "7700\n",
      "tensor(-113.7569, dtype=torch.float64)\n",
      "acceptance rate:  0.83\n",
      "7800\n",
      "tensor(-111.4416, dtype=torch.float64)\n",
      "acceptance rate:  0.83\n",
      "7900\n",
      "tensor(-91.5782, dtype=torch.float64)\n",
      "acceptance rate:  0.85\n"
     ]
    }
   ],
   "source": [
    "def vectorize(xlatent, theta, sigma, time_constant_param_ls):\n",
    "    t1 = torch.reshape(xlatent.detach(), (-1,))\n",
    "    t2 = torch.reshape(theta.detach(), (-1,))\n",
    "    t3 = torch.reshape(sigma.detach(), (-1,))\n",
    "    long_vec = torch.cat((t1, t2, t3))\n",
    "    for i in range(len(time_constant_param_ls)):\n",
    "        long_vec = torch.cat((long_vec, time_constant_param_ls[i].detach()))\n",
    "    return long_vec\n",
    "\n",
    "def get_dim(tensor_shape):\n",
    "    if len(tensor_shape) == 0:\n",
    "        return 1\n",
    "    if len(tensor_shape) == 1:\n",
    "        return tensor_shape[0]\n",
    "    dim = 1\n",
    "    for i in range(len(tensor_shape)):\n",
    "        dim *= tensor_shape[i]\n",
    "    return dim\n",
    "    \n",
    "def devectorize(long_tensor, xlatent_shape, theta_shape, sigma_shape, time_constant_param_dim):\n",
    "    x_latent_dim = get_dim(xlatent_shape)\n",
    "    theta_dim = get_dim(theta_shape)\n",
    "    sigma_dim = get_dim(sigma_shape)\n",
    "    time_constant_param_ls = []\n",
    "    xlatent = torch.reshape(long_tensor[:x_latent_dim],xlatent_shape)\n",
    "    theta = torch.reshape(long_tensor[x_latent_dim:x_latent_dim + theta_dim],theta_shape)\n",
    "    sigma = torch.reshape(long_tensor[x_latent_dim + theta_dim:x_latent_dim + theta_dim + sigma_dim],sigma_shape)\n",
    "    for each in range(x_latent_dim + theta_dim + sigma_dim, long_tensor.shape[0]):\n",
    "        time_constant_param_ls.append(torch.tensor([long_tensor[each]]))\n",
    "    return xlatent, theta, sigma, time_constant_param_ls\n",
    "\n",
    "\n",
    "def NegLogLikelihood(xlatent, theta, sigma, time_constant_param_ls, \n",
    "                     inferred_theta = inferred_theta, \n",
    "                     ydata = ydata, \n",
    "                     CovAllDimensionsPyList = CovAllDimensionsPyList, \n",
    "                     fOdeTorch = fOdeTorch,\n",
    "                     priorTemperature = priorTemperature, \n",
    "                     KinvthetaList = KinvthetaList):\n",
    "    # length of observed y (t)\n",
    "    n = ydata.shape[0]\n",
    "    pdimension = ydata.shape[1]\n",
    "    thetadimension = theta.shape[1]\n",
    "    sigmaSq = torch.pow(sigma, 2)\n",
    "    fderiv = fOdeTorch(theta, xlatent, time_constant_param_ls)\n",
    "    res = torch.zeros([pdimension, 3]).double()\n",
    "    res_theta = torch.zeros(thetadimension).double()\n",
    "    fitDerivError = torch.zeros([n, pdimension]).double()\n",
    "    nobs = torch.zeros([pdimension]).double()\n",
    "    fitLevelErrorSumSq = torch.zeros([pdimension]).double()\n",
    "    for vEachDim in range(pdimension):\n",
    "        fitDerivError[:, vEachDim] = fderiv[:, vEachDim]\n",
    "        fitDerivError[:, vEachDim] -= torch.sparse.mm(CovAllDimensionsPyList[vEachDim]['mphi'], xlatent[:, vEachDim].reshape(-1, 1))[:, 0]\n",
    "        nobs[vEachDim] = torch.sum(torch.isfinite(ydata[:, vEachDim]))\n",
    "        obsIdx = torch.isfinite(ydata[:, vEachDim])\n",
    "        fitLevelErrorSumSq[vEachDim] = torch.sum(torch.square(xlatent[obsIdx, vEachDim] - ydata[obsIdx, vEachDim]))\n",
    "    res[:, 0] = -0.5 * fitLevelErrorSumSq / sigmaSq - torch.log(sigma + 0.0001) * nobs\n",
    "    res[:, 0] /= priorTemperature[2]\n",
    "    KinvfitDerivError = torch.zeros([n, pdimension]).double()\n",
    "    CinvX = torch.zeros([n, pdimension]).double()\n",
    "    for vEachDim in range(pdimension):\n",
    "        # inverse of K\n",
    "        KinvfitDerivError[:, vEachDim] = torch.sparse.mm(CovAllDimensionsPyList[vEachDim]['Kinv'], fitDerivError[:, vEachDim].reshape(-1, 1))[:, 0]\n",
    "        # inverse of Cd\n",
    "        CinvX[:, vEachDim] = torch.sparse.mm(CovAllDimensionsPyList[vEachDim]['Cinv'], xlatent[:, vEachDim].reshape(-1, 1))[:, 0]\n",
    "    for thetaEachDim in range(thetadimension):\n",
    "        res_theta[thetaEachDim] = -0.5 * torch.sum(\n",
    "            (theta[:, thetaEachDim] - inferred_theta[thetaEachDim]) @ torch.sparse.mm(KinvthetaList[thetaEachDim], (theta[:, thetaEachDim] - inferred_theta[thetaEachDim]).reshape(-1, 1))[:, 0])\n",
    "    res[:, 1] = -0.5 * torch.sum(fitDerivError * KinvfitDerivError, dim=0) / priorTemperature[0]\n",
    "    res[:, 2] = -0.5 * torch.sum(xlatent * CinvX, dim=0) / priorTemperature[1]\n",
    "\n",
    "    return -(torch.sum(res) + torch.sum(res_theta))\n",
    "\n",
    "class HMC:\n",
    "    def __init__(self, negllik, all_theta, xlatent_shape, theta_shape, sigma_shape, time_constant_param_ls, lsteps=50, epsilon=1e-5, n_samples=4000, upper_bound = None, lower_bound = None, burn_in_ratio = 0.5):\n",
    "        self.all_theta = all_theta\n",
    "        self.theta_shape = theta_shape\n",
    "        self.xlatent_shape = xlatent_shape\n",
    "        self.sigma_shape = sigma_shape\n",
    "        self.constant_dim = len(time_constant_param_ls)\n",
    "        self.lsteps = lsteps\n",
    "        self.epsilon = epsilon * torch.ones(all_theta.shape)\n",
    "        self.burn_in_ratio = burn_in_ratio\n",
    "        self.n_samples = n_samples\n",
    "        self.total_samples = int(n_samples / (1 - burn_in_ratio))\n",
    "        self.NegLogLikelihood = negllik\n",
    "        self.ub = upper_bound\n",
    "        if upper_bound is not None:\n",
    "            if upper_bound.shape[0] != all_theta.shape[0]:\n",
    "                raise ValueError\n",
    "        self.lb = lower_bound\n",
    "        if lower_bound is not None:\n",
    "            if lower_bound.shape[0] != all_theta.shape[0]:\n",
    "                raise ValueError\n",
    "    \n",
    "    def NegLogLikelihood_vec(self, all_theta):\n",
    "        xlatent_0, theta_0, sigma_0, constant_param_ls_0 = devectorize(all_theta, self.xlatent_shape, self.theta_shape, self.sigma_shape, self.constant_dim)\n",
    "        return NegLogLikelihood(xlatent_0, theta_0, sigma_0, constant_param_ls_0)\n",
    "    \n",
    "    def Nabla(self, theta_torch):\n",
    "        theta_torch = theta_torch.detach()\n",
    "        xlatent, theta, sigma, constant_param_ls = devectorize(theta_torch, self.xlatent_shape, self.theta_shape, self.sigma_shape, self.constant_dim)\n",
    "        xlatent.requires_grad = True  \n",
    "        theta.requires_grad = True\n",
    "        sigma.requires_grad = True\n",
    "        for each in constant_param_ls:\n",
    "            each.requires_grad = True                      \n",
    "        llik = self.NegLogLikelihood(xlatent, theta, sigma, constant_param_ls)\n",
    "        llik.backward()\n",
    "        constant_param_deriv_ls = []\n",
    "        for each in constant_param_ls:\n",
    "            constant_param_deriv_ls.append(each.grad)\n",
    "        v = vectorize(xlatent.grad, theta.grad, sigma.grad, constant_param_deriv_ls)\n",
    "\n",
    "        return v\n",
    "    def sample(self, all_theta, TV_theta_mean, ydata, CovAllDimensionsPyList, fOdeTorch, priorTemperature, KinvthetaList):\n",
    "        def bounce(m, lb, ub):\n",
    "            if lb is None and ub is None:\n",
    "                return m\n",
    "            if lb is None:\n",
    "                max_tensor = torch.clamp(m - ub, min=0)\n",
    "                return m - 2 * max_tensor\n",
    "            if ub is None:\n",
    "                min_tensor = torch.clamp(lb - m, min=0)\n",
    "                return m + 2 * min_tensor\n",
    "            if torch.sum(lb < ub) < m.shape[0]:\n",
    "                raise ValueError\n",
    "            if torch.sum(m >= lb) == m.shape[0] and torch.sum(m <= ub) == m.shape[0]:\n",
    "                return m\n",
    "            if torch.sum(m >= lb) < m.shape[0]:\n",
    "                min_tensor = torch.clamp(lb - m, min=0)\n",
    "                return bounce(m + 2 * min_tensor, lb, ub)\n",
    "            if torch.sum(m <= ub) < m.shape[0]:\n",
    "                max_tensor = torch.clamp(m - ub, min=0)\n",
    "                return bounce(m - 2 * max_tensor, lb, ub)\n",
    "\n",
    "        trace_val = np.zeros(self.total_samples)\n",
    "        samples = np.zeros((self.total_samples, self.all_theta.shape[0]))\n",
    "        random_ls = np.random.uniform(0, 1, self.total_samples)\n",
    "        acceptance_ls = np.zeros(self.total_samples)\n",
    "        nan_ls = np.zeros(self.total_samples)\n",
    "        cur_theta = self.all_theta.clone().detach()\n",
    "        for EachIter in range(self.total_samples): ############\n",
    "            cur_nllik_1 = self.NegLogLikelihood_vec(cur_theta).detach()\n",
    "            rstep = torch.rand(self.epsilon.shape) * self.epsilon + self.epsilon\n",
    "            p = torch.normal(mean=0., std=torch.ones(self.all_theta.shape))\n",
    "            cur_p = p.clone()\n",
    "            theta = cur_theta.clone()         \n",
    "            p = p - rstep * self.Nabla(theta).clone() / 2\n",
    "            for i in range(self.lsteps):\n",
    "                theta = theta + rstep * p\n",
    "                nabla_torch = self.Nabla(theta).clone()\n",
    "                p = p - rstep * nabla_torch\n",
    "                theta = bounce(theta, self.lb, self.ub)\n",
    "            \n",
    "            p = p - rstep * self.Nabla(theta).clone() / 2\n",
    "            \n",
    "            new_nllik = self.NegLogLikelihood_vec(theta)\n",
    "            new_p = 0.5 * torch.sum(torch.square(p))\n",
    "            new_H = new_nllik + new_p\n",
    "            cur_nllik = self.NegLogLikelihood_vec(cur_theta).detach()\n",
    "            cur_H = cur_nllik + 0.5 * torch.sum(torch.square(cur_p))\n",
    "#             print(new_H, cur_H)\n",
    "            \n",
    "            if torch.isnan(theta[0]) or torch.isnan(new_H):\n",
    "                samples[EachIter] = cur_theta.clone()\n",
    "                nan_ls[EachIter] = 1\n",
    "                self.epsilon *= 0.9\n",
    "                print('NaN!')\n",
    "            else:\n",
    "                # accept\n",
    "                tmp = float(torch.exp(cur_H - new_H))\n",
    "#                 print(tmp)\n",
    "                if  tmp > random_ls[EachIter]:\n",
    "                    samples[EachIter] = theta.clone()\n",
    "                    cur_theta = theta.clone()\n",
    "                    acceptance_ls[EachIter] = 1\n",
    "                # reject\n",
    "                else:\n",
    "                    samples[EachIter] = cur_theta.clone()\n",
    "                    \n",
    "            trace_val[EachIter] = self.NegLogLikelihood_vec(cur_theta).item()        \n",
    "\n",
    "            if EachIter > 200 and EachIter < self.total_samples - self.n_samples:\n",
    "                if np.sum(acceptance_ls[EachIter - 100 : EachIter]) < 60:\n",
    "                    # decrease epsilon\n",
    "                    self.epsilon *= 0.995\n",
    "                if np.sum(acceptance_ls[EachIter - 100 : EachIter]) > 90:\n",
    "                    # increase epsilon\n",
    "                    self.epsilon *= 1.005\n",
    "            if EachIter % 100 == 0 and EachIter > 100:\n",
    "                print(EachIter)\n",
    "                print(cur_nllik)\n",
    "                print('acceptance rate: ', np.sum(acceptance_ls[EachIter - 100 : EachIter]) / 100)\n",
    "                if EachIter < self.total_samples - self.n_samples:\n",
    "                    standard_deviation = torch.tensor(np.std(samples[EachIter - 100:EachIter, :], axis = 0))\n",
    "                    if torch.mean(standard_deviation) > 1e-6:\n",
    "                        self.epsilon = 0.05 * standard_deviation * torch.mean(self.epsilon) / torch.mean(standard_deviation) + 0.95 * self.epsilon\n",
    "        return samples, acceptance_ls, trace_val, nan_ls # [self.total_samples-self.n_samples:, :]\n",
    "\n",
    "all_theta_TVMAGI = vectorize(TVMAGI_xlatent_torch, TVMAGI_theta_torch, TVMAGI_sigma_torch, time_constant_param_ls)\n",
    "# all_theta_true = vectorize(true_x_torch, theta_true_torch, TVMAGI_sigma_torch, time_constant_param_ls)\n",
    "all_theta_pointwise = vectorize(pointwise_xlatent_torch, pointwise_theta_torch, TVMAGI_sigma_torch, time_constant_param_ls)\n",
    "sampler = HMC(NegLogLikelihood, all_theta_TVMAGI, \n",
    "              pointwise_xlatent_torch.shape,\n",
    "              pointwise_theta_torch.shape, \n",
    "              TVMAGI_sigma_torch.shape,\n",
    "              time_constant_param_ls, \n",
    "              lower_bound = torch.zeros(all_theta_pointwise.shape))\n",
    "# sampler.Nabla(all_theta)\n",
    "# lower_bound = torch.zeros(all_theta_pointwise.shape)\n",
    "samples, b, c, d = sampler.sample(all_theta_TVMAGI, TV_theta_mean, ydata, CovAllDimensionsPyList, fOdeTorch, priorTemperature, KinvthetaList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "moving-bicycle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, '$p^d$ - trained hyperparameter')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "TVMAGI_theta = TVMAGI_theta_torch.detach().numpy()\n",
    "k = samples[4000:, 256:256+192]\n",
    "val = np.zeros((64, 3))\n",
    "for i in range(4000):\n",
    "    for j in range(64):\n",
    "        val[j] = k[i].reshape(-1, 3)[j]\n",
    "    plt.plot(val[:, 0])\n",
    "plt.plot(TVMAGI_theta[:, 0], label='TVMAGI beta')\n",
    "# plt.plot(np.arange(0, 64, 2), true_ve, label='true')\n",
    "plt.legend()\n",
    "plt.title(r'$\\beta$ - trained hyperparameter')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ordinary-defeat",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_value = [1.8, 0.1, 0.1, 0.05]\n",
    "std = [1., 0.02, 0, 0.025]\n",
    "T = 2\n",
    "tmp = np.linspace(0, 2 * T * np.pi, 64)\n",
    "true_beta = parameter_value[0] - std[0] * np.cos(tmp)\n",
    "true_ve = parameter_value[1]- std[1] * np.cos(tmp)\n",
    "true_vi = parameter_value[2] \n",
    "true_pd = parameter_value[3] + std[3] * np.cos(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "funny-extra",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(10, 0.06, '%=0.5625')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "TVMAGI_theta = TVMAGI_theta_torch.detach().numpy()\n",
    "k = samples[4000:, 256:256+192]\n",
    "beta_ls = np.zeros((4000, 64))\n",
    "\n",
    "for i in range(4000):\n",
    "    for j in range(64):\n",
    "        val[j] = k[i].reshape(-1, 3)[j]\n",
    "    beta_ls[i] = val[:, 2]/4\n",
    "    \n",
    "max_beta = np.amax(beta_ls, axis = 0)\n",
    "min_beta = np.min(beta_ls, axis = 0)\n",
    "plt.plot(TVMAGI_theta[:, 2]/4, label='TVMAGI ve')\n",
    "# plt.plot(min_beta, label='min beta')\n",
    "# plt.plot(max_beta, label='max beta')\n",
    "# lower_95 = np.percentile(beta_ls, 100, axis=0)\n",
    "# upper_95 = np.percentile(all_ve, 0, axis=0)\n",
    "plt.fill_between(np.arange(0, 64, 1), min_beta, max_beta, color='grey', alpha = 0.4)\n",
    "plt.scatter(np.arange(0, 64, 1), true_pd, label='True', s=1)\n",
    "# plt.plot(np.arange(0, 64, 2), true_ve, label='true')\n",
    "plt.legend()\n",
    "plt.title(r'$p^d$ - trained hyperparameter')\n",
    "plt.text(10, 0.06, '%=0.5625')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ready-canal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5625"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.minimum(true_pd <= max_beta, true_pd >= min_beta)) / 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portable-combine",
   "metadata": {},
   "source": [
    "# TVMAGI with HMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "flexible-hampton",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.87143191 0.10205595 0.0924365  0.19611035]\n",
      "0 -33.46622826583106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py:132: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  allow_unreachable=True)  # allow_unreachable flag\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 -61.46563253865944\n",
      "400 -65.46867597014563\n",
      "600 -70.30719567185052\n",
      "800 -75.8779406930725\n",
      "1000 -82.14628031770047\n",
      "1200 -89.11858788285758\n",
      "1400 -96.83452296441611\n",
      "1600 -105.37385043792555\n",
      "1800 -114.86939955901603\n",
      "2000 -125.52391045651167\n",
      "2200 -137.63084675785728\n",
      "2400 -151.57108526671408\n",
      "2600 -167.36598236474424\n",
      "2800 -182.12545914942248\n",
      "3000 -191.45635228751482\n",
      "3200 -197.9308097034147\n",
      "3400 -203.6519816645105\n",
      "3600 -209.0308313392814\n",
      "3800 -214.26151423425827\n",
      "4000 -219.5572896632014\n",
      "4200 -225.18761975798978\n",
      "4400 -231.55542682526388\n",
      "4600 -238.9431599926636\n",
      "4800 -245.99614954845265\n",
      "5000 -251.6919028597476\n",
      "5200 -256.94624629263217\n",
      "5400 -262.0846765678946\n",
      "5600 -267.20006527565323\n",
      "5800 -272.33597765371474\n",
      "6000 -277.5218352361838\n",
      "6200 -282.7747847475248\n",
      "6400 -288.0937203044424\n",
      "6600 -293.4700904446499\n",
      "6800 -298.88509143458487\n",
      "7000 -304.37054876995614\n",
      "7200 -309.98225229812874\n",
      "7400 -315.72715841067986\n",
      "7600 -321.2681428789656\n",
      "7800 -326.0010593191453\n",
      "8000 -329.78837822384327\n",
      "8200 -332.74233223820045\n",
      "8400 -335.0024363230434\n",
      "8600 -336.7114557767537\n",
      "8800 -337.9967715604188\n",
      "9000 -338.97326171144164\n",
      "9200 -339.7225517604884\n",
      "9400 -340.3093871413464\n",
      "9600 -340.7717346832153\n",
      "9800 -341.1408218906167\n",
      "10000 -341.432472822683\n",
      "10200 -341.5599332130558\n",
      "10400 -341.6731362343355\n",
      "10600 -341.7752989308383\n",
      "10800 -341.8667280700818\n",
      "11000 -341.94791873481427\n",
      "11200 -342.01914229688305\n",
      "11400 -342.08130036754335\n",
      "11600 -342.13447329669685\n",
      "11800 -342.17986345757635\n",
      "12000 -342.21761985567616\n",
      "12200 -342.2493352138674\n",
      "12400 -342.2747117382854\n",
      "12600 -342.2945978060435\n",
      "12800 -342.31101310300653\n",
      "13000 -342.32337795359365\n",
      "13200 -342.33273868056483\n",
      "13400 -342.3397591318935\n",
      "13600 -342.344655968153\n",
      "13800 -342.3487514475568\n",
      "14000 -342.3512357735412\n",
      "14200 -342.35308456554554\n",
      "14400 -342.3547810188786\n",
      "14600 -342.35573119143754\n",
      "14800 -342.35638748773994\n",
      "15000 -342.35689255011545\n",
      "15200 -342.35741661748114\n",
      "15400 -342.35752226409403\n",
      "15600 -342.3575904001705\n",
      "15800 -342.3576785348457\n",
      "16000 -342.3577331388785\n",
      "16200 -342.35776596641176\n",
      "16400 -342.3577891156985\n",
      "16600 -342.3578136076236\n",
      "16800 -342.35750100346036\n",
      "17000 -342.3577453925567\n",
      "17200 -342.3576950130128\n",
      "17400 -342.35796295968925\n",
      "17600 -342.35780766009515\n",
      "17800 -342.3578033438474\n",
      "18000 -342.35788965690506\n",
      "18200 -342.3576989848165\n",
      "18400 -342.35778543840865\n",
      "18600 -342.35794628951953\n",
      "18800 -342.35745676872386\n",
      "19000 -342.35755186200834\n",
      "19200 -342.357860204139\n",
      "19400 -342.3575510787878\n",
      "19600 -342.35770570957953\n",
      "19800 -342.3574818629818\n",
      "20000 -342.35673456477184\n",
      "20200 -342.3580900987602\n",
      "20400 -342.3580898653592\n",
      "20600 -342.35807983783474\n",
      "20800 -342.3580707639342\n",
      "21000 -342.35806981540463\n",
      "21200 -342.35807109117604\n",
      "21400 -342.35802681202574\n",
      "21600 -342.3578813643657\n",
      "21800 -342.3579450774761\n",
      "22000 -342.35804251604776\n",
      "22200 -342.3580517796214\n",
      "22400 -342.35802556024896\n",
      "22600 -342.3579995893484\n",
      "22800 -342.35804161805555\n",
      "23000 -342.3580260360532\n",
      "23200 -342.3579489418417\n",
      "23400 -342.35803309965297\n",
      "23600 -342.3580439557304\n",
      "23800 -342.35804420276634\n",
      "24000 -342.3580359203186\n",
      "24200 -342.3578448936381\n",
      "24400 -342.35803228527107\n",
      "24600 -342.3580033069654\n",
      "24800 -342.35799106478976\n",
      "25000 -342.3580588145443\n",
      "25200 -342.3580312386739\n",
      "25400 -342.3580175430024\n",
      "25600 -342.3580560945643\n",
      "25800 -342.35791264773906\n",
      "26000 -342.35806563653244\n",
      "26200 -342.35802201661886\n",
      "26400 -342.35804665256285\n",
      "26600 -342.3579898630657\n",
      "26800 -342.3580314711889\n",
      "27000 -342.357975231447\n",
      "27200 -342.3580639237042\n",
      "27400 -342.3580171752264\n",
      "27600 -342.358005843772\n",
      "27800 -342.358016245552\n",
      "28000 -342.35803307148177\n",
      "28200 -342.357953484572\n",
      "28400 -342.35802513192573\n",
      "28600 -342.358038884357\n",
      "28800 -342.3579487890685\n",
      "29000 -342.35790990990637\n",
      "29200 -342.35803610016154\n",
      "29400 -342.35797724075525\n",
      "29600 -342.3580261714343\n",
      "29800 -342.3580323350543\n",
      "30000 -342.35799394887704\n",
      "30200 -342.3580919152471\n",
      "30400 -342.35809191930855\n",
      "30600 -342.3580919202619\n",
      "30800 -342.3580919185044\n",
      "31000 -342.35809189855416\n",
      "31200 -342.35809123069504\n",
      "31400 -342.35809032148467\n",
      "31600 -342.35805218263175\n",
      "31800 -342.3580820392709\n",
      "32000 -342.35805462307303\n",
      "32200 -342.35808120825726\n",
      "32400 -342.3580754463838\n",
      "32600 -342.3580751658538\n",
      "32800 -342.35807323735673\n",
      "33000 -342.3580751175106\n",
      "33200 -342.3580671461252\n",
      "33400 -342.3580830215078\n",
      "33600 -342.3580767436487\n",
      "33800 -342.35807258714055\n",
      "34000 -342.3580844854069\n",
      "34200 -342.3580563199304\n",
      "34400 -342.35807559360484\n",
      "34600 -342.3580612557484\n",
      "34800 -342.3580812672743\n",
      "35000 -342.35805983390424\n",
      "35200 -342.3580853846412\n",
      "35400 -342.35806053757005\n",
      "35600 -342.35805506941483\n",
      "35800 -342.3580764242717\n",
      "36000 -342.3580566262547\n",
      "36200 -342.358055381375\n",
      "36400 -342.35807871671915\n",
      "36600 -342.35808172986896\n",
      "36800 -342.35807136848587\n",
      "37000 -342.3580385322563\n",
      "37200 -342.3580813737105\n",
      "37400 -342.35803900975424\n",
      "37600 -342.3580747286471\n",
      "37800 -342.3580770891628\n",
      "38000 -342.3580879047108\n",
      "38200 -342.35804488264006\n",
      "38400 -342.35804275957116\n",
      "38600 -342.358069700506\n",
      "38800 -342.3580836157221\n",
      "39000 -342.3580562022509\n",
      "39200 -342.35802733042027\n",
      "39400 -342.35806395564725\n",
      "39600 -342.3580692297127\n",
      "39800 -342.3580684793937\n",
      "40000 -342.3580807735489\n",
      "40200 -342.3580919177176\n",
      "40400 -342.3580919143747\n",
      "40600 -342.358091921841\n",
      "40800 -342.3580919143446\n",
      "41000 -342.3580919191926\n",
      "41200 -342.3580919130283\n",
      "41400 -342.3580919166294\n",
      "41600 -342.3580906316793\n",
      "41800 -342.35809148849097\n",
      "42000 -342.3580903015786\n",
      "42200 -342.3580883670907\n",
      "42400 -342.35808588272994\n",
      "42600 -342.3580886066448\n",
      "42800 -342.358088438894\n",
      "43000 -342.3580744198634\n",
      "43200 -342.3580863531203\n",
      "43400 -342.35808224376274\n",
      "43600 -342.35809104827445\n",
      "43800 -342.35808934021344\n",
      "44000 -342.35809101194053\n",
      "44200 -342.3580858449236\n",
      "44400 -342.35808939601037\n",
      "44600 -342.35808579493977\n",
      "44800 -342.35809036151494\n",
      "45000 -342.3580902242361\n",
      "45200 -342.35808858610534\n",
      "45400 -342.3580817425371\n",
      "45600 -342.3580903843549\n",
      "45800 -342.35808832723103\n",
      "46000 -342.358078382581\n",
      "46200 -342.35809047145585\n",
      "46400 -342.35808539113617\n",
      "46600 -342.3580906473741\n",
      "46800 -342.35809121262133\n",
      "47000 -342.35808641414553\n",
      "47200 -342.35809147596456\n",
      "47400 -342.35808243587303\n",
      "47600 -342.35808652051213\n",
      "47800 -342.35808718441814\n",
      "48000 -342.35809101052985\n",
      "48200 -342.35808276134514\n",
      "48400 -342.35808919008787\n",
      "48600 -342.35808918739247\n",
      "48800 -342.3580865971425\n",
      "49000 -342.35808864674635\n",
      "49200 -342.35807980743544\n",
      "49400 -342.35808966973366\n",
      "49600 -342.3580896100523\n",
      "49800 -342.3580870474196\n",
      "50000 -342.3580887185846\n",
      "50200 -342.3580919117945\n",
      "50400 -342.35809191899216\n",
      "50600 -342.35809191640055\n",
      "50800 -342.35809191830106\n",
      "51000 -342.3580919117077\n",
      "51200 -342.3580919141636\n",
      "51400 -342.35809191392457\n",
      "51600 -342.35809148513215\n",
      "51800 -342.358089682838\n",
      "52000 -342.3580918319819\n",
      "52200 -342.35809169721506\n",
      "52400 -342.3580917198183\n",
      "52600 -342.3580915804657\n",
      "52800 -342.3580899545602\n",
      "53000 -342.35809108705405\n",
      "53200 -342.35809139070835\n",
      "53400 -342.35809119991825\n",
      "53600 -342.3580905813944\n",
      "53800 -342.3580905523524\n",
      "54000 -342.3580917443159\n",
      "54200 -342.3580909824341\n",
      "54400 -342.35809089521\n",
      "54600 -342.35809108467856\n",
      "54800 -342.3580916590263\n",
      "55000 -342.358091121372\n",
      "55200 -342.35809157524824\n",
      "55400 -342.35809118330724\n",
      "55600 -342.35809149081973\n",
      "55800 -342.35809077502887\n",
      "56000 -342.35809092190215\n",
      "56200 -342.3580907034643\n",
      "56400 -342.35809164731097\n",
      "56600 -342.3580884060699\n",
      "56800 -342.3580917111666\n",
      "57000 -342.3580907390846\n",
      "57200 -342.35809086820086\n",
      "57400 -342.3580916355104\n",
      "57600 -342.3580915728815\n",
      "57800 -342.35808960289745\n",
      "58000 -342.3580918290424\n",
      "58200 -342.3580885447139\n",
      "58400 -342.3580917661774\n",
      "58600 -342.3580856718825\n",
      "58800 -342.3580904668123\n",
      "59000 -342.35809057890947\n",
      "59200 -342.35809127413904\n",
      "59400 -342.35809050530787\n",
      "59600 -342.35809137920006\n",
      "59800 -342.3580893753422\n",
      "60000 -342.35809035358\n",
      "60200 -342.3580919121203\n",
      "60400 -342.35809191723774\n",
      "60600 -342.3580919181397\n",
      "60800 -342.3580919184721\n",
      "61000 -342.3580919159571\n",
      "61200 -342.3580919154846\n",
      "61400 -342.3580919189344\n",
      "61600 -342.3580919118746\n",
      "61800 -342.35809189018875\n",
      "62000 -342.3580918784733\n",
      "62200 -342.3580916064556\n",
      "62400 -342.35809126578675\n",
      "62600 -342.3580917749659\n",
      "62800 -342.3580918541429\n",
      "63000 -342.3580918267023\n",
      "63200 -342.3580915597983\n",
      "63400 -342.3580917032814\n",
      "63600 -342.3580917077685\n",
      "63800 -342.35809178491553\n",
      "64000 -342.35809161462186\n",
      "64200 -342.3580918357093\n",
      "64400 -342.3580918623022\n",
      "64600 -342.3580918320033\n",
      "64800 -342.35809181969677\n",
      "65000 -342.35809169219976\n",
      "65200 -342.3580915849459\n",
      "65400 -342.35809159411934\n",
      "65600 -342.35809170233665\n",
      "65800 -342.35809177008457\n",
      "66000 -342.358091847622\n",
      "66200 -342.3580917945288\n",
      "66400 -342.3580916726456\n",
      "66600 -342.3580918596933\n",
      "66800 -342.3580918115992\n",
      "67000 -342.35809183803144\n",
      "67200 -342.3580918352246\n",
      "67400 -342.35809134493144\n",
      "67600 -342.3580917734958\n",
      "67800 -342.358091747124\n",
      "68000 -342.35809182509684\n",
      "68200 -342.3580917920475\n",
      "68400 -342.35809176335346\n",
      "68600 -342.35809181575786\n",
      "68800 -342.3580918028483\n",
      "69000 -342.3580919002009\n",
      "69200 -342.3580915809931\n",
      "69400 -342.35809183930536\n",
      "69600 -342.3580916259511\n",
      "69800 -342.35809165755813\n",
      "70000 -342.35809153771316\n",
      "70200 -342.35809191387585\n",
      "70400 -342.35809191616335\n",
      "70600 -342.3580919188049\n",
      "70800 -342.35809191082075\n",
      "71000 -342.3580919141594\n",
      "71200 -342.35809191768305\n",
      "71400 -342.3580919126026\n",
      "71600 -342.3580919120421\n",
      "71800 -342.35809189498787\n",
      "72000 -342.35809187492663\n",
      "72200 -342.35809188355665\n",
      "72400 -342.35809180488707\n",
      "72600 -342.35809190871447\n",
      "72800 -342.3580918525813\n",
      "73000 -342.3580918005846\n",
      "73200 -342.3580918987376\n",
      "73400 -342.35809187332046\n",
      "73600 -342.3580919104027\n",
      "73800 -342.3580918783407\n",
      "74000 -342.3580918613205\n",
      "74200 -342.3580918867211\n",
      "74400 -342.35809188759947\n",
      "74600 -342.3580918921017\n",
      "74800 -342.358091908253\n",
      "75000 -342.35809186623175\n",
      "75200 -342.3580918766086\n",
      "75400 -342.3580918694973\n",
      "75600 -342.35809189036706\n",
      "75800 -342.3580919064195\n",
      "76000 -342.3580918581878\n",
      "76200 -342.358091892178\n",
      "76400 -342.35809189269804\n",
      "76600 -342.3580918244375\n",
      "76800 -342.35809191110263\n",
      "77000 -342.3580918915574\n",
      "77200 -342.3580917920858\n",
      "77400 -342.35809180092684\n",
      "77600 -342.35809189525565\n",
      "77800 -342.3580918572929\n",
      "78000 -342.35809179489036\n",
      "78200 -342.35809182461446\n",
      "78400 -342.3580918883905\n",
      "78600 -342.35809175126116\n",
      "78800 -342.35809171501865\n",
      "79000 -342.3580915758631\n",
      "79200 -342.35809163125\n",
      "79400 -342.35809185928537\n",
      "79600 -342.35809189477993\n",
      "79800 -342.35809189447764\n",
      "80000 -342.3580918540753\n",
      "80200 -342.3580919129703\n",
      "80400 -342.3580919230408\n",
      "80600 -342.35809191713065\n",
      "80800 -342.35809191233\n",
      "81000 -342.3580919147591\n",
      "81200 -342.35809191262763\n",
      "81400 -342.3580919129067\n",
      "81600 -342.3580919108693\n",
      "81800 -342.35809190283635\n",
      "82000 -342.35809191421544\n",
      "82200 -342.35809191180596\n",
      "82400 -342.35809190628464\n",
      "82600 -342.35809190556984\n",
      "82800 -342.35809191109263\n",
      "83000 -342.35809192035356\n",
      "83200 -342.35809186261696\n",
      "83400 -342.35809191180084\n",
      "83600 -342.35809190342553\n",
      "83800 -342.3580919021306\n",
      "84000 -342.3580919113601\n",
      "84200 -342.3580919047395\n",
      "84400 -342.35809191129243\n",
      "84600 -342.3580919053318\n",
      "84800 -342.3580918745999\n",
      "85000 -342.3580919144352\n",
      "85200 -342.3580919125075\n",
      "85400 -342.3580918965218\n",
      "85600 -342.3580919103326\n",
      "85800 -342.35809191042836\n",
      "86000 -342.3580919086011\n",
      "86200 -342.35809184986545\n",
      "86400 -342.35809185308244\n",
      "86600 -342.3580919120482\n",
      "86800 -342.3580919071831\n",
      "87000 -342.35809189954995\n",
      "87200 -342.35809190729526\n",
      "87400 -342.35809191755027\n",
      "87600 -342.35809191030546\n",
      "87800 -342.35809187306137\n",
      "88000 -342.35809190872794\n",
      "88200 -342.35809189981495\n",
      "88400 -342.3580919152883\n",
      "88600 -342.35809189676945\n",
      "88800 -342.35809191056956\n",
      "89000 -342.3580919139549\n",
      "89200 -342.35809191460714\n",
      "89400 -342.3580918847881\n",
      "89600 -342.3580918864387\n",
      "89800 -342.3580918934227\n",
      "90000 -342.3580919005972\n",
      "90200 -342.35809191296414\n",
      "90400 -342.35809191496395\n",
      "90600 -342.35809191400017\n",
      "90800 -342.3580919168236\n",
      "91000 -342.35809191655164\n",
      "91200 -342.3580919170757\n",
      "91400 -342.35809191282965\n",
      "91600 -342.35809190874966\n",
      "91800 -342.35809191862234\n",
      "92000 -342.35809191473686\n",
      "92200 -342.3580919132092\n",
      "92400 -342.35809189654907\n",
      "92600 -342.3580919153567\n",
      "92800 -342.3580919113533\n",
      "93000 -342.35809191339695\n",
      "93200 -342.35809191464557\n",
      "93400 -342.3580919150507\n",
      "93600 -342.3580919142598\n",
      "93800 -342.3580919156026\n",
      "94000 -342.3580919166453\n",
      "94200 -342.3580919173976\n",
      "94400 -342.3580919107901\n",
      "94600 -342.3580919115506\n",
      "94800 -342.35809191185785\n",
      "95000 -342.35809191581654\n",
      "95200 -342.3580919115294\n",
      "95400 -342.3580919179942\n",
      "95600 -342.35809191752776\n",
      "95800 -342.35809191247665\n",
      "96000 -342.3580919156395\n",
      "96200 -342.35809191517706\n",
      "96400 -342.3580919093701\n",
      "96600 -342.358091913971\n",
      "96800 -342.3580919119923\n",
      "97000 -342.3580919127261\n",
      "97200 -342.3580919117357\n",
      "97400 -342.35809191478364\n",
      "97600 -342.35809191447083\n",
      "97800 -342.35809191484435\n",
      "98000 -342.35809190974123\n",
      "98200 -342.35809191432173\n",
      "98400 -342.3580919095989\n",
      "98600 -342.35809191302167\n",
      "98800 -342.35809191992143\n",
      "99000 -342.358091910717\n",
      "99200 -342.35809190476203\n",
      "99400 -342.3580919153469\n",
      "99600 -342.3580919174\n",
      "99800 -342.35809191607\n",
      "0 -8.198578591917105\n",
      "100 -12.659334901164108\n",
      "200 -17.48769003536072\n",
      "300 -22.118914781023257\n",
      "400 -26.597072223047462\n",
      "500 -30.9528124892354\n",
      "600 -34.862914154559796\n",
      "700 -38.47249945284518\n",
      "800 -41.87243132126645\n",
      "900 -45.03993734111026\n",
      "1000 -47.952135725396374\n",
      "1100 -50.709919294304704\n",
      "1200 -53.24447310195863\n",
      "1300 -55.4496812476972\n",
      "1400 -57.77471981084649\n",
      "1500 -59.60152514921401\n",
      "1600 -61.32429655466299\n",
      "1700 -63.268164856226605\n",
      "1800 -63.710052432416774\n",
      "1900 -66.09478688448101\n",
      "2000 -66.6114062354249\n",
      "2100 -68.23444714173081\n",
      "2200 -69.43253025943336\n",
      "2300 -67.55588483015546\n",
      "2400 -69.1640604852018\n",
      "2500 -70.70804636371375\n",
      "2600 -72.21640352030478\n",
      "2700 -72.26782242789277\n",
      "2800 -69.8151544180742\n",
      "2900 -73.69333142202223\n",
      "3000 -72.9846371120102\n",
      "3100 -72.52705200407806\n",
      "3200 -73.36332284022627\n",
      "3300 -73.56704780854426\n",
      "3400 -72.99205238054914\n",
      "3500 -73.34500586818884\n",
      "3600 -74.80555596412911\n",
      "3700 -74.98464130947266\n",
      "3800 -74.53073333429109\n",
      "3900 -74.64299826274127\n",
      "4000 -73.9761366081623\n",
      "4100 -71.66765194036793\n",
      "4200 -73.20191505269156\n",
      "4300 -75.13755423800195\n",
      "4400 -72.7701158465004\n",
      "4500 -74.74525116538746\n",
      "4600 -75.08158552333968\n",
      "4700 -72.65854920371277\n",
      "4800 -75.22739078252181\n",
      "4900 -74.22213642764528\n",
      "5000 -75.14078840638118\n",
      "5100 -73.97584879852458\n",
      "5200 -75.21446517636139\n",
      "5300 -73.51599594742993\n",
      "5400 -73.79450795452338\n",
      "5500 -74.65615232220914\n",
      "5600 -74.63258939696547\n",
      "5700 -74.7603630745283\n",
      "5800 -74.03853738410243\n",
      "5900 -74.85894278603814\n",
      "6000 -75.21247483789637\n",
      "6100 -75.01808260747063\n",
      "6200 -74.42076990821806\n",
      "6300 -74.51778420482134\n",
      "6400 -73.06345519762078\n",
      "6500 -75.22270169110494\n",
      "6600 -75.22840169386996\n",
      "6700 -74.56955949594628\n",
      "6800 -75.2445640812512\n",
      "6900 -73.44076760806789\n",
      "7000 -75.15847985320441\n",
      "7100 -73.50740375596457\n",
      "7200 -75.02826740381526\n",
      "7300 -74.3841034094389\n",
      "7400 -74.02962290475355\n",
      "7500 -74.44365788310688\n",
      "7600 -75.23389254580286\n",
      "7700 -74.8610811029688\n",
      "7800 -74.36467528866284\n",
      "7900 -75.01118540314681\n",
      "8000 -71.8195795061623\n",
      "8100 -73.08142188285883\n",
      "8200 -72.09089900244723\n",
      "8300 -75.23875418488342\n",
      "8400 -74.69815009221321\n",
      "8500 -75.21320858989691\n",
      "8600 -74.47404143600825\n",
      "8700 -73.5064613386208\n",
      "8800 -75.229929052394\n",
      "8900 -74.39387625968925\n",
      "9000 -75.13224766773311\n",
      "9100 -74.80934909880509\n",
      "9200 -75.14159294234321\n",
      "9300 -75.22392695599461\n",
      "9400 -75.16954305769144\n",
      "9500 -74.65237345912873\n",
      "9600 -74.45522758701985\n",
      "9700 -72.66657626878393\n",
      "9800 -74.53605478450137\n",
      "9900 -74.886902551442\n",
      "10000 -74.84913624828263\n",
      "10100 -75.13946117181631\n",
      "10200 -74.48759094974778\n",
      "10300 -73.43357152576908\n",
      "10400 -74.86443780612758\n",
      "10500 -72.9306378597413\n",
      "10600 -75.14808443666426\n",
      "10700 -74.97773316043045\n",
      "10800 -71.74098623615762\n",
      "10900 -74.74489830709865\n",
      "11000 -74.75108124689454\n",
      "11100 -73.24938621688686\n",
      "11200 -74.67937171247962\n",
      "11300 -74.55788592864494\n",
      "11400 -74.51821060535553\n",
      "11500 -75.13171982413418\n",
      "11600 -73.49856016751639\n",
      "11700 -75.17712993977034\n",
      "11800 -73.75968070915359\n",
      "11900 -74.43322579230443\n",
      "12000 -74.9990553274175\n",
      "12100 -75.01948379504097\n",
      "12200 -74.98013572105384\n",
      "12300 -74.49139560978912\n",
      "12400 -73.86182525062159\n",
      "12500 -74.88408972587315\n",
      "12600 -74.45903083844492\n",
      "12700 -75.1702949811741\n",
      "12800 -74.6893245609823\n",
      "12900 -74.89004345169857\n",
      "13000 -74.08927518059915\n",
      "13100 -74.62093770131271\n",
      "13200 -74.67846567576277\n",
      "13300 -73.95554530463801\n",
      "13400 -74.68867957992893\n",
      "13500 -75.20390807454234\n",
      "13600 -73.51515571745584\n",
      "13700 -73.67501657133265\n",
      "13800 -73.28773081515311\n",
      "13900 -73.2412596476329\n",
      "14000 -72.38236629513369\n",
      "14100 -75.18735925398832\n",
      "14200 -74.59903687484938\n",
      "14300 -74.96802550864831\n",
      "14400 -74.30093029934187\n",
      "14500 -73.00259181895456\n",
      "14600 -74.20310104797635\n",
      "14700 -75.2484087296238\n",
      "14800 -74.74834059211341\n",
      "14900 -74.17121269964932\n",
      "2.0063191033126913 4.451756444904535 0.010625770192304297\n",
      "0 -168.56700485990336\n",
      "100 -169.16093626383702\n",
      "200 -171.74782069979696\n",
      "300 -174.1403160428098\n",
      "400 -175.7214245080349\n",
      "500 -177.02800239433301\n",
      "600 -178.21503966114057\n",
      "700 -179.30023387712342\n",
      "800 -180.29772767556713\n",
      "900 -181.21902366916632\n",
      "1000 -182.07359647938696\n",
      "1100 -182.80002462999818\n",
      "1200 -182.7989873845531\n",
      "1300 -181.89889938424994\n",
      "1400 -184.8066716335472\n",
      "1500 -185.55394003223034\n",
      "1600 -186.07987491169405\n",
      "1700 -186.6989509217896\n",
      "1800 -187.19385639445125\n",
      "1900 -182.62749216757925\n",
      "2000 -188.06424348244087\n",
      "2100 -188.57670358902507\n",
      "2200 -188.8992810311044\n",
      "2300 -189.39028548235413\n",
      "2400 -189.67700851684143\n",
      "2500 -189.9777192117125\n",
      "2600 -185.73002172435255\n",
      "2700 -190.23948536499256\n",
      "2800 -191.18445179027438\n",
      "2900 -191.38423123718374\n",
      "3000 -191.87567621147147\n",
      "3100 -192.06353291002915\n",
      "3200 -192.45781186855135\n",
      "3300 -192.4378648917108\n",
      "3400 -192.92218222232012\n",
      "3500 -184.0598319651156\n",
      "3600 -192.77899730070985\n",
      "3700 -192.97393620904376\n",
      "3800 -193.97693439939565\n",
      "3900 -193.43578736142211\n",
      "4000 -194.18069411269974\n",
      "4100 -194.44827449317168\n",
      "4200 -194.70709099142186\n",
      "4300 -194.2978902646765\n",
      "4400 -184.89384536881388\n",
      "4500 -195.07873375424217\n",
      "4600 -194.89319117775256\n",
      "4700 -195.12289560673733\n",
      "4800 -195.80968510778365\n",
      "4900 -195.99349098357322\n",
      "5000 -195.93005992135267\n",
      "5100 -196.24661265402753\n",
      "5200 -196.39154967876104\n",
      "5300 -194.74070902010908\n",
      "5400 -195.78730066788268\n",
      "5500 -193.64956284247614\n",
      "5600 -196.73368429518882\n",
      "5700 -196.41983553838588\n",
      "5800 -197.045919020292\n",
      "5900 -197.0432145504899\n",
      "6000 -196.8674835429081\n",
      "6100 -196.86290645783777\n",
      "6200 -197.38899506642846\n",
      "6300 -197.40806923610805\n",
      "6400 -196.0967582164463\n",
      "6500 -197.5524004725438\n",
      "6600 -197.48340305157785\n",
      "6700 -197.56348144421142\n",
      "6800 -197.09632532587463\n",
      "6900 -197.58351481239342\n",
      "7000 -197.65914797129884\n",
      "7100 -197.91034140530857\n",
      "7200 -197.6772823753607\n",
      "7300 -197.97859820501318\n",
      "7400 -197.93205260403542\n",
      "7500 -198.06481190298837\n",
      "7600 -195.85805998979063\n",
      "7700 -197.72313929508027\n",
      "7800 -198.10931403083595\n",
      "7900 -197.9151493381539\n",
      "8000 -198.12165247969767\n",
      "8100 -197.95157289313113\n",
      "8200 -198.146883461115\n",
      "8300 -198.07589494102194\n",
      "8400 -192.88934049692236\n",
      "8500 -197.90775626680906\n",
      "8600 -198.2807456690309\n",
      "8700 -198.29097308643648\n",
      "8800 -198.243265410898\n",
      "8900 -198.27576981443468\n",
      "9000 -198.11629090888522\n",
      "9100 -198.24790370126172\n",
      "9200 -198.3068332197145\n",
      "9300 -189.8343868025915\n",
      "9400 -197.5974990508284\n",
      "9500 -198.0191766765812\n",
      "9600 -198.29945778364308\n",
      "9700 -198.3008569789222\n",
      "9800 -198.30805556319063\n",
      "9900 -198.31989754085788\n",
      "10000 -198.3011032823398\n",
      "10100 -194.41131655933853\n",
      "10200 -195.28866484624206\n",
      "10300 -191.6071831551057\n",
      "10400 -198.22958370091476\n",
      "10500 -198.30944543809963\n",
      "10600 -198.31771781274873\n",
      "10700 -198.30583671966994\n",
      "10800 -198.31663020479223\n",
      "10900 -198.31682950093347\n",
      "11000 -198.27453643823807\n",
      "11100 -198.3098332374298\n",
      "11200 -197.4136495279031\n",
      "11300 -197.5996220967371\n",
      "11400 -198.31158608389066\n",
      "11500 -198.31428398058424\n",
      "11600 -198.3136853719027\n",
      "11700 -198.20883601195578\n",
      "11800 -197.75485623312585\n",
      "11900 -195.31633632084964\n",
      "12000 -195.53790090274006\n",
      "12100 -198.31112819610166\n",
      "12200 -198.2791591813171\n",
      "12300 -198.31232924014745\n",
      "12400 -198.21785546411303\n",
      "12500 -198.30410060249255\n",
      "12600 -198.3114338971644\n",
      "12700 -186.76160836685867\n",
      "12800 -198.05321465325943\n",
      "12900 -194.1041537218152\n",
      "13000 -197.71973063415928\n",
      "13100 -198.277050433098\n",
      "13200 -198.3033532420633\n",
      "13300 -198.20919655028032\n",
      "13400 -198.30191466965255\n",
      "13500 -198.31134044135138\n",
      "13600 -198.31245608824892\n",
      "13700 -198.10303419257733\n",
      "13800 -198.27534537088917\n",
      "13900 -197.258454332096\n",
      "14000 -197.95736628185873\n",
      "14100 -197.75473662197172\n",
      "14200 -198.2474550171484\n",
      "14300 -198.23477214509384\n",
      "14400 -198.30897069387996\n",
      "14500 -198.30276461036618\n",
      "14600 -198.29099430876812\n",
      "14700 -198.30557099728114\n",
      "14800 -198.31263080880817\n",
      "14900 -198.23177082340368\n",
      "0.10305201206987638 7.812961737076428 0.009979556384394123\n",
      "0 29.35864193913369\n",
      "100 -74.08323859126118\n",
      "200 -75.19809997909469\n",
      "300 -76.03013058263991\n",
      "400 -76.82227883252699\n",
      "500 -77.69105566749006\n",
      "600 -78.86470471796484\n",
      "700 -81.07591979611306\n",
      "800 -81.3166008993699\n",
      "900 -82.57345358406171\n",
      "1000 -84.745515704804\n",
      "1100 -84.69377497885947\n",
      "1200 -83.95812136585606\n",
      "1300 -84.41025264045322\n",
      "1400 -85.24934549603094\n",
      "1500 -84.90584190132006\n",
      "1600 -84.07613502362071\n",
      "1700 -85.30204911042763\n",
      "1800 -85.31217441087827\n",
      "1900 -83.75008403584823\n",
      "2000 -84.49778754442193\n",
      "2100 -84.51374594865787\n",
      "2200 -85.39433582746373\n",
      "2300 -83.91720435711653\n",
      "2400 -85.35884639101329\n",
      "2500 -83.85918740502956\n",
      "2600 -84.52357137441257\n",
      "2700 -85.2630365042921\n",
      "2800 -84.04031325295169\n",
      "2900 -83.43741920738327\n",
      "3000 -85.36538723178748\n",
      "3100 -85.20312541690011\n",
      "3200 -84.7415842827282\n",
      "3300 -85.18501787755312\n",
      "3400 -84.3524674739942\n",
      "3500 -84.45142256974223\n",
      "3600 -85.24073817264453\n",
      "3700 -84.30369243068512\n",
      "3800 -84.77257014633003\n",
      "3900 -84.31922499017575\n",
      "4000 -84.2482680039806\n",
      "4100 -84.77628297492583\n",
      "4200 -84.76033719810718\n",
      "4300 -84.86777100618173\n",
      "4400 -83.74715240664517\n",
      "4500 -83.54381670070201\n",
      "4600 -84.99245932654988\n",
      "4700 -84.83770866132724\n",
      "4800 -84.38584963864147\n",
      "4900 -85.39744965262301\n",
      "5000 -84.9692331377143\n",
      "5100 -84.85818711377642\n",
      "5200 -85.03162699830405\n",
      "5300 -83.80743397450988\n",
      "5400 -84.71281355809896\n",
      "5500 -84.33572668375281\n",
      "5600 -85.0245284063405\n",
      "5700 -85.50611480199433\n",
      "5800 -83.67273679318436\n",
      "5900 -85.25257801932396\n",
      "6000 -84.94352449624108\n",
      "6100 -84.81297377896452\n",
      "6200 -85.34098312755175\n",
      "6300 -84.98923804970454\n",
      "6400 -84.43352638464901\n",
      "6500 -85.21696847604713\n",
      "6600 -85.13908683791377\n",
      "6700 -83.74964334713492\n",
      "6800 -84.92906095828731\n",
      "6900 -84.81385956072415\n",
      "7000 -85.32984110350691\n",
      "7100 -85.17353802378759\n",
      "7200 -84.21734186032253\n",
      "7300 -85.3070838948357\n",
      "7400 -84.5619065409734\n",
      "7500 -84.9020165054873\n",
      "7600 -83.97330604537214\n",
      "7700 -85.30791540707222\n",
      "7800 -85.38526023844662\n",
      "7900 -84.47542393749995\n",
      "8000 -85.24757240570874\n",
      "8100 -85.21834639336572\n",
      "8200 -84.11130024692764\n",
      "8300 -84.94533999367154\n",
      "8400 -84.44914761426264\n",
      "8500 -85.32228932982055\n",
      "8600 -85.17845673603762\n",
      "8700 -84.18658566220013\n",
      "8800 -84.77101110594646\n",
      "8900 -82.91319336778449\n",
      "9000 -84.4406824602503\n",
      "9100 -84.05040922415465\n",
      "9200 -85.28030263288487\n",
      "9300 -84.98550909450341\n",
      "9400 -84.30477028272114\n",
      "9500 -85.26305834199061\n",
      "9600 -85.36230738542893\n",
      "9700 -82.81749702450661\n",
      "9800 -84.25143376530944\n",
      "9900 -85.42275904460283\n",
      "10000 -85.23463750100126\n",
      "10100 -85.30079587392609\n",
      "10200 -84.3825556114532\n",
      "10300 -84.19086386654884\n",
      "10400 -84.73508726015325\n",
      "10500 -85.22999159796578\n",
      "10600 -84.40210656695976\n",
      "10700 -83.56239574352479\n",
      "10800 -84.47371655195168\n",
      "10900 -85.4874667757754\n",
      "11000 -84.36689529391518\n",
      "11100 -84.53620594807744\n",
      "11200 -83.64779267327168\n",
      "11300 -84.06580297172526\n",
      "11400 -84.61886715540248\n",
      "11500 -84.68276291655\n",
      "11600 -84.57597272781426\n",
      "11700 -83.49218356449077\n",
      "11800 -84.60555780448531\n",
      "11900 -85.1223205654822\n",
      "12000 -84.07039092885265\n",
      "12100 -85.40942672384384\n",
      "12200 -85.36432879260622\n",
      "12300 -85.00614790862403\n",
      "12400 -83.43229968468265\n",
      "12500 -85.06447095812212\n",
      "12600 -84.17787969399697\n",
      "12700 -84.43695376971186\n",
      "12800 -85.23621176953704\n",
      "12900 -83.1650340688551\n",
      "13000 -84.80212047653583\n",
      "13100 -84.1140187865356\n",
      "13200 -84.68141359505383\n",
      "13300 -83.85133564201149\n",
      "13400 -85.20923482385311\n",
      "13500 -84.39480654229554\n",
      "13600 -85.45202222495575\n",
      "13700 -83.67212276939763\n",
      "13800 -83.79278157313203\n",
      "13900 -85.09670489108348\n",
      "14000 -84.12494302470542\n",
      "14100 -85.38635332516904\n",
      "14200 -83.70010631922014\n",
      "14300 -85.28204095415953\n",
      "14400 -83.47993178771671\n",
      "14500 -84.70389551266712\n",
      "14600 -84.95086435119948\n",
      "14700 -85.50555422532544\n",
      "14800 -85.014602608151\n",
      "14900 -84.99209632556202\n",
      "0.3461152139550641 2.0098616551183413 0.013967415431826157\n",
      "0 446.7142011566109\n",
      "500 16.007464811030765\n",
      "1000 -39.28933788895299\n",
      "1500 -55.506034627910644\n",
      "2000 -65.53841389813843\n",
      "2500 -74.11817868341224\n",
      "3000 -82.18415487858672\n",
      "3500 -89.94512872445307\n",
      "4000 -97.47435617904011\n",
      "4500 -104.81696476754682\n",
      "5000 -112.00926315222253\n",
      "5500 -119.0814979115986\n",
      "6000 -126.05849137163376\n",
      "6500 -132.9607305472391\n",
      "7000 -139.80246958041343\n",
      "7500 -146.59328501554236\n",
      "8000 -153.3417198837919\n",
      "8500 -160.06421845246865\n",
      "9000 -166.78951030025053\n",
      "9500 -173.56178957245578\n",
      "10000 -180.42772254568862\n",
      "10500 -186.71649086894215\n",
      "11000 -193.13673225128912\n",
      "11500 -199.70759580331006\n",
      "12000 -206.48159714700873\n",
      "12500 -213.49600405496946\n",
      "13000 -220.77809195254687\n",
      "13500 -228.3251787358063\n",
      "14000 -236.12108935196082\n",
      "14500 -244.11648380619835\n",
      "15000 -252.20785939397413\n",
      "15500 -260.2345493537811\n",
      "16000 -268.0291744970325\n",
      "16500 -275.60967923676975\n",
      "17000 -283.1658101640526\n",
      "17500 -290.7825700711667\n",
      "18000 -298.24627602906907\n",
      "18500 -305.14319092356345\n",
      "19000 -310.9889087846438\n",
      "19500 -315.7201021765783\n",
      "20000 -319.75978194598497\n",
      "20500 -323.1598161717166\n",
      "21000 -326.3544376251952\n",
      "21500 -329.00923197557876\n",
      "22000 -330.6902142946878\n",
      "22500 -331.5910829667804\n",
      "23000 -332.1416143968265\n",
      "23500 -332.5165550824434\n",
      "24000 -332.7858620696283\n",
      "24500 -332.9878645228627\n",
      "25000 -333.14690784759557\n",
      "25500 -333.27747705109505\n",
      "26000 -333.3894150917767\n",
      "26500 -333.4870557527617\n",
      "27000 -333.57343332408414\n",
      "27500 -333.6514243650571\n",
      "28000 -333.72138209098716\n",
      "28500 -333.78433679137305\n",
      "29000 -333.84107990460745\n",
      "29500 -333.8922013706015\n",
      "30000 -333.93836114546076\n",
      "30500 -333.9760153167701\n",
      "31000 -334.0104815875773\n",
      "31500 -334.0417004456386\n",
      "32000 -334.0658774209103\n",
      "32500 -334.0950833823513\n",
      "33000 -334.1175897304475\n",
      "33500 -334.1375886751112\n",
      "34000 -334.1553450122902\n",
      "34500 -334.1700321302794\n",
      "35000 -334.18537257440096\n",
      "35500 -334.198334879566\n",
      "36000 -334.20992934714286\n",
      "36500 -334.22039031955575\n",
      "37000 -334.2326421124106\n",
      "37500 -334.2430693290841\n",
      "38000 -334.25322583109585\n",
      "38500 -334.262944531881\n",
      "39000 -334.27230211418623\n",
      "39500 -334.2811642545678\n",
      "40000 -334.2898959656048\n",
      "40500 -334.29734472130957\n",
      "41000 -334.30450906965547\n",
      "41500 -334.31101080302454\n",
      "42000 -334.3179564077844\n",
      "42500 -334.3242599969875\n",
      "43000 -334.33026374272066\n",
      "43500 -334.33552036513555\n",
      "44000 -334.3413570078014\n",
      "44500 -334.34646030427893\n",
      "45000 -334.35124086619584\n",
      "45500 -334.3557580942799\n",
      "46000 -334.35998253270157\n",
      "46500 -334.3639340012082\n",
      "47000 -334.367611300745\n",
      "47500 -334.3710126695289\n",
      "48000 -334.3741497059421\n",
      "48500 -334.37701608223193\n",
      "49000 -334.37961845865436\n",
      "49500 -334.3817633948703\n",
      "50000 -334.38404855718414\n",
      "50500 -334.3857216690367\n",
      "51000 -334.3872107851978\n",
      "51500 -334.3883598146601\n",
      "52000 -334.38962040526997\n",
      "52500 -334.39056308777964\n",
      "53000 -334.3913315609705\n",
      "53500 -334.39194203196786\n",
      "54000 -334.39231618528544\n",
      "54500 -334.3927416622922\n",
      "55000 -334.3929733518579\n",
      "55500 -334.39309048896655\n",
      "56000 -334.3931878074754\n",
      "56500 -334.3932171980145\n",
      "57000 -334.39321194532164\n",
      "57500 -334.39323417457075\n",
      "58000 -334.39323611546973\n",
      "58500 -334.3932325526417\n",
      "59000 -334.3931644882137\n",
      "59500 -334.39323614613795\n",
      "60000 -334.39322087520145\n",
      "60500 -334.39323720937006\n",
      "61000 -334.3932372228928\n",
      "61500 -334.3932371773105\n",
      "62000 -334.39323438454875\n",
      "62500 -334.39323784488107\n",
      "63000 -334.3932370837014\n",
      "63500 -334.3932355525239\n",
      "64000 -334.3931810897718\n",
      "64500 -334.39320349751483\n",
      "65000 -334.3932372709565\n",
      "65500 -334.3932368636194\n",
      "66000 -334.39321954970814\n",
      "66500 -334.3932355809184\n",
      "67000 -334.3932372025809\n",
      "67500 -334.3929936467515\n",
      "68000 -334.39323393993783\n",
      "68500 -334.39323677587043\n",
      "69000 -334.3932359317422\n",
      "69500 -334.39323765557583\n",
      "70000 -334.3932373618609\n",
      "70500 -334.39283169906685\n",
      "71000 -334.3932236841536\n",
      "71500 -334.39321117770123\n",
      "72000 -334.39323749272154\n",
      "72500 -334.39287376279157\n",
      "73000 -334.3932277414305\n",
      "73500 -334.39323549481793\n",
      "74000 -334.39323717898486\n",
      "74500 -334.3910386195664\n",
      "75000 -334.39313620667605\n",
      "75500 -334.39323044043294\n",
      "76000 -334.39323649716454\n",
      "76500 -334.3932362680195\n",
      "77000 -334.3931819555937\n",
      "77500 -334.3932101168136\n",
      "78000 -334.39323754686296\n",
      "78500 -334.39282683180016\n",
      "79000 -334.39321589211784\n",
      "79500 -334.39323648367906\n",
      "80000 -334.3932352581406\n",
      "80500 -334.39323767381677\n",
      "81000 -334.39323306054246\n",
      "81500 -334.3932367287659\n",
      "82000 -334.3932369846889\n",
      "82500 -334.3932375219718\n",
      "83000 -334.3932374433499\n",
      "83500 -334.3932328280825\n",
      "84000 -334.39323684043\n",
      "84500 -334.39323741444264\n",
      "85000 -334.39323793562136\n",
      "85500 -334.393237594443\n",
      "86000 -334.393227767331\n",
      "86500 -334.39323760251506\n",
      "87000 -334.3932381000699\n",
      "87500 -334.3925791228462\n",
      "88000 -334.3932212522439\n",
      "88500 -334.3931894026304\n",
      "89000 -334.39323739768287\n",
      "89500 -334.3932379866214\n",
      "90000 -334.3932027663111\n",
      "90500 -334.39323809704047\n",
      "91000 -334.39323760112495\n",
      "91500 -334.3932379030304\n",
      "92000 -334.39323665254966\n",
      "92500 -334.3932376158065\n",
      "93000 -334.39323721869164\n",
      "93500 -334.39322386605727\n",
      "94000 -334.3931781012189\n",
      "94500 -334.3932379471163\n",
      "95000 -334.39323785744654\n",
      "95500 -334.3932381190262\n",
      "96000 -334.3932376008484\n",
      "96500 -334.39323814961784\n",
      "97000 -334.3932036632145\n",
      "97500 -334.39323719767987\n",
      "98000 -334.393237496153\n",
      "98500 -334.3932369474352\n",
      "99000 -334.39323634816634\n",
      "99500 -334.3932366171748\n",
      "200\n",
      "tensor(-169.3726, dtype=torch.float64)\n",
      "acceptance rate:  0.7\n",
      "300\n",
      "tensor(-148.4726, dtype=torch.float64)\n",
      "acceptance rate:  0.74\n",
      "400\n",
      "tensor(-146.5882, dtype=torch.float64)\n",
      "acceptance rate:  0.72\n",
      "500\n",
      "tensor(-139.6998, dtype=torch.float64)\n",
      "acceptance rate:  0.75\n",
      "600\n",
      "tensor(-148.9021, dtype=torch.float64)\n",
      "acceptance rate:  0.76\n",
      "700\n",
      "tensor(-137.9264, dtype=torch.float64)\n",
      "acceptance rate:  0.79\n",
      "800\n",
      "tensor(-152.9183, dtype=torch.float64)\n",
      "acceptance rate:  0.71\n",
      "900\n",
      "tensor(-124.2254, dtype=torch.float64)\n",
      "acceptance rate:  0.66\n",
      "1000\n",
      "tensor(-153.0477, dtype=torch.float64)\n",
      "acceptance rate:  0.79\n",
      "1100\n",
      "tensor(-123.7879, dtype=torch.float64)\n",
      "acceptance rate:  0.81\n",
      "1200\n",
      "tensor(-123.0121, dtype=torch.float64)\n",
      "acceptance rate:  0.72\n",
      "1300\n",
      "tensor(-120.1567, dtype=torch.float64)\n",
      "acceptance rate:  0.82\n",
      "1400\n",
      "tensor(-144.2972, dtype=torch.float64)\n",
      "acceptance rate:  0.73\n",
      "1500\n",
      "tensor(-129.2594, dtype=torch.float64)\n",
      "acceptance rate:  0.75\n",
      "1600\n",
      "tensor(-150.5354, dtype=torch.float64)\n",
      "acceptance rate:  0.78\n",
      "1700\n",
      "tensor(-126.1479, dtype=torch.float64)\n",
      "acceptance rate:  0.79\n",
      "1800\n",
      "tensor(-125.9112, dtype=torch.float64)\n",
      "acceptance rate:  0.8\n",
      "1900\n",
      "tensor(-120.5116, dtype=torch.float64)\n",
      "acceptance rate:  0.78\n",
      "2000\n",
      "tensor(-145.8691, dtype=torch.float64)\n",
      "acceptance rate:  0.81\n",
      "2100\n",
      "tensor(-122.7370, dtype=torch.float64)\n",
      "acceptance rate:  0.83\n",
      "2200\n",
      "tensor(-133.8812, dtype=torch.float64)\n",
      "acceptance rate:  0.83\n",
      "2300\n",
      "tensor(-137.7965, dtype=torch.float64)\n",
      "acceptance rate:  0.75\n",
      "2400\n",
      "tensor(-126.3249, dtype=torch.float64)\n",
      "acceptance rate:  0.84\n",
      "2500\n",
      "tensor(-126.9529, dtype=torch.float64)\n",
      "acceptance rate:  0.87\n",
      "2600\n",
      "tensor(-121.7600, dtype=torch.float64)\n",
      "acceptance rate:  0.78\n",
      "2700\n",
      "tensor(-130.9522, dtype=torch.float64)\n",
      "acceptance rate:  0.77\n",
      "2800\n",
      "tensor(-121.0469, dtype=torch.float64)\n",
      "acceptance rate:  0.81\n",
      "2900\n",
      "tensor(-115.0209, dtype=torch.float64)\n",
      "acceptance rate:  0.68\n",
      "3000\n",
      "tensor(-128.3609, dtype=torch.float64)\n",
      "acceptance rate:  0.77\n",
      "3100\n",
      "tensor(-146.3967, dtype=torch.float64)\n",
      "acceptance rate:  0.79\n",
      "3200\n",
      "tensor(-130.8992, dtype=torch.float64)\n",
      "acceptance rate:  0.86\n",
      "3300\n",
      "tensor(-136.6460, dtype=torch.float64)\n",
      "acceptance rate:  0.88\n",
      "3400\n",
      "tensor(-123.6209, dtype=torch.float64)\n",
      "acceptance rate:  0.86\n",
      "3500\n",
      "tensor(-126.3323, dtype=torch.float64)\n",
      "acceptance rate:  0.78\n",
      "3600\n",
      "tensor(-127.8101, dtype=torch.float64)\n",
      "acceptance rate:  0.87\n",
      "3700\n",
      "tensor(-125.8838, dtype=torch.float64)\n",
      "acceptance rate:  0.89\n",
      "3800\n",
      "tensor(-124.6319, dtype=torch.float64)\n",
      "acceptance rate:  0.8\n",
      "3900\n",
      "tensor(-144.1991, dtype=torch.float64)\n",
      "acceptance rate:  0.76\n",
      "4000\n",
      "tensor(-114.0772, dtype=torch.float64)\n",
      "acceptance rate:  0.77\n",
      "4100\n",
      "tensor(-123.4842, dtype=torch.float64)\n",
      "acceptance rate:  0.85\n",
      "4200\n",
      "tensor(-136.9395, dtype=torch.float64)\n",
      "acceptance rate:  0.86\n",
      "4300\n",
      "tensor(-100.4225, dtype=torch.float64)\n",
      "acceptance rate:  0.82\n",
      "4400\n",
      "tensor(-119.2147, dtype=torch.float64)\n",
      "acceptance rate:  0.85\n",
      "4500\n",
      "tensor(-122.1593, dtype=torch.float64)\n",
      "acceptance rate:  0.89\n",
      "4600\n",
      "tensor(-124.9064, dtype=torch.float64)\n",
      "acceptance rate:  0.91\n",
      "4700\n",
      "tensor(-117.9902, dtype=torch.float64)\n",
      "acceptance rate:  0.89\n",
      "4800\n",
      "tensor(-92.3427, dtype=torch.float64)\n",
      "acceptance rate:  0.8\n",
      "4900\n",
      "tensor(-125.6045, dtype=torch.float64)\n",
      "acceptance rate:  0.93\n",
      "5000\n",
      "tensor(-110.8433, dtype=torch.float64)\n",
      "acceptance rate:  0.77\n",
      "5100\n",
      "tensor(-119.0318, dtype=torch.float64)\n",
      "acceptance rate:  0.82\n",
      "5200\n",
      "tensor(-118.5486, dtype=torch.float64)\n",
      "acceptance rate:  0.87\n",
      "5300\n",
      "tensor(-125.2295, dtype=torch.float64)\n",
      "acceptance rate:  0.82\n",
      "5400\n",
      "tensor(-120.8514, dtype=torch.float64)\n",
      "acceptance rate:  0.84\n",
      "5500\n",
      "tensor(-108.6948, dtype=torch.float64)\n",
      "acceptance rate:  0.86\n",
      "5600\n",
      "tensor(-126.5203, dtype=torch.float64)\n",
      "acceptance rate:  0.83\n",
      "5700\n",
      "tensor(-82.8029, dtype=torch.float64)\n",
      "acceptance rate:  0.87\n",
      "5800\n",
      "tensor(-118.7790, dtype=torch.float64)\n",
      "acceptance rate:  0.77\n",
      "5900\n",
      "tensor(-105.6912, dtype=torch.float64)\n",
      "acceptance rate:  0.86\n",
      "6000\n",
      "tensor(-120.4793, dtype=torch.float64)\n",
      "acceptance rate:  0.77\n",
      "6100\n",
      "tensor(-97.0557, dtype=torch.float64)\n",
      "acceptance rate:  0.87\n",
      "6200\n",
      "tensor(-107.7527, dtype=torch.float64)\n",
      "acceptance rate:  0.84\n",
      "6300\n",
      "tensor(-82.7531, dtype=torch.float64)\n",
      "acceptance rate:  0.85\n",
      "6400\n",
      "tensor(-126.2806, dtype=torch.float64)\n",
      "acceptance rate:  0.89\n",
      "6500\n",
      "tensor(-121.9807, dtype=torch.float64)\n",
      "acceptance rate:  0.85\n",
      "6600\n",
      "tensor(-113.0734, dtype=torch.float64)\n",
      "acceptance rate:  0.86\n",
      "6700\n",
      "tensor(-103.0808, dtype=torch.float64)\n",
      "acceptance rate:  0.89\n",
      "6800\n",
      "tensor(-98.5197, dtype=torch.float64)\n",
      "acceptance rate:  0.9\n",
      "6900\n",
      "tensor(-117.4627, dtype=torch.float64)\n",
      "acceptance rate:  0.75\n",
      "7000\n",
      "tensor(-140.0163, dtype=torch.float64)\n",
      "acceptance rate:  0.88\n",
      "7100\n",
      "tensor(-131.4547, dtype=torch.float64)\n",
      "acceptance rate:  0.79\n",
      "7200\n",
      "tensor(-148.7997, dtype=torch.float64)\n",
      "acceptance rate:  0.82\n",
      "7300\n",
      "tensor(-99.5577, dtype=torch.float64)\n",
      "acceptance rate:  0.74\n",
      "7400\n",
      "tensor(-111.7935, dtype=torch.float64)\n",
      "acceptance rate:  0.82\n",
      "7500\n",
      "tensor(-113.5891, dtype=torch.float64)\n",
      "acceptance rate:  0.89\n",
      "7600\n",
      "tensor(-112.7019, dtype=torch.float64)\n",
      "acceptance rate:  0.75\n",
      "7700\n",
      "tensor(-115.9044, dtype=torch.float64)\n",
      "acceptance rate:  0.8\n",
      "7800\n",
      "tensor(-130.2732, dtype=torch.float64)\n",
      "acceptance rate:  0.85\n",
      "7900\n",
      "tensor(-93.7759, dtype=torch.float64)\n",
      "acceptance rate:  0.85\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import scipy\n",
    "import argparse\n",
    "import sys\n",
    "from arma import ode_system, solve_magi, matrix\n",
    "import time\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "\n",
    "# Derivatves of X according to the ODE structure\n",
    "def fOde(theta, x):\n",
    "    \"\"\"\n",
    "    theta: list[4]: beta, ve, vi, pd\n",
    "    x: array(n, 4)\n",
    "    r: array(n, 2)\n",
    "    \"\"\"\n",
    "    global N\n",
    "    logS = x[:, 0]\n",
    "    logE = x[:, 1]\n",
    "    logI = x[:, 2]\n",
    "    logD = x[:, 3]\n",
    "    logSdt = -theta[0] * np.exp(logI) / N  # (1)\n",
    "    logEdt = theta[0] * np.exp(logS + logI - logE) / N - theta[1]  # (2)\n",
    "    logIdt = np.exp(logE - logI) * theta[1] - theta[2]  # (3)\n",
    "    logDdt = np.exp(logI - logD) * 0.25 * theta[3] * theta[2]  # (4)\n",
    "    return np.stack([logSdt, logEdt, logIdt, logDdt], axis=1)\n",
    "\n",
    "\n",
    "# Derivatives of X\n",
    "def fOdeDx(theta, x):\n",
    "    \"\"\"\n",
    "    returns derivation of x given theta\n",
    "    theta: list[4]\n",
    "    x: array(n, 4)\n",
    "    r: array(n, 4, 4)\n",
    "    \"\"\"\n",
    "    resultDx = np.zeros(shape=[np.shape(x)[0], np.shape(x)[1], np.shape(x)[1]])\n",
    "    global N\n",
    "    logS = x[:, 0]\n",
    "    logE = x[:, 1]\n",
    "    logI = x[:, 2]\n",
    "    logD = x[:, 3]\n",
    "    # [:, i, j]: 第j个方程关于第i个状态求导\n",
    "    # (1) / dI\n",
    "    resultDx[:, 2, 0] = -theta[0] * np.exp(logI) / N\n",
    "    # (1) / dS, (1) /dE, (1) / dD = 0\n",
    "    # (2) / dS\n",
    "    resultDx[:, 0, 1] = theta[0] * np.exp(logS + logI - logE) / N\n",
    "    # (2) / dE\n",
    "    resultDx[:, 1, 1] = -theta[0] * np.exp(logS + logI - logE) / N\n",
    "    # (2) / dI\n",
    "    resultDx[:, 2, 1] = theta[0] * np.exp(logS + logI - logE) / N\n",
    "    # (2) / dD = 0\n",
    "    # (3) / dS = 0\n",
    "    # (3) / dE\n",
    "    resultDx[:, 1, 2] = np.exp(logE - logI) * theta[1]\n",
    "    # (3) / dI\n",
    "    resultDx[:, 2, 2] = -np.exp(logE - logI) * theta[1]\n",
    "    # (3) / dD = 0, (4) / dS, dE = 0\n",
    "    # (4) / dI\n",
    "    resultDx[:, 2, 3] = np.exp(logI - logD) * 0.25 * theta[3] * theta[2]\n",
    "    # (4) / dD\n",
    "    resultDx[:, 3, 3] = -np.exp(logI - logD) * 0.25 * theta[3] * theta[2]\n",
    "    return resultDx\n",
    "\n",
    "\n",
    "def fOdeDtheta(theta, x):\n",
    "    \"\"\"\n",
    "    returns derivation of theta given x\n",
    "    theta: list[4]\n",
    "    x: array(n, 4)\n",
    "    r: array(n, 4, 4)\n",
    "    \"\"\"\n",
    "    global N\n",
    "    resultDtheta = np.zeros(shape=[np.shape(x)[0], np.shape(theta)[0], np.shape(x)[1]])\n",
    "    logS = x[:, 0]\n",
    "    logE = x[:, 1]\n",
    "    logI = x[:, 2]\n",
    "    logD = x[:, 3]\n",
    "    # [:, i, j]: 第j个方程对theta_i求导\n",
    "    # (1) / dRe\n",
    "    resultDtheta[:, 0, 0] = -np.exp(logI) / N\n",
    "    # (2) / d theta[0]\n",
    "    resultDtheta[:, 0, 1] = np.exp(logS + logI - logE) / N\n",
    "    # (2) / theta[1]\n",
    "    resultDtheta[:, 1, 1] = -1.\n",
    "    # (3) / dtheta[1]\n",
    "    resultDtheta[:, 1, 2] = np.exp(logE - logI)\n",
    "    # (3) / dtheta[2]\n",
    "    resultDtheta[:, 2, 2] = -1.\n",
    "    # (4) / theta[2]\n",
    "    resultDtheta[:, 2, 3] = np.exp(logI - logD) * 0.25 * theta[3] \n",
    "    # (4) / theta[3]\n",
    "    resultDtheta[:, 3, 3] = np.exp(logI - logD) * 0.25 * theta[2]\n",
    "    return resultDtheta\n",
    "\n",
    "def fOdeTorch(theta, x, constant_param_ls):\n",
    "    \"\"\"\n",
    "    theta: list[4]: beta, ve, vi, pd\n",
    "    x: array(n, 4)\n",
    "    r: array(n, 2)\n",
    "    \"\"\"\n",
    "    global N\n",
    "    logS = x[:, 0]\n",
    "    logE = x[:, 1]\n",
    "    logI = x[:, 2]\n",
    "    logD = x[:, 3]\n",
    "    logSdt = -theta[:, 0] * torch.exp(logI) / N  # (1)\n",
    "    logEdt = theta[:, 0] * torch.exp(logS + logI - logE) / N - theta[:, 1]  # (2)\n",
    "    logIdt = torch.exp(logE - logI) * theta[:, 1] - constant_param_ls[0]  # (3)\n",
    "    # reparametrize on pd\n",
    "    logDdt = torch.exp(logI - logD) * 0.25 * theta[:, 2] * constant_param_ls[0]  # (4)\n",
    "    return torch.stack([logSdt, logEdt, logIdt, logDdt], axis=1)\n",
    "\n",
    "\n",
    "def copy_mat(arma_mat):\n",
    "    return np.copy(matrix(arma_mat).reshape([-1])).reshape([arma_mat.n_rows, arma_mat.n_cols])\n",
    "\n",
    "\n",
    "def pointwisethetasigmallikTorch(xlatent, theta, time_constant_param_ls, sigma, inferred_theta, ydata, CovAllDimensionsPyList, fOdeTorch,\n",
    "                                 priorTemperature, obs_per_day, positive_param=True):\n",
    "    # length of observed y (t)\n",
    "    n = ydata.shape[0]\n",
    "    pdimension = ydata.shape[1]\n",
    "    thetadimension = theta.shape[1]\n",
    "    sigmaSq = torch.pow(sigma, 2)\n",
    "    fderiv = fOdeTorch(theta, xlatent, time_constant_param_ls)\n",
    "    res = torch.zeros([pdimension, 3]).double()\n",
    "    fitDerivError = torch.zeros([n, pdimension]).double()\n",
    "    nobs = torch.zeros([pdimension]).double()\n",
    "    fitLevelErrorSumSq = torch.zeros([pdimension]).double()\n",
    "    for vEachDim in range(pdimension):\n",
    "        fitDerivError[:, vEachDim] = fderiv[:, vEachDim]\n",
    "        tmp = torch.sparse.mm(CovAllDimensionsPyList[vEachDim]['mphi'], xlatent[:, vEachDim].reshape(-1, 1))\n",
    "        fitDerivError[:, vEachDim] -= tmp[:, 0]\n",
    "        nobs[vEachDim] = torch.sum(torch.isfinite(ydata[:, vEachDim]))\n",
    "        obsIdx = torch.isfinite(ydata[:, vEachDim])\n",
    "        fitLevelErrorSumSq[vEachDim] = torch.sum(torch.square(xlatent[obsIdx, vEachDim] - ydata[obsIdx, vEachDim]))\n",
    "    res[:, 0] = -0.5 * fitLevelErrorSumSq / sigmaSq - torch.log(sigma + 0.001) * nobs\n",
    "    res[:, 0] /= priorTemperature[2]\n",
    "    KinvfitDerivError = torch.zeros([n, pdimension]).double()\n",
    "    CinvX = torch.zeros([n, pdimension]).double()\n",
    "    for vEachDim in range(pdimension):\n",
    "        # inverse of K\n",
    "        KinvfitDerivError[:, vEachDim] = torch.sparse.mm(CovAllDimensionsPyList[vEachDim]['Kinv'], fitDerivError[:, vEachDim].reshape(-1, 1))[:, 0]\n",
    "        # inverse of Cd\n",
    "        CinvX[:, vEachDim] = torch.sparse.mm(CovAllDimensionsPyList[vEachDim]['Cinv'], xlatent[:, vEachDim].reshape(-1, 1))[:, 0]\n",
    "    res[:, 1] = -0.5 * torch.sum(fitDerivError * KinvfitDerivError, dim=0) / priorTemperature[0]\n",
    "    #  prior distriobution of X\n",
    "    res[:, 2] = -0.5 * torch.sum(xlatent * CinvX, dim=0) / priorTemperature[1]\n",
    "    theta_lb = torch.clamp(theta[:, 2], min=0.)\n",
    "#     theta_ub = torch.clamp(theta[:, 2], max=0.3)\n",
    "    return torch.sum(res) - 1e6 * torch.sum(torch.square(theta[:, 2] - theta_lb))\n",
    "\n",
    "def MaternKernel(d, phi_1, phi_2, nu=2.5):\n",
    "    \"\"\"\n",
    "    construct a kernel given time points and hyper parameters\n",
    "    \"\"\"\n",
    "    if nu == 2.5:  \n",
    "        a = torch.square(phi_1) * (\n",
    "                1. + np.sqrt(5) * d / phi_2 + 5. * torch.square(d) / (3. * torch.square(phi_2))) * torch.exp(\n",
    "            -np.sqrt(5) * d / phi_2)\n",
    "        return a.double()\n",
    "    else:\n",
    "        a = torch.square(phi_1) * (1. + np.sqrt(3) * d / phi_2) * torch.exp(-np.sqrt(3) * d / phi_2)       \n",
    "        return a.double()\n",
    "\n",
    "\n",
    "def kernelllik(phi_1, phi_2, sigma, y, d_matrix, phi1_lb, phi2_lb, sigma_lb):\n",
    "    \"\"\"\n",
    "    optimize the kernel hyperparameters by maximizing marginal likelihood\n",
    "    \"\"\"\n",
    "    phi_1_bounded = torch.clamp(phi_1, min=phi1_lb)\n",
    "    phi_2_bounded = torch.clamp(phi_2, min=phi2_lb)\n",
    "    sigma_bounded = torch.clamp(sigma, min=sigma_lb)\n",
    "    K = MaternKernel(d_matrix, phi_1, phi_2)\n",
    "    K += torch.square(sigma) * torch.eye(y.shape[0]).double()\n",
    "    return -y.shape[0] * np.log(np.sqrt(2 * np.pi)) - 0.5 * y @ torch.inverse(K) @ y - 0.5 * torch.logdet(K) - 1e8 * torch.square(phi_1 - phi_1_bounded) - 1e8 * torch.square(phi_2 - phi_2_bounded) - 1e8 * torch.square(sigma - sigma_bounded)\n",
    "\n",
    "def to_band(matrix, bandwidth):\n",
    "    dim = matrix.shape[0]\n",
    "    for i in range(dim):\n",
    "        for j in range(dim):\n",
    "            if i > j + bandwidth or i < j - bandwidth:\n",
    "                matrix[i][j] = 0\n",
    "    return matrix.to_sparse()\n",
    "\n",
    "def xthetasigmallikTorch(xlatent, theta, time_constant_param_ls, sigma, inferred_theta, ydata, CovAllDimensionsPyList, fOdeTorch,\n",
    "                         priorTemperature,\n",
    "                         KinvthetaList, positive=True):\n",
    "    # length of observed y (t)\n",
    "    n = ydata.shape[0]\n",
    "    pdimension = ydata.shape[1]\n",
    "    thetadimension = theta.shape[1]\n",
    "    sigmaSq = torch.pow(sigma, 2)\n",
    "    fderiv = fOdeTorch(theta, xlatent, time_constant_param_ls)\n",
    "    res = torch.zeros([pdimension, 3]).double()\n",
    "    res_theta = torch.zeros(thetadimension).double()\n",
    "    res2 = torch.zeros(1).double()\n",
    "    fitDerivError = torch.zeros([n, pdimension]).double()\n",
    "    nobs = torch.zeros([pdimension]).double()\n",
    "    fitLevelErrorSumSq = torch.zeros([pdimension]).double()\n",
    "    for vEachDim in range(pdimension):\n",
    "        fitDerivError[:, vEachDim] = fderiv[:, vEachDim]\n",
    "        fitDerivError[:, vEachDim] -= torch.sparse.mm(CovAllDimensionsPyList[vEachDim]['mphi'], xlatent[:, vEachDim].reshape(-1, 1))[:, 0]\n",
    "        nobs[vEachDim] = torch.sum(torch.isfinite(ydata[:, vEachDim]))\n",
    "        obsIdx = torch.isfinite(ydata[:, vEachDim])\n",
    "        fitLevelErrorSumSq[vEachDim] = torch.sum(torch.square(xlatent[obsIdx, vEachDim] - ydata[obsIdx, vEachDim]))\n",
    "    res[:, 0] = -0.5 * fitLevelErrorSumSq / sigmaSq - torch.log(sigma + 0.0001) * nobs\n",
    "    res[:, 0] /= priorTemperature[2]\n",
    "    KinvfitDerivError = torch.zeros([n, pdimension]).double()\n",
    "    CinvX = torch.zeros([n, pdimension]).double()\n",
    "    for vEachDim in range(pdimension):\n",
    "        # inverse of K\n",
    "        KinvfitDerivError[:, vEachDim] = torch.sparse.mm(CovAllDimensionsPyList[vEachDim]['Kinv'], fitDerivError[:, vEachDim].reshape(-1, 1))[:, 0]\n",
    "        # inverse of Cd\n",
    "        CinvX[:, vEachDim] = torch.sparse.mm(CovAllDimensionsPyList[vEachDim]['Cinv'], xlatent[:, vEachDim].reshape(-1, 1))[:, 0]\n",
    "    for thetaEachDim in range(thetadimension):\n",
    "        res_theta[thetaEachDim] = -0.5 * torch.sum(\n",
    "            (theta[:, thetaEachDim] - inferred_theta[thetaEachDim]) @ torch.sparse.mm(KinvthetaList[thetaEachDim], (\n",
    "                    theta[:, thetaEachDim] - inferred_theta[thetaEachDim]).reshape(-1, 1)))\n",
    "    res[:, 1] = -0.5 * torch.sum(fitDerivError * KinvfitDerivError, dim=0) / priorTemperature[0]\n",
    "    res[:, 2] = -0.5 * torch.sum(xlatent * CinvX, dim=0) / priorTemperature[1]\n",
    "    theta_lb = torch.clamp(theta[:, 2], min = 0.)\n",
    "    return torch.sum(res) + torch.sum(res_theta)  - 1e6 * torch.sum(torch.square(theta[:, 2] - theta_lb))\n",
    "\n",
    "\n",
    "\n",
    "def vectorize(xlatent, theta, sigma, time_constant_param_ls):\n",
    "    t1 = torch.reshape(xlatent.detach(), (-1,))\n",
    "    t2 = torch.reshape(theta.detach(), (-1,))\n",
    "    t3 = torch.reshape(sigma.detach(), (-1,))\n",
    "    long_vec = torch.cat((t1, t2, t3))\n",
    "    for i in range(len(time_constant_param_ls)):\n",
    "        long_vec = torch.cat((long_vec, time_constant_param_ls[i].detach()))\n",
    "    return long_vec\n",
    "\n",
    "def get_dim(tensor_shape):\n",
    "    if len(tensor_shape) == 0:\n",
    "        return 1\n",
    "    if len(tensor_shape) == 1:\n",
    "        return tensor_shape[0]\n",
    "    dim = 1\n",
    "    for i in range(len(tensor_shape)):\n",
    "        dim *= tensor_shape[i]\n",
    "    return dim\n",
    "    \n",
    "def devectorize(long_tensor, xlatent_shape, theta_shape, sigma_shape, time_constant_param_dim):\n",
    "    x_latent_dim = get_dim(xlatent_shape)\n",
    "    theta_dim = get_dim(theta_shape)\n",
    "    sigma_dim = get_dim(sigma_shape)\n",
    "    time_constant_param_ls = []\n",
    "    xlatent = torch.reshape(long_tensor[:x_latent_dim],xlatent_shape)\n",
    "    theta = torch.reshape(long_tensor[x_latent_dim:x_latent_dim + theta_dim],theta_shape)\n",
    "    sigma = torch.reshape(long_tensor[x_latent_dim + theta_dim:x_latent_dim + theta_dim + sigma_dim],sigma_shape)\n",
    "    for each in range(x_latent_dim + theta_dim + sigma_dim, long_tensor.shape[0]):\n",
    "        time_constant_param_ls.append(torch.tensor([long_tensor[each]]))\n",
    "    return xlatent, theta, sigma, time_constant_param_ls\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "def TVMAGI_sampler(use_data_idx = 0,\n",
    "                    days = 32,\n",
    "                    discretization = 2,\n",
    "                    obs_per_day = 1,\n",
    "                    theta_lowerbound=np.array([0., 0., 0., 0.]),\n",
    "                    theta_upperbound=np.array([np.inf, 1., 1., 1.]),\n",
    "                    param_names = ['re', 've', 'vi', 'pd'],\n",
    "                    is_time_varying=[True, True, False, True],\n",
    "                    use_trajectory='inferred',\n",
    "                    learning_rate=np.array([1e-4, 1e-3, 1e-5]),\n",
    "                    n_iter = [15001, 100000, 15000, 100000],\n",
    "                    phi1_lb_ls=np.array([2., 0.1, 0.1]),\n",
    "                    phi2_lb_ls=np.array([2., 2., 2.]),\n",
    "                    sigma_lb_ls = np.array([0.01, 0.01, 0.01]),\n",
    "                    bandwidth=20):\n",
    " \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    yobs = observations[use_data_idx]\n",
    "    yobs[:, 1] = np.interp(np.arange(0, days, 1), np.arange(0, days, 2), yobs[::2, 1])\n",
    "    start_time = time.time()\n",
    "    nobs, p_dim = yobs.shape[0], yobs.shape[1]\n",
    "    n_points = days * discretization\n",
    "    theta_dim = theta_lowerbound.shape[0]\n",
    "    d_matrix = torch.zeros((n_points, n_points), dtype=torch.double)\n",
    "    for i in range(n_points):\n",
    "        for j in range(n_points):\n",
    "            if i > j:\n",
    "                d_matrix[i][j] = (i - j) / (obs_per_day * discretization)\n",
    "            else:\n",
    "                d_matrix[i][j] = (j - i) / (obs_per_day * discretization)\n",
    "    Ode_system = ode_system(\"ODE-python\", fOde, fOdeDx, fOdeDtheta,\n",
    "                             thetaLowerBound=theta_lowerbound,\n",
    "                             thetaUpperBound=theta_upperbound)\n",
    "    tvecObs = np.arange(0, days, 1. / obs_per_day)\n",
    "    tvecFull = np.arange(0, days, 1. / (obs_per_day * discretization))\n",
    "    yFull = np.ndarray([n_points, p_dim])\n",
    "    yFull.fill(np.nan)\n",
    "    yFull[np.arange(0, discretization * nobs, discretization).astype(int), :] = yobs\n",
    "    xInitExogenous = np.zeros_like(yFull)\n",
    "    # interpolate: find the ydata of tvecFull given observations\n",
    "    for i in range(p_dim):\n",
    "        xInitExogenous[:, i] = np.interp(tvecFull, tvecObs, yobs[:, i])\n",
    "\n",
    "\n",
    "    # First stage: use MAGI package to optimize theta as constant #####################\n",
    "\n",
    "    time_1 = time.time()\n",
    "    result = solve_magi(\n",
    "        yFull,\n",
    "        Ode_system,\n",
    "        tvecFull,\n",
    "        sigmaExogenous=np.array([]),\n",
    "        phiExogenous=np.array([[]]),\n",
    "        xInitExogenous=xInitExogenous,\n",
    "        thetaInitExogenous=np.array([]),\n",
    "        muExogenous=np.array([[]]),\n",
    "        dotmuExogenous=np.array([[]]),\n",
    "        priorTemperatureLevel=yFull.shape[0] / yobs.shape[0],\n",
    "        priorTemperatureDeriv=yFull.shape[0] / yobs.shape[0],\n",
    "        priorTemperatureObs=1.0,\n",
    "        kernel=\"generalMatern\",\n",
    "        nstepsHmc=100,\n",
    "        burninRatioHmc=0.5,\n",
    "        niterHmc=n_iter[0],\n",
    "        stepSizeFactorHmc=0.01,\n",
    "        nEpoch=1,\n",
    "        bandSize=bandwidth,\n",
    "        useFrequencyBasedPrior=True,\n",
    "        useBand=True,\n",
    "        useMean=False,\n",
    "        useScalerSigma=False,\n",
    "        useFixedSigma=False,\n",
    "        verbose=True)\n",
    "    samplesCpp = result['samplesCpp']\n",
    "    llikId = 0\n",
    "    xId = range(np.max(llikId) + 1, np.max(llikId) + yFull.size + 1)\n",
    "    # dimension of theta\n",
    "    thetaId = range(np.max(xId) + 1, np.max(xId) + theta_dim + 1)\n",
    "    sigmaId = range(np.max(thetaId) + 1, np.max(thetaId) + yFull.shape[1] + 1)\n",
    "    burnin = int(n_iter[0] * 0.5)\n",
    "    xsampled = samplesCpp[xId, (burnin + 1):]\n",
    "    xsampled = xsampled.reshape([yFull.shape[1], yFull.shape[0], -1])\n",
    "    CovAllDimensionsPyList = []\n",
    "    thetaSampled = samplesCpp[thetaId, (burnin + 1):]\n",
    "    inferred_theta = np.mean(thetaSampled, axis=-1)\n",
    "    print(inferred_theta)\n",
    "    sigmaSampled = samplesCpp[sigmaId, (burnin + 1):]\n",
    "    inferred_sigma = np.mean(sigmaSampled, axis=-1)\n",
    "    inferred_trajectory = np.mean(xsampled, axis=-1)\n",
    "    MAGI_time = time.time() - time_1\n",
    "    for each_gpcov in result['result_solved'].covAllDimensions:\n",
    "            each_pycov = dict(\n",
    "                Cinv=to_band(torch.from_numpy(matrix(each_gpcov.Cinv)).double(),bandwidth=bandwidth), \n",
    "                Kinv=to_band(torch.from_numpy(matrix(each_gpcov.Kinv)).double(),bandwidth=bandwidth),\n",
    "                mphi=to_band(torch.from_numpy(matrix(each_gpcov.mphi)).double(),bandwidth=bandwidth),\n",
    "            )\n",
    "            CovAllDimensionsPyList.append(each_pycov)\n",
    "            \n",
    "    # Pointwise optimization #############################################\n",
    "    time_2 = time.time()\n",
    "    TV_theta_mean = np.zeros(int(sum(is_time_varying)))\n",
    "    tv_index = 0\n",
    "    for thetaEachDim in range(theta_dim):\n",
    "        if is_time_varying[thetaEachDim] == True:\n",
    "            TV_theta_mean[tv_index] = inferred_theta[thetaEachDim]\n",
    "            tv_index += 1\n",
    "\n",
    "    if use_trajectory == 'observation':\n",
    "        pointwise_xlatent_torch = torch.tensor(xInitExogenous, requires_grad=True, dtype=torch.double)\n",
    "    elif use_trajectory == 'inferred':\n",
    "        pointwise_xlatent_torch = torch.tensor(inferred_trajectory.transpose(), requires_grad=True, dtype=torch.double)\n",
    "    else:\n",
    "        raise ValueError\n",
    "    tmp1 = np.array([TV_theta_mean])\n",
    "    initial_tvtheta = np.repeat(tmp1, pointwise_xlatent_torch.shape[0], axis=0)\n",
    "    pointwise_theta_torch = torch.tensor(initial_tvtheta, requires_grad=True, dtype=torch.double)\n",
    "    pointwise_sigma_torch = torch.tensor(inferred_sigma, requires_grad=True, dtype=torch.double)\n",
    "    time_constant_param_ls = []\n",
    "    for thetaEachDim in range(theta_dim):\n",
    "        if is_time_varying[thetaEachDim] == 0:\n",
    "            param_name = param_names[thetaEachDim]\n",
    "            locals()[param_name] = torch.tensor([inferred_theta[thetaEachDim]], requires_grad=True, dtype=torch.double)\n",
    "            time_constant_param_ls.append(eval(param_name))\n",
    "\n",
    "    ydata = torch.from_numpy(yFull).double()\n",
    "    priorTemperature = torch.tensor([discretization, discretization, 1.0])  # ?\n",
    "    pointwise_optimizer = torch.optim.Adam([pointwise_xlatent_torch, pointwise_theta_torch, pointwise_sigma_torch] + time_constant_param_ls, lr=1e-4)  # , weight_decay = 1.0\n",
    "    pointwise_lr_scheduler = torch.optim.lr_scheduler.StepLR(pointwise_optimizer, step_size=10000, gamma=0.5)\n",
    "    cur_loss = 1e12\n",
    "    LossVal = np.zeros(n_iter[1])\n",
    "    backward_time_ls = np.zeros(n_iter[1])\n",
    "    step_time_ls = np.zeros(n_iter[1])\n",
    "    for epoch in range(n_iter[1]):\n",
    "        pointwise_optimizer.zero_grad()\n",
    "        # compute loss function\n",
    "        llik = pointwisethetasigmallikTorch(pointwise_xlatent_torch, pointwise_theta_torch, time_constant_param_ls, pointwise_sigma_torch,\n",
    "                                            TV_theta_mean, ydata, CovAllDimensionsPyList, fOdeTorch,\n",
    "                                            priorTemperature, obs_per_day)\n",
    "        new_loss = -llik\n",
    "        LossVal[epoch] = new_loss\n",
    "        if epoch % 200 == 0:\n",
    "            print(epoch, new_loss.item())\n",
    "            diff = new_loss.item() - cur_loss\n",
    "    #             if torch.isnan(new_loss) == False and diff > -0.01 and diff < 0.01:\n",
    "    #                 break\n",
    "            cur_loss = new_loss.item()\n",
    "        time_b = time.time()\n",
    "        new_loss.backward()\n",
    "        backward_time_ls[epoch] = time.time() - time_b\n",
    "        time_s = time.time()\n",
    "        pointwise_optimizer.step()\n",
    "        step_time_ls[epoch] = time.time() - time_s\n",
    "        pointwise_lr_scheduler.step()\n",
    "        \n",
    "    \n",
    "    # Kernel estimation ########################################\n",
    "    time_3 = time.time()\n",
    "    pointwise_theta = pointwise_theta_torch.detach().numpy()\n",
    "    hyperparamList = []\n",
    "\n",
    "    # optimize the hyperparameters of kernels\n",
    "    for thetaEachDimension in range(pointwise_theta.shape[1]):\n",
    "        phi_1 = torch.tensor(phi1_lb_ls[thetaEachDimension], requires_grad=True, dtype=torch.double)\n",
    "        phi_2 = torch.tensor(phi2_lb_ls[thetaEachDimension], requires_grad=True, dtype=torch.double)\n",
    "        sigma = torch.tensor(sigma_lb_ls[thetaEachDimension], requires_grad=True, dtype=torch.double)\n",
    "        kernel_optimizer = torch.optim.Adam([phi_1, phi_2, sigma], lr=learning_rate[1])\n",
    "        kernel_backward_time = np.zeros(n_iter[2])\n",
    "        kernel_step_time = np.zeros(n_iter[2])\n",
    "        for epoch in range(n_iter[2]):\n",
    "            kernel_optimizer.zero_grad()\n",
    "            loss = -kernelllik(phi_1, \n",
    "                               phi_2, \n",
    "                               sigma, \n",
    "                               pointwise_theta_torch[:, thetaEachDimension] - torch.mean(pointwise_theta_torch[:, thetaEachDimension]), \n",
    "                               d_matrix, \n",
    "                               phi1_lb_ls[thetaEachDimension], \n",
    "                               phi2_lb_ls[thetaEachDimension],\n",
    "                               sigma_lb_ls[thetaEachDimension])\n",
    "            time0 = time.time()\n",
    "            loss.backward()\n",
    "            kernel_backward_time[epoch] = time.time() - time0\n",
    "            time0 = time.time()\n",
    "            kernel_optimizer.step()\n",
    "            kernel_step_time[epoch] = time.time() - time0\n",
    "            if epoch % 100 == 0:\n",
    "                print(epoch, loss.item())\n",
    "        print(phi_1.detach().item(), phi_2.detach().item(), sigma.detach().item()) \n",
    "        kernel_backward_time = np.mean(kernel_backward_time)\n",
    "        kernel_step_time = np.mean(kernel_step_time)\n",
    "        hyperparamList.append([phi_1.detach().item(), phi_2.detach().item(), sigma.detach().item()])\n",
    "\n",
    "    KinvthetaList = []\n",
    "    for thetaEachDimension in range(pointwise_theta.shape[1]):\n",
    "        ker = MaternKernel(d_matrix, torch.tensor(hyperparamList[thetaEachDimension][0]), torch.tensor(hyperparamList[thetaEachDimension][1]))\n",
    "        KinvthetaList.append(to_band(torch.inverse(ker), bandwidth = bandwidth))\n",
    "    kernel_total_time = time.time() - time_3            \n",
    "            \n",
    "    # TVMAGI optimization #############################        \n",
    "    time_4 = time.time()\n",
    "    TVMAGI_xlatent_torch = torch.tensor(pointwise_xlatent_torch.detach().numpy(), requires_grad=True, dtype=torch.double)\n",
    "    TVMAGI_theta_torch = torch.tensor(pointwise_theta_torch.detach().numpy(), requires_grad=True, dtype=torch.double)\n",
    "    TVMAGI_sigma_torch = torch.tensor(inferred_sigma, requires_grad=True, dtype=torch.double)\n",
    "    TVMAGI_optimizer = torch.optim.Adam([TVMAGI_xlatent_torch, TVMAGI_theta_torch, TVMAGI_sigma_torch] + time_constant_param_ls, lr=1e-5)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(TVMAGI_optimizer, step_size=10000, gamma=0.9)\n",
    "    cur_loss = 1e12\n",
    "    TVMAGI_backward_time = np.zeros(n_iter[3])\n",
    "    TVMAGI_step_time = np.zeros(n_iter[3])\n",
    "    for epoch in range(n_iter[3]):\n",
    "        TVMAGI_optimizer.zero_grad()\n",
    "        # compute loss function\n",
    "        llik = xthetasigmallikTorch(TVMAGI_xlatent_torch, TVMAGI_theta_torch, time_constant_param_ls, TVMAGI_sigma_torch,\n",
    "                                                    TV_theta_mean, ydata, CovAllDimensionsPyList, fOdeTorch,\n",
    "                                                    priorTemperature, KinvthetaList)\n",
    "        loss = -llik\n",
    "        if epoch % 500 == 0:\n",
    "            print(epoch, loss.item())\n",
    "    #             if torch.isnan(loss) == False and loss - cur_loss > -0.01 and loss - cur_loss < 0.01:\n",
    "    #                 break\n",
    "            cur_loss = loss.item()\n",
    "        time0 = time.time()\n",
    "        loss.backward()\n",
    "        TVMAGI_backward_time[epoch] = time.time() - time0\n",
    "        time0 = time.time()\n",
    "        TVMAGI_optimizer.step()\n",
    "        TVMAGI_step_time[epoch] = time.time() - time0\n",
    "        lr_scheduler.step() \n",
    "\n",
    "    # TVMAGI sampler #######################################\n",
    "    def NegLogLikelihood(xlatent, theta, sigma, time_constant_param_ls, \n",
    "                         inferred_theta = inferred_theta, \n",
    "                         ydata = ydata, \n",
    "                         CovAllDimensionsPyList = CovAllDimensionsPyList, \n",
    "                         fOdeTorch = fOdeTorch,\n",
    "                         priorTemperature = priorTemperature, \n",
    "                         KinvthetaList = KinvthetaList):\n",
    "        # length of observed y (t)\n",
    "        n = ydata.shape[0]\n",
    "        pdimension = ydata.shape[1]\n",
    "        thetadimension = theta.shape[1]\n",
    "        sigmaSq = torch.pow(sigma, 2)\n",
    "        fderiv = fOdeTorch(theta, xlatent, time_constant_param_ls)\n",
    "        res = torch.zeros([pdimension, 3]).double()\n",
    "        res_theta = torch.zeros(thetadimension).double()\n",
    "        fitDerivError = torch.zeros([n, pdimension]).double()\n",
    "        nobs = torch.zeros([pdimension]).double()\n",
    "        fitLevelErrorSumSq = torch.zeros([pdimension]).double()\n",
    "        for vEachDim in range(pdimension):\n",
    "            fitDerivError[:, vEachDim] = fderiv[:, vEachDim]\n",
    "            fitDerivError[:, vEachDim] -= torch.sparse.mm(CovAllDimensionsPyList[vEachDim]['mphi'], xlatent[:, vEachDim].reshape(-1, 1))[:, 0]\n",
    "            nobs[vEachDim] = torch.sum(torch.isfinite(ydata[:, vEachDim]))\n",
    "            obsIdx = torch.isfinite(ydata[:, vEachDim])\n",
    "            fitLevelErrorSumSq[vEachDim] = torch.sum(torch.square(xlatent[obsIdx, vEachDim] - ydata[obsIdx, vEachDim]))\n",
    "        res[:, 0] = -0.5 * fitLevelErrorSumSq / sigmaSq - torch.log(sigma + 0.0001) * nobs\n",
    "        res[:, 0] /= priorTemperature[2]\n",
    "        KinvfitDerivError = torch.zeros([n, pdimension]).double()\n",
    "        CinvX = torch.zeros([n, pdimension]).double()\n",
    "        for vEachDim in range(pdimension):\n",
    "            # inverse of K\n",
    "            KinvfitDerivError[:, vEachDim] = torch.sparse.mm(CovAllDimensionsPyList[vEachDim]['Kinv'], fitDerivError[:, vEachDim].reshape(-1, 1))[:, 0]\n",
    "            # inverse of Cd\n",
    "            CinvX[:, vEachDim] = torch.sparse.mm(CovAllDimensionsPyList[vEachDim]['Cinv'], xlatent[:, vEachDim].reshape(-1, 1))[:, 0]\n",
    "        for thetaEachDim in range(thetadimension):\n",
    "            res_theta[thetaEachDim] = -0.5 * torch.sum(\n",
    "                (theta[:, thetaEachDim] - inferred_theta[thetaEachDim]) @ torch.sparse.mm(KinvthetaList[thetaEachDim], (theta[:, thetaEachDim] - inferred_theta[thetaEachDim]).reshape(-1, 1))[:, 0])\n",
    "        res[:, 1] = -0.5 * torch.sum(fitDerivError * KinvfitDerivError, dim=0) / priorTemperature[0]\n",
    "        res[:, 2] = -0.5 * torch.sum(xlatent * CinvX, dim=0) / priorTemperature[1]\n",
    "\n",
    "        return -(torch.sum(res) + torch.sum(res_theta))\n",
    "\n",
    "    class HMC:\n",
    "        def __init__(self, negllik, all_theta, xlatent_shape, theta_shape, sigma_shape, time_constant_param_ls, lsteps=50, epsilon=1e-5, n_samples=4000, upper_bound = None, lower_bound = None, burn_in_ratio = 0.5):\n",
    "            self.all_theta = all_theta\n",
    "            self.theta_shape = theta_shape\n",
    "            self.xlatent_shape = xlatent_shape\n",
    "            self.sigma_shape = sigma_shape\n",
    "            self.constant_dim = len(time_constant_param_ls)\n",
    "            self.lsteps = lsteps\n",
    "            self.epsilon = epsilon * torch.ones(all_theta.shape)\n",
    "            self.burn_in_ratio = burn_in_ratio\n",
    "            self.n_samples = n_samples\n",
    "            self.total_samples = int(n_samples / (1 - burn_in_ratio))\n",
    "            self.NegLogLikelihood = negllik\n",
    "            self.ub = upper_bound\n",
    "            if upper_bound is not None:\n",
    "                if upper_bound.shape[0] != all_theta.shape[0]:\n",
    "                    raise ValueError\n",
    "            self.lb = lower_bound\n",
    "            if lower_bound is not None:\n",
    "                if lower_bound.shape[0] != all_theta.shape[0]:\n",
    "                    raise ValueError\n",
    "\n",
    "        def NegLogLikelihood_vec(self, all_theta):\n",
    "            xlatent_0, theta_0, sigma_0, constant_param_ls_0 = devectorize(all_theta, self.xlatent_shape, self.theta_shape, self.sigma_shape, self.constant_dim)\n",
    "            return NegLogLikelihood(xlatent_0, theta_0, sigma_0, constant_param_ls_0)\n",
    "\n",
    "        def Nabla(self, theta_torch):\n",
    "            theta_torch = theta_torch.detach()\n",
    "            xlatent, theta, sigma, constant_param_ls = devectorize(theta_torch, self.xlatent_shape, self.theta_shape, self.sigma_shape, self.constant_dim)\n",
    "            xlatent.requires_grad = True  \n",
    "            theta.requires_grad = True\n",
    "            sigma.requires_grad = True\n",
    "            for each in constant_param_ls:\n",
    "                each.requires_grad = True                      \n",
    "            llik = self.NegLogLikelihood(xlatent, theta, sigma, constant_param_ls)\n",
    "            llik.backward()\n",
    "            constant_param_deriv_ls = []\n",
    "            for each in constant_param_ls:\n",
    "                constant_param_deriv_ls.append(each.grad)\n",
    "            v = vectorize(xlatent.grad, theta.grad, sigma.grad, constant_param_deriv_ls)\n",
    "\n",
    "            return v\n",
    "        def sample(self, all_theta, TV_theta_mean, ydata, CovAllDimensionsPyList, fOdeTorch, priorTemperature, KinvthetaList):\n",
    "            def bounce(m, lb, ub):\n",
    "                if lb is None and ub is None:\n",
    "                    return m\n",
    "                if lb is None:\n",
    "                    max_tensor = torch.clamp(m - ub, min=0)\n",
    "                    return m - 2 * max_tensor\n",
    "                if ub is None:\n",
    "                    min_tensor = torch.clamp(lb - m, min=0)\n",
    "                    return m + 2 * min_tensor\n",
    "                if torch.sum(lb < ub) < m.shape[0]:\n",
    "                    raise ValueError\n",
    "                if torch.sum(m >= lb) == m.shape[0] and torch.sum(m <= ub) == m.shape[0]:\n",
    "                    return m\n",
    "                if torch.sum(m >= lb) < m.shape[0]:\n",
    "                    min_tensor = torch.clamp(lb - m, min=0)\n",
    "                    return bounce(m + 2 * min_tensor, lb, ub)\n",
    "                if torch.sum(m <= ub) < m.shape[0]:\n",
    "                    max_tensor = torch.clamp(m - ub, min=0)\n",
    "                    return bounce(m - 2 * max_tensor, lb, ub)\n",
    "\n",
    "            trace_val = np.zeros(self.total_samples)\n",
    "            samples = np.zeros((self.total_samples, self.all_theta.shape[0]))\n",
    "            random_ls = np.random.uniform(0, 1, self.total_samples)\n",
    "            acceptance_ls = np.zeros(self.total_samples)\n",
    "            nan_ls = np.zeros(self.total_samples)\n",
    "            cur_theta = self.all_theta.clone().detach()\n",
    "            for EachIter in range(self.total_samples): ############\n",
    "                cur_nllik_1 = self.NegLogLikelihood_vec(cur_theta).detach()\n",
    "                rstep = torch.rand(self.epsilon.shape) * self.epsilon + self.epsilon\n",
    "                p = torch.normal(mean=0., std=torch.ones(self.all_theta.shape))\n",
    "                cur_p = p.clone()\n",
    "                theta = cur_theta.clone()         \n",
    "                p = p - rstep * self.Nabla(theta).clone() / 2\n",
    "                for i in range(self.lsteps):\n",
    "                    theta = theta + rstep * p\n",
    "                    nabla_torch = self.Nabla(theta).clone()\n",
    "                    p = p - rstep * nabla_torch\n",
    "                    theta = bounce(theta, self.lb, self.ub)\n",
    "\n",
    "                p = p - rstep * self.Nabla(theta).clone() / 2\n",
    "\n",
    "                new_nllik = self.NegLogLikelihood_vec(theta)\n",
    "                new_p = 0.5 * torch.sum(torch.square(p))\n",
    "                new_H = new_nllik + new_p\n",
    "                cur_nllik = self.NegLogLikelihood_vec(cur_theta).detach()\n",
    "                cur_H = cur_nllik + 0.5 * torch.sum(torch.square(cur_p))\n",
    "    #             print(new_H, cur_H)\n",
    "\n",
    "                if torch.isnan(theta[0]) or torch.isnan(new_H):\n",
    "                    samples[EachIter] = cur_theta.clone()\n",
    "                    nan_ls[EachIter] = 1\n",
    "                    self.epsilon *= 0.9\n",
    "                    print('NaN!')\n",
    "                else:\n",
    "                    # accept\n",
    "                    tmp = float(torch.exp(cur_H - new_H))\n",
    "    #                 print(tmp)\n",
    "                    if  tmp > random_ls[EachIter]:\n",
    "                        samples[EachIter] = theta.clone()\n",
    "                        cur_theta = theta.clone()\n",
    "                        acceptance_ls[EachIter] = 1\n",
    "                    # reject\n",
    "                    else:\n",
    "                        samples[EachIter] = cur_theta.clone()\n",
    "\n",
    "                trace_val[EachIter] = self.NegLogLikelihood_vec(cur_theta).item()        \n",
    "\n",
    "                if EachIter > 200 and EachIter < self.total_samples - self.n_samples:\n",
    "                    if np.sum(acceptance_ls[EachIter - 100 : EachIter]) < 60:\n",
    "                        # decrease epsilon\n",
    "                        self.epsilon *= 0.995\n",
    "                    if np.sum(acceptance_ls[EachIter - 100 : EachIter]) > 80:\n",
    "                        # increase epsilon\n",
    "                        self.epsilon *= 1.005\n",
    "                if EachIter % 100 == 0 and EachIter > 100:\n",
    "                    print(EachIter)\n",
    "                    print(cur_nllik)\n",
    "                    print('acceptance rate: ', np.sum(acceptance_ls[EachIter - 100 : EachIter]) / 100)\n",
    "                    if EachIter < self.total_samples - self.n_samples:\n",
    "                        standard_deviation = torch.tensor(np.std(samples[EachIter - 100:EachIter, :], axis = 0))\n",
    "                        if torch.mean(standard_deviation) > 1e-6:\n",
    "                            self.epsilon = 0.05 * standard_deviation * torch.mean(self.epsilon) / torch.mean(standard_deviation) + 0.95 * self.epsilon\n",
    "            return samples, acceptance_ls, trace_val, nan_ls # [self.total_samples-self.n_samples:, :]\n",
    "        \n",
    "        \n",
    "        \n",
    "    all_theta_TVMAGI = vectorize(TVMAGI_xlatent_torch, TVMAGI_theta_torch, TVMAGI_sigma_torch, time_constant_param_ls)\n",
    "    all_theta_pointwise = vectorize(pointwise_xlatent_torch, pointwise_theta_torch, TVMAGI_sigma_torch, time_constant_param_ls)\n",
    "    sampler = HMC(NegLogLikelihood, all_theta_TVMAGI, \n",
    "                  pointwise_xlatent_torch.shape,\n",
    "                  pointwise_theta_torch.shape, \n",
    "                  TVMAGI_sigma_torch.shape,\n",
    "                  time_constant_param_ls, \n",
    "                  lower_bound = torch.zeros(all_theta_pointwise.shape))\n",
    "    # sampler.Nabla(all_theta)\n",
    "    # lower_bound = torch.zeros(all_theta_pointwise.shape)\n",
    "    samples, b, c, d = sampler.sample(all_theta_TVMAGI, TV_theta_mean, ydata, CovAllDimensionsPyList, fOdeTorch, priorTemperature, KinvthetaList)\n",
    "    np.save('samples_' + str(use_data_idx) + '.npy', samples)\n",
    "    return samples\n",
    "\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "if __name__ ==  '__main__': \n",
    "    observations = np.load('SEIRD observations.npy')\n",
    "    N = 100000.\n",
    "    pool = Pool(processes=1)\n",
    "    results = pool.map(TVMAGI_sampler, range(1))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cultural-hawaiian",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'samples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e65da7a89a16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m192\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mbeta_ls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'samples' is not defined"
     ]
    }
   ],
   "source": [
    "k = samples[4000:, 256:256+192]\n",
    "beta_ls = np.zeros((4000, 64))\n",
    "\n",
    "for i in range(4000):\n",
    "    for j in range(64):\n",
    "        val[j] = k[i].reshape(-1, 3)[j]\n",
    "    beta_ls[i] = val[:, 2]/4\n",
    "    \n",
    "max_beta = np.amax(beta_ls, axis = 0)\n",
    "min_beta = np.min(beta_ls, axis = 0)\n",
    "plt.plot(TVMAGI_theta[:, 2]/4, label='TVMAGI ve')\n",
    "# plt.plot(min_beta, label='min beta')\n",
    "# plt.plot(max_beta, label='max beta')\n",
    "# lower_95 = np.percentile(beta_ls, 100, axis=0)\n",
    "# upper_95 = np.percentile(all_ve, 0, axis=0)\n",
    "plt.fill_between(np.arange(0, 64, 1), min_beta, max_beta, color='grey', alpha = 0.4)\n",
    "plt.scatter(np.arange(0, 64, 1), true_pd, label='True', s=1)\n",
    "# plt.plot(np.arange(0, 64, 2), true_ve, label='true')\n",
    "plt.legend()\n",
    "plt.title(r'$p^d$ - trained hyperparameter')\n",
    "plt.text(10, 0.06, '%=0.5625')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superior-yeast",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
