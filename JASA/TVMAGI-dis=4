import numpy as np
import torch
import scipy
import argparse
import sys
from arma import ode_system, solve_magi
from arma import matrix
import time
import pickle
import matplotlib.pyplot as plt
import scipy.special as fun
from multiprocessing import Pool

torch.set_num_threads(1)
torch.set_default_dtype(torch.double)

# Derivatves of X according to the ODE structure
def fOde(theta, x):
    """
    theta: list[4]: beta, ve, vi, pd
    x: array(n, 4)
    r: array(n, 2)
    """
    global N
    logS = x[:, 0]
    logE = x[:, 1]
    logI = x[:, 2]
    logD = x[:, 3]
    logSdt = -theta[0] * np.exp(logI) / N  # (1)
    logEdt = theta[0] * np.exp(logS + logI - logE) / N - theta[1]  # (2)
    logIdt = np.exp(logE - logI) * theta[1] - theta[2]  # (3)
    logDdt = np.exp(logI - logD) * 0.25 * theta[3] * theta[2]  # (4)
    return np.stack([logSdt, logEdt, logIdt, logDdt], axis=1)


# Derivatives of X
def fOdeDx(theta, x):
    """
    returns derivation of x given theta
    theta: list[4]
    x: array(n, 4)
    r: array(n, 4, 4)
    """
    resultDx = np.zeros(shape=[np.shape(x)[0], np.shape(x)[1], np.shape(x)[1]])
    global N
    logS = x[:, 0]
    logE = x[:, 1]
    logI = x[:, 2]
    logD = x[:, 3]
    # [:, i, j]: 第j个方程关于第i个状态求导
    # (1) / dI
    resultDx[:, 2, 0] = -theta[0] * np.exp(logI) / N
    # (1) / dS, (1) /dE, (1) / dD = 0
    # (2) / dS
    resultDx[:, 0, 1] = theta[0] * np.exp(logS + logI - logE) / N
    # (2) / dE
    resultDx[:, 1, 1] = -theta[0] * np.exp(logS + logI - logE) / N
    # (2) / dI
    resultDx[:, 2, 1] = theta[0] * np.exp(logS + logI - logE) / N
    # (2) / dD = 0
    # (3) / dS = 0
    # (3) / dE
    resultDx[:, 1, 2] = np.exp(logE - logI) * theta[1]
    # (3) / dI
    resultDx[:, 2, 2] = -np.exp(logE - logI) * theta[1]
    # (3) / dD = 0, (4) / dS, dE = 0
    # (4) / dI
    resultDx[:, 2, 3] = np.exp(logI - logD) * 0.25 * theta[3] * theta[2]
    # (4) / dD
    resultDx[:, 3, 3] = -np.exp(logI - logD) * 0.25 * theta[3] * theta[2]
    return resultDx


def fOdeDtheta(theta, x):
    """
    returns derivation of theta given x
    theta: list[4]
    x: array(n, 4)
    r: array(n, 4, 4)
    """
    global N
    resultDtheta = np.zeros(shape=[np.shape(x)[0], np.shape(theta)[0], np.shape(x)[1]])
    logS = x[:, 0]
    logE = x[:, 1]
    logI = x[:, 2]
    logD = x[:, 3]
    # [:, i, j]: 第j个方程对theta_i求导
    # (1) / dRe
    resultDtheta[:, 0, 0] = -np.exp(logI) / N
    # (2) / d theta[0]
    resultDtheta[:, 0, 1] = np.exp(logS + logI - logE) / N
    # (2) / theta[1]
    resultDtheta[:, 1, 1] = -1.
    # (3) / dtheta[1]
    resultDtheta[:, 1, 2] = np.exp(logE - logI)
    # (3) / dtheta[2]
    resultDtheta[:, 2, 2] = -1.
    # (4) / theta[2]
    resultDtheta[:, 2, 3] = np.exp(logI - logD) * 0.25 * theta[3] 
    # (4) / theta[3]
    resultDtheta[:, 3, 3] = np.exp(logI - logD) * 0.25 * theta[2]
    return resultDtheta

def fOdeTorch(theta, x, constant_param_ls):
    """
    theta: list[4]: beta, ve, vi, pd
    x: array(n, 4)
    r: array(n, 2)
    """
    global N
    logS = x[:, 0]
    logE = x[:, 1]
    logI = x[:, 2]
    logD = x[:, 3]
    logSdt = -theta[:, 0] * torch.exp(logI) / N  # (1)
    logEdt = theta[:, 0] * torch.exp(logS + logI - logE) / N - theta[:, 1]  # (2)
    logIdt = torch.exp(logE - logI) * theta[:, 1] - constant_param_ls[0]  # (3)
    # reparametrize on pd
    logDdt = torch.exp(logI - logD) * 0.25 * theta[:, 2] * constant_param_ls[0]  # (4)
    return torch.stack([logSdt, logEdt, logIdt, logDdt], axis=1)


def copy_mat(arma_mat):
    return np.copy(matrix(arma_mat).reshape([-1])).reshape([arma_mat.n_rows, arma_mat.n_cols])


def pointwisethetasigmallikTorch(xlatent, theta, time_constant_param_ls, sigma, inferred_theta, ydata, CovAllDimensionsPyList, fOdeTorch,
                                 priorTemperature, obs_per_day, positive_param=True):
    # length of observed y (t)
    n = ydata.shape[0]
    pdimension = ydata.shape[1]
    thetadimension = theta.shape[1]
    sigmaSq = torch.pow(sigma, 2)
    fderiv = fOdeTorch(theta, xlatent, time_constant_param_ls)
    res = torch.zeros([pdimension, 3]).double()
    fitDerivError = torch.zeros([n, pdimension]).double()
    nobs = torch.zeros([pdimension]).double()
    fitLevelErrorSumSq = torch.zeros([pdimension]).double()
    for vEachDim in range(pdimension):
        fitDerivError[:, vEachDim] = fderiv[:, vEachDim]
        tmp = torch.sparse.mm(CovAllDimensionsPyList[vEachDim]['mphi'], xlatent[:, vEachDim].reshape(-1, 1))
        fitDerivError[:, vEachDim] -= tmp[:, 0]
        nobs[vEachDim] = torch.sum(torch.isfinite(ydata[:, vEachDim]))
        obsIdx = torch.isfinite(ydata[:, vEachDim])
        fitLevelErrorSumSq[vEachDim] = torch.sum(torch.square(xlatent[obsIdx, vEachDim] - ydata[obsIdx, vEachDim]))
    res[:, 0] = -0.5 * fitLevelErrorSumSq / sigmaSq - torch.log(sigma + 0.001) * nobs
    res[:, 0] /= priorTemperature[2]
    KinvfitDerivError = torch.zeros([n, pdimension]).double()
    CinvX = torch.zeros([n, pdimension]).double()
    for vEachDim in range(pdimension):
        # inverse of K
        KinvfitDerivError[:, vEachDim] = torch.sparse.mm(CovAllDimensionsPyList[vEachDim]['Kinv'], fitDerivError[:, vEachDim].reshape(-1, 1))[:, 0]
        # inverse of Cd
        CinvX[:, vEachDim] = torch.sparse.mm(CovAllDimensionsPyList[vEachDim]['Cinv'], xlatent[:, vEachDim].reshape(-1, 1))[:, 0]
    res[:, 1] = -0.5 * torch.sum(fitDerivError * KinvfitDerivError, dim=0) / priorTemperature[0]
    #  prior distriobution of X
    res[:, 2] = -0.5 * torch.sum(xlatent * CinvX, dim=0) / priorTemperature[1]
    theta_lb = torch.clamp(theta[:, 2], min=0.)
#     theta_ub = torch.clamp(theta[:, 2], max=0.3)
    return torch.sum(res) - 1e6 * torch.sum(torch.square(theta[:, 2] - theta_lb))



def to_band(matrix, bandwidth):
    dim = matrix.shape[0]
    for i in range(dim):
        for j in range(dim):
            if i > j + bandwidth or i < j - bandwidth:
                matrix[i][j] = 0
    return matrix.to_sparse()

def xthetasigmallikTorch(xlatent, theta, time_constant_param_ls, sigma, inferred_theta, ydata, CovAllDimensionsPyList, fOdeTorch,
                         priorTemperature,
                         KinvthetaList, positive=True):
    # length of observed y (t)
    n = ydata.shape[0]
    pdimension = ydata.shape[1]
    thetadimension = theta.shape[1]
    sigmaSq = torch.pow(sigma, 2)
    fderiv = fOdeTorch(theta, xlatent, time_constant_param_ls)
    res = torch.zeros([pdimension, 3]).double()
    res_theta = torch.zeros(thetadimension).double()
    res2 = torch.zeros(1).double()
    fitDerivError = torch.zeros([n, pdimension]).double()
    nobs = torch.zeros([pdimension]).double()
    fitLevelErrorSumSq = torch.zeros([pdimension]).double()
    for vEachDim in range(pdimension):
        fitDerivError[:, vEachDim] = fderiv[:, vEachDim]
        fitDerivError[:, vEachDim] -= torch.sparse.mm(CovAllDimensionsPyList[vEachDim]['mphi'], xlatent[:, vEachDim].reshape(-1, 1))[:, 0]
        nobs[vEachDim] = torch.sum(torch.isfinite(ydata[:, vEachDim]))
        obsIdx = torch.isfinite(ydata[:, vEachDim])
        fitLevelErrorSumSq[vEachDim] = torch.sum(torch.square(xlatent[obsIdx, vEachDim] - ydata[obsIdx, vEachDim]))
    res[:, 0] = -0.5 * fitLevelErrorSumSq / sigmaSq - torch.log(sigma + 0.0001) * nobs
    res[:, 0] /= priorTemperature[2]
    KinvfitDerivError = torch.zeros([n, pdimension]).double()
    CinvX = torch.zeros([n, pdimension]).double()
    for vEachDim in range(pdimension):
        # inverse of K
        KinvfitDerivError[:, vEachDim] = torch.sparse.mm(CovAllDimensionsPyList[vEachDim]['Kinv'], fitDerivError[:, vEachDim].reshape(-1, 1))[:, 0]
        # inverse of Cd
        CinvX[:, vEachDim] = torch.sparse.mm(CovAllDimensionsPyList[vEachDim]['Cinv'], xlatent[:, vEachDim].reshape(-1, 1))[:, 0]
    for thetaEachDim in range(thetadimension):
        res_theta[thetaEachDim] = -0.5 * torch.sum(
            (theta[:, thetaEachDim] - inferred_theta[thetaEachDim]) @ torch.sparse.mm(KinvthetaList[thetaEachDim], (
                    theta[:, thetaEachDim] - inferred_theta[thetaEachDim]).reshape(-1, 1)))
    res[:, 1] = -0.5 * torch.sum(fitDerivError * KinvfitDerivError, dim=0) / priorTemperature[0]
    res[:, 2] = -0.5 * torch.sum(xlatent * CinvX, dim=0) / priorTemperature[1]
    theta_lb = torch.clamp(theta[:, 2], min = 0.)
    return torch.sum(res) + torch.sum(res_theta)  - 1e6 * torch.sum(torch.square(theta[:, 2] - theta_lb))


class Bessel(torch.autograd.Function):
    @staticmethod
    def forward(ctx, inp, nu):
        ctx._nu = nu
        ctx.save_for_backward(inp)
        mat = fun.kv(nu, inp.detach().numpy())
        return (torch.from_numpy(np.array(mat)))

    @staticmethod
    def backward(ctx, grad_out):
        inp, = ctx.saved_tensors
        nu = ctx._nu
        grad_in = grad_out.numpy() * np.array(fun.kvp(nu, inp.detach().numpy()))
        return (torch.from_numpy(grad_in), None)


class generalMatern(object):

    # has_lengthscale = True

    def __init__(self, nu, lengthscale, **kwargs):
        # super(Matern,self).__init__(**kwargs)
        self.nu = nu
        self.log_lengthscale = torch.tensor(np.log(lengthscale))
        self.log_lengthscale.requires_grad_(True)

    def _set_lengthscale(self, lengthscale):
        self.log_lengthscale = torch.tensor(np.log(lengthscale))

    def lengthscale(self):
        return (torch.exp(self.log_lengthscale).item())

    def forward(self, x1, x2=None, **params):
        lengthscale = torch.exp(self.log_lengthscale)
        x1 = x1.squeeze()
        if x2 is None: x2 = x1
        r_ = (x1.reshape(-1, 1) - x2.reshape(1, -1)).abs()
        r_ = np.sqrt(2. * self.nu) * r_ / lengthscale
        # handle limit at 0, allows more efficient backprop
        r_ = torch.clamp(r_, min=1e-15)
        C_ = np.power(2, 1 - self.nu) * np.exp(-fun.loggamma(self.nu)) * torch.pow(r_, self.nu)
        mat = Bessel.apply(r_, self.nu)
        C_ = C_ * mat
        return (C_)

    def C(self, x1, x2=None):
        return (self.forward(x1, x2).detach())


def GPTrain(train_x, train_y, nu, lengthscale_lb=10., learning_rate=1e-5, noisy=True, max_iter=10, verbose=False,
            eps=1e-6):
    # preprocess input data
    n = train_x.size(0)
    # normalized x to 0 and 1
    x_range = [torch.min(train_x).item(), torch.max(train_x).item()]
    train_x = (train_x - x_range[0]) / (x_range[1] - x_range[0])
    #     train_x[0] = eps
    # set up kernel
    kernel = generalMatern(nu=nu, lengthscale=1.1 * lengthscale_lb / (x_range[1] - x_range[0]))
    # lambda = noise/outputscale
    log_lambda = torch.tensor(np.log(1e-2))
    log_lambda.requires_grad_(True)
    loglb_normalized = torch.log(torch.tensor(lengthscale_lb / (x_range[1] - x_range[0])))
    optimizer = torch.optim.LBFGS([kernel.log_lengthscale, log_lambda], lr=learning_rate)
    # training
    prev_loss = np.Inf
    for i in range(max_iter):
        print(max_iter)
        R = kernel.forward(train_x) + torch.exp(log_lambda) * torch.eye(n)
        e, v = torch.eig(R, eigenvectors=True)
        e = e[:, 0]  # eigenvalues
        a = v.T @ torch.ones(n)
        b = v.T @ train_y
        mean = ((a / e).T @ b) / ((a / e).T @ a)
        d = v.T @ (train_y - mean)
        outputscale = 1. / n * (d / e).T @ d

        def closure():
            optimizer.zero_grad()
            R = kernel.forward(train_x) + torch.exp(log_lambda) * torch.eye(n)
            e, v = torch.eig(R, eigenvectors=True)
            e = e[:, 0]  # eigenvalues
            a = v.T @ torch.ones(n)
            b = v.T @ train_y
            mean = ((a / e).T @ b) / ((a / e).T @ a)
            d = v.T @ (train_y - mean)
            outputscale = 1. / n * (d / e).T @ d
            loss = torch.log(outputscale) + torch.mean(torch.log(e))
            tmp0 = torch.clamp(kernel.log_lengthscale, max=0.)
            loss = loss + 1e3 * torch.sum(torch.square(kernel.log_lengthscale - tmp0))
            tmp = torch.clamp(kernel.log_lengthscale, min=loglb_normalized)
            loss  = loss + 1e3 * torch.sum(torch.square(kernel.log_lengthscale - tmp))
            tmp2 = torch.clamp(log_lambda, min=np.log(1e-6))
            loss  = loss + 1e3 * torch.sum(torch.square(log_lambda - tmp2))
            print(loss)
            print((x_range[1] - x_range[0]) * torch.exp(kernel.log_lengthscale).item(), torch.exp(log_lambda))
            loss.backward()
            return loss

        optimizer.step(closure)

    R = kernel.forward(train_x) + torch.exp(log_lambda) * torch.eye(n)
    Rinv = torch.inverse(R)
    ones = torch.ones(n)
    mean = ((ones.T @ Rinv @ train_y) / (ones.T @ Rinv @ ones)).item()
    outputscale = (1 / n * (train_y - mean).T @ Rinv @ (train_y - mean)).item()
    noisescale = outputscale * torch.exp(log_lambda).item()
    return mean, outputscale, noisescale, (x_range[1] - x_range[0]) * torch.exp(kernel.log_lengthscale).item()


def TVMAGI_sampler(use_data_idx = 6,
                days = 32,
                discretization = 2,
                obs_per_day = 1,
                theta_lowerbound=np.array([0., 0., 0., 0.]),
                theta_upperbound=np.array([np.inf, 1., 1., 1.]),
                param_names = ['re', 've', 'vi', 'pd'],
                is_time_varying=[True, True, False, True],
                use_trajectory='inferred',
                learning_rate=np.array([1e-3, 1e-3, 1e-5]),
                n_iter = [15001, 100000, 1000, 10000],
                phi1_lb_ls=np.array([1., 0.1, 0.1]),
                phi2_lb_ls=np.array([5., 5., 5.]),
                sigma_lb_ls = np.array([0.01, 0.01, 0.01]),
                bandwidth=20,
                nu=2.01,
                observations = np.load('SEIRD_observations.npy'),
                N = 100000.):
    
    
    yobs = observations[use_data_idx]
    yobs[:, 1] = np.interp(np.arange(0, days, 1), np.arange(0, days, 2), yobs[::2, 1])
    nobs, p_dim = yobs.shape[0], yobs.shape[1]
    n_points = days * discretization
    theta_dim = theta_lowerbound.shape[0]
    d_matrix = torch.zeros((n_points, n_points), dtype=torch.double)
    for i in range(n_points):
        for j in range(n_points):
            if i > j:
                d_matrix[i][j] = (i - j) / (obs_per_day * discretization)
            else:
                d_matrix[i][j] = (j - i) / (obs_per_day * discretization)
    Ode_system = ode_system("ODE-python", fOde, fOdeDx, fOdeDtheta,
                             thetaLowerBound=theta_lowerbound,
                             thetaUpperBound=theta_upperbound)
    tvecObs = np.arange(0, days, 1. / obs_per_day)
    tvecFull = np.arange(0, days, 1. / (obs_per_day * discretization))
    yFull = np.ndarray([n_points, p_dim])
    yFull.fill(np.nan)
    yFull[np.arange(0, discretization * nobs, discretization).astype(int), :] = yobs
    xInitExogenous = np.zeros_like(yFull)
    # interpolate: find the ydata of tvecFull given observations
    for i in range(p_dim):
        xInitExogenous[:, i] = np.interp(tvecFull, tvecObs, yobs[:, i])


    # First stage: use MAGI package to optimize theta as constant #####################

    result = solve_magi(
        yFull,
        Ode_system,
        tvecFull,
        sigmaExogenous=np.array([]),
        phiExogenous=np.array([[]]),
        xInitExogenous=xInitExogenous,
        thetaInitExogenous=np.array([]),
        muExogenous=np.array([[]]),
        dotmuExogenous=np.array([[]]),
        priorTemperatureLevel=yFull.shape[0] / yobs.shape[0],
        priorTemperatureDeriv=yFull.shape[0] / yobs.shape[0],
        priorTemperatureObs=1.0,
        kernel="generalMatern",
        nstepsHmc=100,
        burninRatioHmc=0.5,
        niterHmc=n_iter[0],
        stepSizeFactorHmc=0.01,
        nEpoch=1,
        bandSize=bandwidth,
        useFrequencyBasedPrior=True,
        useBand=False,
        useMean=False,
        useScalerSigma=False,
        useFixedSigma=False,
        verbose=True)
    samplesCpp = result['samplesCpp']
    llikId = 0
    xId = range(np.max(llikId) + 1, np.max(llikId) + yFull.size + 1)
    # dimension of theta
    thetaId = range(np.max(xId) + 1, np.max(xId) + theta_dim + 1)
    sigmaId = range(np.max(thetaId) + 1, np.max(thetaId) + yFull.shape[1] + 1)
    burnin = int(n_iter[0] * 0.5)
    xsampled = samplesCpp[xId, (burnin + 1):]
    xsampled = xsampled.reshape([yFull.shape[1], yFull.shape[0], -1])
    CovAllDimensionsPyList = []
    thetaSampled = samplesCpp[thetaId, (burnin + 1):]
    inferred_theta = np.mean(thetaSampled, axis=-1)
    print(inferred_theta)
    sigmaSampled = samplesCpp[sigmaId, (burnin + 1):]
    inferred_sigma = np.mean(sigmaSampled, axis=-1)
    inferred_trajectory = np.mean(xsampled, axis=-1)
    for each_gpcov in result['result_solved'].covAllDimensions:
            each_pycov = dict(
                Cinv=to_band(torch.from_numpy(matrix(each_gpcov.Cinv)).double(),bandwidth=bandwidth), 
                Kinv=to_band(torch.from_numpy(matrix(each_gpcov.Kinv)).double(),bandwidth=bandwidth),
                mphi=to_band(torch.from_numpy(matrix(each_gpcov.mphi)).double(),bandwidth=bandwidth),
            )
            CovAllDimensionsPyList.append(each_pycov)
            
    TV_theta_mean = np.zeros(int(sum(is_time_varying)))
    tv_index = 0
    for thetaEachDim in range(theta_dim):
        if is_time_varying[thetaEachDim] == True:
            TV_theta_mean[tv_index] = inferred_theta[thetaEachDim]
            tv_index += 1

    if use_trajectory == 'observation':
        pointwise_xlatent_torch = torch.tensor(xInitExogenous, requires_grad=True, dtype=torch.double)
    elif use_trajectory == 'inferred':
        pointwise_xlatent_torch = torch.tensor(inferred_trajectory.transpose(), requires_grad=True, dtype=torch.double)
    else:
        raise ValueError
    tmp1 = np.array([TV_theta_mean])
    initial_tvtheta = np.repeat(tmp1, pointwise_xlatent_torch.shape[0], axis=0)
    pointwise_theta_torch = torch.tensor(initial_tvtheta, requires_grad=True, dtype=torch.double)
    pointwise_sigma_torch = torch.tensor(inferred_sigma, requires_grad=True, dtype=torch.double)
    time_constant_param_ls = []
    for thetaEachDim in range(theta_dim):
        if is_time_varying[thetaEachDim] == 0:
            param_name = param_names[thetaEachDim]
            locals()[param_name] = torch.tensor([inferred_theta[thetaEachDim]], requires_grad=True, dtype=torch.double)
            time_constant_param_ls.append(eval(param_name))

    ydata = torch.from_numpy(yFull).double()
    priorTemperature = torch.tensor([discretization, discretization, 1.0])  # ?
    pointwise_optimizer = torch.optim.Adam([pointwise_xlatent_torch, pointwise_theta_torch, pointwise_sigma_torch] + time_constant_param_ls, lr=1e-4)  # , weight_decay = 1.0
    pointwise_lr_scheduler = torch.optim.lr_scheduler.StepLR(pointwise_optimizer, step_size=10000, gamma=0.5)
    cur_loss = np.Inf
    LossVal = np.zeros(n_iter[1])
    for epoch in range(n_iter[1]):
        pointwise_optimizer.zero_grad()
        # compute loss function
        llik = pointwisethetasigmallikTorch(pointwise_xlatent_torch, pointwise_theta_torch, time_constant_param_ls, pointwise_sigma_torch,
                                            TV_theta_mean, ydata, CovAllDimensionsPyList, fOdeTorch,
                                            priorTemperature, obs_per_day)
        new_loss = -llik
        LossVal[epoch] = new_loss
        if epoch % 200 == 0:
            print(epoch, new_loss.item())
            diff = new_loss.item() - cur_loss
    #             if torch.isnan(new_loss) == False and diff > -0.01 and diff < 0.01:
    #                 break
            cur_loss = new_loss.item()
        new_loss.backward()
        pointwise_optimizer.step()
        pointwise_lr_scheduler.step()
        
    # kernel estimation #######################
    KinvthetaList = []
    KthetaList = []      
    for EachDim in range(pointwise_theta_torch.shape[1]):        
        a, outputscale, c, lengthscale = GPTrain(torch.tensor(np.arange(0., days, 1 / discretization)), pointwise_theta_torch.detach()[:, EachDim], nu)
        print(a, outputscale, c, lengthscale)
        ker = outputscale * generalMatern(nu, lengthscale).C(torch.tensor(np.arange(0., days, 1 / discretization)))
        KinvthetaList.append(to_band(torch.inverse(ker), bandwidth = bandwidth))
        KthetaList.append(ker)
    
    
    # TVMAGI optimization #############################
    TVMAGI_xlatent_torch = torch.tensor(pointwise_xlatent_torch.detach().numpy(), requires_grad=True, dtype=torch.double)
    TVMAGI_theta_torch = torch.tensor(pointwise_theta_torch.detach().numpy(), requires_grad=True, dtype=torch.double)
    TVMAGI_sigma_torch = torch.tensor(inferred_sigma, requires_grad=True, dtype=torch.double)
    TVMAGI_optimizer = torch.optim.Adam([TVMAGI_xlatent_torch, TVMAGI_theta_torch, TVMAGI_sigma_torch] + time_constant_param_ls, lr=1e-5)
    lr_scheduler = torch.optim.lr_scheduler.StepLR(TVMAGI_optimizer, step_size=10000, gamma=0.9)
    cur_loss = np.Inf
    for epoch in range(n_iter[3]):
        TVMAGI_optimizer.zero_grad()
        # compute loss function
        llik = xthetasigmallikTorch(TVMAGI_xlatent_torch, TVMAGI_theta_torch, time_constant_param_ls, TVMAGI_sigma_torch,
                                                    TV_theta_mean, ydata, CovAllDimensionsPyList, fOdeTorch,
                                                    priorTemperature, KinvthetaList)
        loss = -llik
        if epoch % 500 == 0:
            print(epoch, loss.item())
    #             if torch.isnan(loss) == False and loss - cur_loss > -0.01 and loss - cur_loss < 0.01:
    #                 break
            cur_loss = loss.item()
        loss.backward()
        TVMAGI_optimizer.step()
        lr_scheduler.step()
        
    np.save('SEIRD-theta-'+ str(use_data_idx) + '.npy', TVMAGI_theta_torch.detach().numpy())
    np.save('SEIRD-xinit-'+ str(use_data_idx) + '.npy', TVMAGI_xlatent_torch.detach().numpy())
    import pickle
    f = open("time_constant_param_ls" + str(use_data_idx) + ".pkl", "wb")
    pickle.dump(time_constant_param_ls, f)
        
        
#     # HMC sampler ##############
#     def vectorize(xlatent, theta, sigma, time_constant_param_ls):
#         t1 = torch.reshape(xlatent.detach(), (-1,))
#         t2 = torch.reshape(theta.detach(), (-1,))
#         t3 = torch.reshape(sigma.detach(), (-1,))
#         long_vec = torch.cat((t1, t2, t3))
#         for i in range(len(time_constant_param_ls)):
#             long_vec = torch.cat((long_vec, time_constant_param_ls[i].detach()))
#         return long_vec

#     def get_dim(tensor_shape):
#         if len(tensor_shape) == 0:
#             return 1
#         if len(tensor_shape) == 1:
#             return tensor_shape[0]
#         dim = 1
#         for i in range(len(tensor_shape)):
#             dim *= tensor_shape[i]
#         return dim

#     def devectorize(long_tensor, xlatent_shape, theta_shape, sigma_shape, time_constant_param_dim):
#         x_latent_dim = get_dim(xlatent_shape)
#         theta_dim = get_dim(theta_shape)
#         sigma_dim = get_dim(sigma_shape)
#         time_constant_param_ls = []
#         xlatent = torch.reshape(long_tensor[:x_latent_dim],xlatent_shape)
#         theta = torch.reshape(long_tensor[x_latent_dim:x_latent_dim + theta_dim],theta_shape)
#         sigma = torch.reshape(long_tensor[x_latent_dim + theta_dim:x_latent_dim + theta_dim + sigma_dim],sigma_shape)
#         for each in range(x_latent_dim + theta_dim + sigma_dim, long_tensor.shape[0]):
#             time_constant_param_ls.append(torch.tensor([long_tensor[each]]))
#         return xlatent, theta, sigma, time_constant_param_ls


#     def NegLogLikelihood(xlatent, theta, sigma, time_constant_param_ls, 
#                          inferred_theta = inferred_theta, 
#                          ydata = ydata, 
#                          CovAllDimensionsPyList = CovAllDimensionsPyList, 
#                          fOdeTorch = fOdeTorch,
#                          priorTemperature = priorTemperature, 
#                          KinvthetaList = KinvthetaList):
#         # length of observed y (t)
#         n = ydata.shape[0]
#         pdimension = ydata.shape[1]
#         thetadimension = theta.shape[1]
#         sigmaSq = torch.pow(sigma, 2)
#         fderiv = fOdeTorch(theta, xlatent, time_constant_param_ls)
#         res = torch.zeros([pdimension, 3]).double()
#         res_theta = torch.zeros(thetadimension).double()
#         fitDerivError = torch.zeros([n, pdimension]).double()
#         nobs = torch.zeros([pdimension]).double()
#         fitLevelErrorSumSq = torch.zeros([pdimension]).double()
#         for vEachDim in range(pdimension):
#             fitDerivError[:, vEachDim] = fderiv[:, vEachDim]
#             fitDerivError[:, vEachDim] -= torch.sparse.mm(CovAllDimensionsPyList[vEachDim]['mphi'], xlatent[:, vEachDim].reshape(-1, 1))[:, 0]
#             nobs[vEachDim] = torch.sum(torch.isfinite(ydata[:, vEachDim]))
#             obsIdx = torch.isfinite(ydata[:, vEachDim])
#             fitLevelErrorSumSq[vEachDim] = torch.sum(torch.square(xlatent[obsIdx, vEachDim] - ydata[obsIdx, vEachDim]))
#         res[:, 0] = -0.5 * fitLevelErrorSumSq / sigmaSq - torch.log(sigma + 0.0001) * nobs
#         res[:, 0] /= priorTemperature[2]
#         KinvfitDerivError = torch.zeros([n, pdimension]).double()
#         CinvX = torch.zeros([n, pdimension]).double()
#         for vEachDim in range(pdimension):
#             # inverse of K
#             KinvfitDerivError[:, vEachDim] = torch.sparse.mm(CovAllDimensionsPyList[vEachDim]['Kinv'], fitDerivError[:, vEachDim].reshape(-1, 1))[:, 0]
#             # inverse of Cd
#             CinvX[:, vEachDim] = torch.sparse.mm(CovAllDimensionsPyList[vEachDim]['Cinv'], xlatent[:, vEachDim].reshape(-1, 1))[:, 0]
#         for thetaEachDim in range(thetadimension):
#             res_theta[thetaEachDim] = -0.5 * torch.sum(
#                 (theta[:, thetaEachDim] - inferred_theta[thetaEachDim]) @ torch.sparse.mm(KinvthetaList[thetaEachDim], (theta[:, thetaEachDim] - inferred_theta[thetaEachDim]).reshape(-1, 1))[:, 0])
#         res[:, 1] = -0.5 * torch.sum(fitDerivError * KinvfitDerivError, dim=0) / priorTemperature[0]
#         res[:, 2] = -0.5 * torch.sum(xlatent * CinvX, dim=0) / priorTemperature[1]

#         return -(torch.sum(res) + torch.sum(res_theta))

#     class HMC:
#         def __init__(self, negllik, all_theta, xlatent_shape, theta_shape, sigma_shape, time_constant_param_ls, lsteps=50, epsilon=1e-5, n_samples=4000, upper_bound = None, lower_bound = None, burn_in_ratio = 0.5):
#             self.all_theta = all_theta
#             self.theta_shape = theta_shape
#             self.xlatent_shape = xlatent_shape
#             self.sigma_shape = sigma_shape
#             self.constant_dim = len(time_constant_param_ls)
#             self.lsteps = lsteps
#             self.epsilon = epsilon * torch.ones(all_theta.shape)
#             self.burn_in_ratio = burn_in_ratio
#             self.n_samples = n_samples
#             self.total_samples = int(n_samples / (1 - burn_in_ratio))
#             self.NegLogLikelihood = negllik
#             self.ub = upper_bound
#             if upper_bound is not None:
#                 if upper_bound.shape[0] != all_theta.shape[0]:
#                     raise ValueError
#             self.lb = lower_bound
#             if lower_bound is not None:
#                 if lower_bound.shape[0] != all_theta.shape[0]:
#                     raise ValueError

#         def NegLogLikelihood_vec(self, all_theta):
#             xlatent_0, theta_0, sigma_0, constant_param_ls_0 = devectorize(all_theta, self.xlatent_shape, self.theta_shape, self.sigma_shape, self.constant_dim)
#             return NegLogLikelihood(xlatent_0, theta_0, sigma_0, constant_param_ls_0)

#         def Nabla(self, theta_torch):
#             theta_torch = theta_torch.detach()
#             xlatent, theta, sigma, constant_param_ls = devectorize(theta_torch, self.xlatent_shape, self.theta_shape, self.sigma_shape, self.constant_dim)
#             xlatent.requires_grad = True  
#             theta.requires_grad = True
#             sigma.requires_grad = True
#             for each in constant_param_ls:
#                 each.requires_grad = True                      
#             llik = self.NegLogLikelihood(xlatent, theta, sigma, constant_param_ls)
#             llik.backward()
#             constant_param_deriv_ls = []
#             for each in constant_param_ls:
#                 constant_param_deriv_ls.append(each.grad)
#             v = vectorize(xlatent.grad, theta.grad, sigma.grad, constant_param_deriv_ls)

#             return v
#         def sample(self, all_theta, TV_theta_mean, ydata, CovAllDimensionsPyList, fOdeTorch, priorTemperature, KinvthetaList):
#             def bounce(m, lb, ub):
#                 if lb is None and ub is None:
#                     return m
#                 if lb is None:
#                     max_tensor = torch.clamp(m - ub, min=0)
#                     return m - 2 * max_tensor
#                 if ub is None:
#                     min_tensor = torch.clamp(lb - m, min=0)
#                     return m + 2 * min_tensor
#                 if torch.sum(lb < ub) < m.shape[0]:
#                     raise ValueError
#                 if torch.sum(m >= lb) == m.shape[0] and torch.sum(m <= ub) == m.shape[0]:
#                     return m
#                 if torch.sum(m >= lb) < m.shape[0]:
#                     min_tensor = torch.clamp(lb - m, min=0)
#                     return bounce(m + 2 * min_tensor, lb, ub)
#                 if torch.sum(m <= ub) < m.shape[0]:
#                     max_tensor = torch.clamp(m - ub, min=0)
#                     return bounce(m - 2 * max_tensor, lb, ub)

#             trace_val = np.zeros(self.total_samples)
#             samples = np.zeros((self.total_samples, self.all_theta.shape[0]))
#             random_ls = np.random.uniform(0, 1, self.total_samples)
#             acceptance_ls = np.zeros(self.total_samples)
#             nan_ls = np.zeros(self.total_samples)
#             cur_theta = self.all_theta.clone().detach()
#             for EachIter in range(self.total_samples): ############
#                 cur_nllik_1 = self.NegLogLikelihood_vec(cur_theta).detach()
#                 rstep = torch.rand(self.epsilon.shape) * self.epsilon + self.epsilon
#                 p = torch.normal(mean=0., std=torch.ones(self.all_theta.shape))
#                 cur_p = p.clone()
#                 theta = cur_theta.clone()         
#                 p = p - rstep * self.Nabla(theta).clone() / 2
#                 for i in range(self.lsteps):
#                     theta = theta + rstep * p
#                     nabla_torch = self.Nabla(theta).clone()
#                     p = p - rstep * nabla_torch
#                     theta = bounce(theta, self.lb, self.ub)

#                 p = p - rstep * self.Nabla(theta).clone() / 2

#                 new_nllik = self.NegLogLikelihood_vec(theta)
#                 new_p = 0.5 * torch.sum(torch.square(p))
#                 new_H = new_nllik + new_p
#                 cur_nllik = self.NegLogLikelihood_vec(cur_theta).detach()
#                 cur_H = cur_nllik + 0.5 * torch.sum(torch.square(cur_p))
#     #             print(new_H, cur_H)

#                 if torch.isnan(theta[0]) or torch.isnan(new_H):
#                     samples[EachIter] = cur_theta.clone()
#                     nan_ls[EachIter] = 1
#                     self.epsilon *= 0.9
#                     print('NaN!')
#                 else:
#                     # accept
#                     tmp = float(torch.exp(cur_H - new_H))
#     #                 print(tmp)
#                     if  tmp > random_ls[EachIter]:
#                         samples[EachIter] = theta.clone()
#                         cur_theta = theta.clone()
#                         acceptance_ls[EachIter] = 1
#                     # reject
#                     else:
#                         samples[EachIter] = cur_theta.clone()

#                 trace_val[EachIter] = self.NegLogLikelihood_vec(cur_theta).item()        

#                 if EachIter > 200 and EachIter < self.total_samples - self.n_samples:
#                     if np.sum(acceptance_ls[EachIter - 100 : EachIter]) < 60:
#                         # decrease epsilon
#                         self.epsilon *= 0.995
#                     if np.sum(acceptance_ls[EachIter - 100 : EachIter]) > 90:
#                         # increase epsilon
#                         self.epsilon *= 1.005
#                 if EachIter % 100 == 0 and EachIter > 100:
#                     print(EachIter)
#                     print(cur_nllik)
#                     print('acceptance rate: ', np.sum(acceptance_ls[EachIter - 100 : EachIter]) / 100)
#                     if EachIter < self.total_samples - self.n_samples:
#                         standard_deviation = torch.tensor(np.std(samples[EachIter - 100:EachIter, :], axis = 0))
#                         if torch.mean(standard_deviation) > 1e-6:
#                             self.epsilon = 0.05 * standard_deviation * torch.mean(self.epsilon) / torch.mean(standard_deviation) + 0.95 * self.epsilon
#             return samples, acceptance_ls, trace_val, nan_ls # [self.total_samples-self.n_samples:, :]
  
#     all_theta_TVMAGI = vectorize(TVMAGI_xlatent_torch, TVMAGI_theta_torch, TVMAGI_sigma_torch, time_constant_param_ls)
#     all_theta_pointwise = vectorize(pointwise_xlatent_torch, pointwise_theta_torch, TVMAGI_sigma_torch, time_constant_param_ls)
#     sampler = HMC(NegLogLikelihood, all_theta_TVMAGI, 
#                   pointwise_xlatent_torch.shape,
#                   pointwise_theta_torch.shape, 
#                   TVMAGI_sigma_torch.shape,
#                   time_constant_param_ls, 
#                   lower_bound = torch.zeros(all_theta_pointwise.shape))
#     # sampler.Nabla(all_theta)
#     # lower_bound = torch.zeros(all_theta_pointwise.shape)
#     samples, b, c, d = sampler.sample(all_theta_TVMAGI, TV_theta_mean, ydata, CovAllDimensionsPyList, fOdeTorch, priorTemperature, KinvthetaList)
#     k = samples[4000:, 256:256+192]
#     n_total = days * obs_per_day * discretization
#     beta_ls = np.zeros((4000, n_total))
#     ve_ls = np.zeros((4000, n_total))
#     pd_ls = np.zeros((4000, n_total))

#     for i in range(4000):
#         val = np.zeros((n_total, 3))
#         for j in range(n_total):
#             val[j] = k[i].reshape(-1, 3)[j]
#         beta_ls[i] = val[:, 0]
#         ve_ls[i] = val[:, 1]
#         pd_ls[i] = val[:, 2] / 4

#     max_beta = np.percentile(beta_ls, 100, axis=0)
#     min_beta = np.percentile(beta_ls, 0, axis=0)
#     mean_beta = np.mean(alpha_ls, axis=0)
#     max_ve = np.percentile(ve_ls, 100, axis=0)
#     min_ve = np.percentile(ve_ls, 0, axis=0)
#     mean_ve = np.mean(ve_ls, axis=0)
#     max_pd = np.percentile(pd_ls, 100, axis=0)
#     min_pd = np.percentile(pd_ls, 0, axis=0)
#     mean_pd = np.mean(pd_ls, axis=0)
#     np.save('SEIRD-beta-mean-'+ str(use_data_idx) + '.npy', mean_beta)
#     np.save('SEIRD-beta-min-'+ str(use_data_idx) + '.npy', min_beta)
#     np.save('SEIRD-beta-max-'+ str(use_data_idx) + '.npy', max_beta)
#     np.save('SEIRD-ve-mean-'+ str(use_data_idx) + '.npy', mean_ve)
#     np.save('SEIRD-ve-min-'+ str(use_data_idx) + '.npy', min_ve)
#     np.save('SEIRD-ve-max-'+ str(use_data_idx) + '.npy', max_ve)
#     np.save('SEIRD-pd-mean-'+ str(use_data_idx) + '.npy', mean_pd)
#     np.save('SEIRD-pd-min-'+ str(use_data_idx) + '.npy', min_pd)
#     np.save('SEIRD-pd-max-'+ str(use_data_idx) + '.npy', max_pd)
#     np.save('SEIRD-TVMAGI-' + str(use_data_idx) + '.npy', TVMAGI_theta_torch.detach().numpy())
#     return samples, b, c, d







if __name__ ==  '__main__': 
    observations = np.load('SEIRD_observations.npy')
    N = 100000.
    pool = Pool(processes=100)
    results = pool.map(TVMAGI_sampler, range(100))
    pool.close()
    pool.join()
